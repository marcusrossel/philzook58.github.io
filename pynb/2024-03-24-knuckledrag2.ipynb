{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "date: 2024-03-24\n",
    "title: \"Knuckledragger 2: ATP for Python ITP\"\n",
    "---\n",
    "\n",
    "Knuckledragger is the moniker I've given to an approach and [library](https://github.com/philzook58/knuckledragger) I'm developing to do interactive theorem proving in python with the heavy lifting done by pre existing automated solvers.\n",
    "\n",
    "I intend to match the base formalism of my system to what the automated solvers offer. This hopefully avoids some impedance mismatch that occurs in sledgehammer systems.\n",
    "\n",
    "The reason to use python is because it is available everywhere and has libraries for everything. It already has an excellent interactive ecosystem in the form of jupyter notebooks and you can easily standup web demos in the form of links to google collab\n",
    "\n",
    "That there isn't a good library already in python for unification, lambda evaluation, term indexing is somewhat mysterious to me.\n",
    "\n",
    "I've tried to avoid having much code. By [piggybacking on pyz3's ast](https://www.philipzucker.com/python-itp/) you can go pretty far. But there are limitations and at a certain point this is an awkward design. I'm not sure z3's support of quantifier reasoning is complete enough to go the distance.\n",
    "\n",
    "There are another older class of solver beside SMT. Roughly these are the resolution and superposition provers that compete in the [CASC](https://tptp.org/CASC/) automated reasoning competition. Two prominent systems are [vampire](https://vprover.github.io/) and [eprover](https://github.com/eprover/eprover). These solvers are saturation style, inferring more and more theorems until they find the goal. In this respect they are reminiscent of datalog and egraph saturation. This also more closely matches the definitions of logical reasoning as the minimal set of syntax reachable under inference rules from axioms. Most importantly, even though I usually poo poo this, the solvers chase completeness for their inference.\n",
    "\n",
    "# Terms\n",
    "\n",
    "For the purposes of a library, it is worth going verbose to offer more functionality. The lightest weight term type in python is a string tagged tuple. But really, for clarity, frozen dataclasses are quite nice. You get a lot of functionality for free and it is less weird. The weirder the library I build, probably the less people will use it.\n",
    "\n",
    "Some core datatypes you'll see in theorem proving developments are variables and function symbols. A function symbol with no arguments is called a constant.\n",
    "\n",
    "Variables represents implicitly universally quantified pieces of statements. Like `f(f(X)) = f(X)` is the statement that `f` is idempotent. If we had quantifiers, this would be expressed as `forall x, f(f(x)) = f(x)`. There is no law that logic must contain quantifiers.\n",
    "\n",
    "I specialized out `Eq` because I want it to print infix, basically it is still just a 2-arity function symbol. A very special one that the solvers have a lot of support for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Term:\n",
    "    pass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Var(Term):\n",
    "    name: str\n",
    "    def __repr__(self):\n",
    "        return self.name.title()\n",
    "    def vars(self):\n",
    "        return {self}\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Fn(Term):\n",
    "    name: str\n",
    "    args: tuple[Term, ...]\n",
    "    def __eq__(self : Term, other : Term):\n",
    "        return Eq(self, other)\n",
    "    def vars(self):\n",
    "        return {v for a in self.args for v in a.vars()}\n",
    "    def __repr__(self):\n",
    "        return f\"{self.name}({', '.join(map(str, self.args))})\"\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Eq(Term):\n",
    "    lhs: Term\n",
    "    rhs: Term\n",
    "    def __repr__(self):\n",
    "        return f\"{self.lhs} = {self.rhs}\"\n",
    "    def vars(self):\n",
    "        return self.lhs.vars() | self.rhs.vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "def Vars(names):\n",
    "    return [Var(name) for name in names.split()]\n",
    "def Function(name):\n",
    "    return lambda *args: Fn(name, args)\n",
    "def Const(name):\n",
    "    return Fn(name, ())\n",
    "def Consts(names):\n",
    "    return [Const(name) for name in names.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequents\n",
    "\n",
    "This setup is sparser than that is typical in first order logic, where I'd also have connectives like `and` and `or`. \n",
    "\n",
    "This is somewhat inspired by my understanding of the underpinnings of [Isabelle](https://isabelle.in.tum.de/). In isabelle, the base theory (Pure) let's you take sequents and unify them together to get new sequents. What we're doing here is a similar thing, but now we can discharge the unification obligation plus some search to the ATP.\n",
    "\n",
    "I could equally have called the following a clause. The hypotheses are the negative literals and the conclusions are the positive literals.\n",
    "\n",
    "I find it a very interesting perspective to not consider these automated theorem provers as classical first order predicate logic reasoning, but instead as operating at various levels of metalogic. The resolution rule is itself constructing partial proof trees. Each clause is a sequent judgement. Perhaps this has something to do with multiple conclusion logic https://en.wikipedia.org/wiki/Multiple-conclusion_logic \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Sequent():\n",
    "    hyps: tuple[Term,...]\n",
    "    concs: tuple[Term,...]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{list(self.hyps)} |- {list(self.concs)}\"\n",
    "    def fof(self):\n",
    "        vars = {v for t in self.hyps for v in t.vars()} | {v for t in self.concs for v in t.vars()}\n",
    "        if len(self.hyps) == 0:\n",
    "            return f\"![{','.join(map(repr,vars))}] : ({' & '.join(map(repr,self.concs))})\"\n",
    "        return f\"![{','.join(map(repr,vars))}] : ({' & '.join(map(repr,self.hyps))} => {' & '.join(map(repr,self.concs))})\"\n",
    "    def cnf(self):\n",
    "        if len(self.hyps) == 0:\n",
    "            return \" | \".join(map(repr,self.concs))\n",
    "        else:\n",
    "            forms = [f\"~{h}\" for h in self.hyps]\n",
    "            forms.extend(map(repr,self.concs))\n",
    "            return \" | \".join(forms)\n",
    "            #return f\"{' | '.join(self.concs)} | {' | '.join(hyps)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packaging Eprover\n",
    "I recently tried an experiment in packaging eprover for easy installation and use from. I built a version using [cosmopolitan libc](https://github.com/jart/cosmopolitan) which produces a dat binary that works on linux, windows, mac, x86_64 or aarch64. Then I just included the binary in the repo. I needed to fiddle with a couple eprover makefiles to make the usage of cc, ar, ranlib no longer hardcoded, but otherwise it was fairly painless. \n",
    "\n",
    "Then I copied the binary, put it in my python package, committed it to the git repo, and added a line to my pyproject.toml stating I have extra data files. The `__init__.py` offers the path of this binary because it already can ask the `__file__` magic variable where itself is.\n",
    "\n",
    "The repo for this is here. https://github.com/philzook58/pyeprover\n",
    "I did the same thing for vampire, but did not compile using cosmopolitan. https://github.com/philzook58/pyvampire\n",
    "\n",
    "Anyway, long story short is the following line should just work and then `eprover.run([\"myfile.p])` can call the solver.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/philzook58/pyeprover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proofs and Theorems\n",
    "This is a variation of my proof data structure. There are different approaches possible to this. The important thing is you need a strong distinction between formulas and proven theorems. Here they are distinct because the `Theorem` is an index into a hidden `__proof_db` data structure that actually holds the formula/sequent.\n",
    "\n",
    "The two ways of controlling the db are `assume` and `infer`. Assume let's you just add stuff. The point is not to prevent you from tossing in axioms if you want. The point is to allow you to distinguish between things that follow from deduction vs axioms.\n",
    "\n",
    "`infer` calls eprover. It takes in _`Theorem`_ hypotheses and a `Sequent` conclusion you want to prove. If it succeeds, it returns a `Theorem` pointing to the `Sequent` proven.\n",
    "\n",
    "It turns the hypotheses given into `cnf` TPTP formulas. It turns the conclusion info a `fof` formula, because `cnf` does not support the `conjecture` clause type and I don't want to skolemize and stuff if the solver already does this.\n",
    "\n",
    "\n",
    "Since I hold pointers to the previous theorems in the proof_db, it really is a proof data structure representing some kind of proof dag. Optionally, I can record the output of eprover, which contrains a lower tptp proof info.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eprover\n",
    "from typing import Union\n",
    "from dataclasses import dataclass\n",
    "\n",
    "class ProofDB():\n",
    "    def __init__(self):\n",
    "        self.__proofdb = []\n",
    "    def last_theorem(self):\n",
    "        return Theorem(len(self.__proofdb) - 1, self)\n",
    "    def assume(self, sequent : Union[Sequent, Term]):\n",
    "        if isinstance(sequent,Term):\n",
    "            sequent = Sequent((), [sequent])\n",
    "        assert isinstance(sequent, Sequent)\n",
    "        self.__proofdb.append((sequent, \"assume\"))\n",
    "        return self.last_theorem()\n",
    "    def infer(self, hyps: list[\"Theorem\"], conc: Union[Sequent, Term], timeout=1.0, save_proof=False) -> Sequent:\n",
    "        # could be neat to make hyps optional and just toss proof_db to the solver if hyps=None\n",
    "        assert all(isinstance(hyp, Theorem) for hyp in hyps)\n",
    "        if isinstance(conc,Term):\n",
    "            conc = Sequent((), [conc])\n",
    "        with open(\"/tmp/myfile.p\", \"w\") as f:\n",
    "            for hyp in hyps:\n",
    "                f.write(f\"cnf(thm{hyp.index},axiom, {hyp.cnf()}).\\n\")\n",
    "            f.write(f\"fof(goal, conjecture, {conc.fof()}).\\n\")\n",
    "        res = eprover.run( [\"/tmp/myfile.p\"], timeout=timeout, capture_output=True)\n",
    "        if \"SZS status Theorem\" not in res.stdout.decode(\"utf-8\"):\n",
    "            raise Exception(f\"Proof failed \\n{hyps}\\n----------------(eprover)\\n{conc}\\n\", res.stdout.decode(\"utf-8\")) \n",
    "        if save_proof:\n",
    "            self.__proofdb.append((conc, (\"infer\", hyps, res.stdout)))\n",
    "        else:   \n",
    "            self.__proofdb.append((conc, (\"infer\", hyps)))\n",
    "        return self.last_theorem()\n",
    "    def __getitem__(self, key):\n",
    "        return self.__proofdb[key]\n",
    "    def __len__(self):\n",
    "        return len(self.__proofdb)\n",
    "\n",
    "# It isn't persay such an issue that theorem's constructor is not protected because the proof db holds the only reference.\n",
    "@dataclass(frozen=True)\n",
    "class Theorem():\n",
    "    index: int\n",
    "    proofdb: ProofDB\n",
    "    def formula(self):\n",
    "        return self.proofdb[self.index][0]\n",
    "    def cnf(self):\n",
    "        return self.formula().cnf()\n",
    "    def __repr__(self):\n",
    "        return f\"Theorem({self.formula()})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some example usage. A simple involutive problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(f(X)) == f(X)=f(f(X)) = f(X)\n",
      "type(f(f(X)))=<class '__main__.Fn'>\n",
      "involute_f=Theorem([] |- [f(f(X)) = f(X)])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "X,Y,Z,A,B,C = Vars(\"X Y Z A B C\")\n",
    "f = Function(\"f\")\n",
    "\n",
    "db = ProofDB()\n",
    "\n",
    "print(f\"{f(f(X)) == f(X)=}\")\n",
    "print(f\"{type(f(f(X)))=}\")\n",
    "involute_f = db.assume(f(f(X)) == f(X))\n",
    "print(f\"{involute_f=}\")\n",
    "print(all(list(isinstance(h, Theorem) for h in [involute_f])))\n",
    "thm1 = db.infer([involute_f] , f(f(f(f(X)))) == f(X))\n",
    "#thm2 = db.infer([involute_f],  f(X) == X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two simple theorems of group theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theorem([] |- [mul(inv(X), X) = e()]) Theorem([] |- [mul(e(), X) = X])\n"
     ]
    }
   ],
   "source": [
    "mul = Function(\"mul\")\n",
    "inv = Function(\"inv\")\n",
    "e = Const(\"e\")\n",
    "\n",
    "db = ProofDB()\n",
    "right_inv = db.assume(mul(X, inv(X)) == e)\n",
    "right_id = db.assume(mul(X, e) == X)\n",
    "assoc = db.assume(mul(mul(X, Y), Z) == mul(X, mul(Y, Z)))\n",
    "\n",
    "group_ax = [right_inv, right_id, assoc]\n",
    "\n",
    "left_inv = db.infer(group_ax, mul(inv(X), X) == e)\n",
    "left_id = db.infer(group_ax, mul(e, X) == X)\n",
    "#comm = db.infer(group_ax, mul(X, Y) == mul(Y, X))\n",
    "print(left_inv, left_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bits and Bobbles\n",
    "Next I probably want the typed form of tptp supported and it is especially appealing to use `thf` which supports lambdas.\n",
    "\n",
    "I wonder if I'm inching towards just building python versions of everything. Unification, simple resolution provers. We'll see. It's nice to have that stuff in the library if I do. \n",
    "\n",
    "I probably also should just make dataclasses for special logical formula constructos like ForAll, Exists, And, Or, Implies, etc. \n",
    "\n",
    "HOLpy is a tour de force. They should've made it a library though. Making their own fronted seems like a bad battle to pick, but what do I know.\n",
    "\n",
    "Isabelle is so good.\n",
    "\n",
    "There is probably a clean prolog version of this. asserta keyed theorems to a proof_db protected by assume/infer predicates. Lambda prolog and Amy Felty's tutorial. I'm not sure why I am unaware of something like this in the modern prolog community. The new python interop \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peano\n",
    "db = ProofDB()\n",
    "N, M = Vars(\"N M\")\n",
    "\n",
    "# definition of nats\n",
    "zero = Const(\"zero\")\n",
    "succ = Function(\"succ\")\n",
    "succ_inject =   db.assume(Sequent([succ(N) == succ(M)], [N == M]))\n",
    "succ_distinct = db.assume(Sequent([succ(N) == zero], []))\n",
    "# induction without quantifiers PRA style\n",
    "def induction(n, formula):\n",
    "    base = db.infer([], subst(formula, n, zero))\n",
    "    step = db.infer([], Sequent([formula], [subst(formula, n, succ(n))]))\n",
    "    return db.assume(formula)\n",
    "# definition of plus\n",
    "plus = Function(\"plus\")\n",
    "db.assume(plus(zero, N) == N)\n",
    "plus = Function(\"plus\")\n",
    "db.assume(plus(succ(N), M) == succ(plus(N, M)))\n",
    "#induction(N, plus(zero, N) == N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set theory\n",
    "empty = Const(\"empty\")\n",
    "def comprehension(n, formula):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'L' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m List \u001b[38;5;241m=\u001b[39m Function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m list_constuct \u001b[38;5;241m=\u001b[39m db\u001b[38;5;241m.\u001b[39massume([List(\u001b[43mL\u001b[49m)], [L \u001b[38;5;241m==\u001b[39m cons(hd(L),tl(L)), L \u001b[38;5;241m==\u001b[39m nil])\n\u001b[1;32m      5\u001b[0m hd_cons \u001b[38;5;241m=\u001b[39m db\u001b[38;5;241m.\u001b[39massume([List(Y)], [ hd(cons(X,Y)) \u001b[38;5;241m==\u001b[39m X]) \u001b[38;5;66;03m# injectivity\u001b[39;00m\n\u001b[1;32m      6\u001b[0m tl_cons \u001b[38;5;241m=\u001b[39m db\u001b[38;5;241m.\u001b[39massume([List(Y)], [ tl(cons(X,Y)) \u001b[38;5;241m==\u001b[39m Y])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'L' is not defined"
     ]
    }
   ],
   "source": [
    "# adts. Painful in untyped. tptp doesn't have support?\n",
    "List = Function(\"list\")\n",
    "\n",
    "list_constuct = db.assume([List(L)], [L == cons(hd(L),tl(L)), L == nil])\n",
    "\n",
    "hd_cons = db.assume([List(Y)], [ hd(cons(X,Y)) == X]) # injectivity\n",
    "tl_cons = db.assume([List(Y)], [ tl(cons(X,Y)) == Y])\n",
    "\n",
    "#db.assume([List(X), List(Y), X == Y], [hd(X)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typed terms.\n",
    "It is nice / good sanity checking to give terms types.\n",
    "It also gives us access to intrinsic reasoning about ints etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "class Type:\n",
    "    pass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class STVar(Type):\n",
    "    name: str\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SVar(Type):\n",
    "    name: str\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TConst(Type):\n",
    "    name: str\n",
    "    args: tuple[Type] = ()\n",
    "\n",
    "def TFun(*args):\n",
    "    if len(args) == 1:\n",
    "        return args[0]\n",
    "    else:\n",
    "        return TConst('->', (args[0], TFun(*args[1:])))\n",
    "\n",
    "\n",
    "BoolType = TConst(\"bool\")\n",
    "NatType = TConst('nat')\n",
    "IntType = TConst('int')\n",
    "RealType = TConst('real')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
