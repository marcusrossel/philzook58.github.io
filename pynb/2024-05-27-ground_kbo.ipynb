{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Ground Knuth Bendix Ordering is Basically Size with Tie Breaking\n",
    "date: 2024-05-27\n",
    "---\n",
    "\n",
    "A natural and interesting topic is the idea of a Term ordering.\n",
    "\n",
    "[Terms](https://en.wikipedia.org/wiki/Term_(logic)) are syntax trees that we can throw different features into. An example of a term may be an arithmetic expression like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('+', [('*', [('lit', 2), ('lit', 42)]), ('lit', 3)])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (2 * 42) + 3\n",
    "(\"+\", [(\"*\", [(\"lit\", 2), (\"lit\",42)]), (\"lit\", 3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we want to compare terms? Well, having an ordering on terms enables the use of datastructures like balanced search trees.\n",
    "\n",
    "But also perhaps more crucially, it is a mathematical way of describing the notion of \"simpler\".\n",
    "\n",
    "The most basic and obvious notion of simpler is to count the number of nodes in the term. Its size.\n",
    "\n",
    "For some term rewriting simplifiers we may write, we can know thew thing isn't going to loop (not terminate) if it always makes a little progress into a strictly simpler term. Presumably any reasonable order does not an infinite chain of simpler terms, so the rewriter has to stop eventually. We get rid of extra `0`, constant propagate, fuse out things like `-(-x) -> x` or `x - x -> 0`. All of these rules make smaller terms so we can't loop forever.\n",
    "\n",
    "If we want to show that a particular term rewriting system has this nontermination property, we need to find a term ordering. This term ordering has to comply with the fact that the left and right hand side of rules contain pattern variables, which may stand in for any term. This puts pretty strong and complicated constraints on the term ordering.\n",
    "\n",
    "I find the definition of the knuth bendix ordering pretty confusing. And yet, when you consider the ground case, it all becomes much simpler and more obvious. This is a general rule of thumb for understand many techniques in automated reasoning. The ground stuff is a subcase and much easer.\n",
    "\n",
    "![knuth bendix ordering](/assets/traat/kbo.png)\n",
    "\n",
    "\n",
    "1. KBO1 says that if the term sizes are different, the bigger one is larger.\n",
    "2. KBO2 is the tiebreaking rules for equal sized terms. 2a is not relevant to the ground case. 2b says to break ties using the head symbol. 2c says to recurse lexicographically (compare in order) into the arguments.\n",
    "\n",
    "Pretty straightforward actually. Compare by size. Tie break by head symbol and then recurse into arguments.\n",
    "\n",
    "Here I implement this over z3 asts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from z3 import *\n",
    "Order = Enum(\"Order\", [\"LT\", \"EQ\", \"NGE\", \"GT\"])\n",
    "def kbo(t, r):\n",
    "    pass\n",
    "\n",
    "def size(t:ExprRef):\n",
    "    return 1 + sum(size(x) for x in t.children())\n",
    "def ground_kbo(t1, t2):\n",
    "    if t1.eq(t2): # optimization\n",
    "        return Order.EQ\n",
    "    s1 = size(t1)\n",
    "    s2 = size(t2)\n",
    "    if s1 < s2:\n",
    "        return Order.LT\n",
    "    elif s1 > s2:\n",
    "        return Order.GT\n",
    "    else:\n",
    "        if t1.num_args() < t1.num_args():\n",
    "            return Order.LT\n",
    "        elif t2.num_args() > t2.num_args():\n",
    "            return Order.GT\n",
    "        n1, n2 = t1.decl().name(), t2.decl().name()\n",
    "        if n1 < n2:\n",
    "            return Order.LT\n",
    "        elif n1 > n2:\n",
    "            return Order.GT\n",
    "        else:\n",
    "            for x,y in zip(t1.children(), t2.children()):\n",
    "                o = ground_kbo(x,y)\n",
    "                if o != Order.EQ:\n",
    "                    return o\n",
    "            assert False, \"unreachable\"\n",
    "\n",
    "\n",
    "x,y,z = Ints(\"x y z\")\n",
    "assert ground_kbo(x,x) == Order.EQ\n",
    "assert ground_kbo(x,y) == Order.LT\n",
    "assert ground_kbo(y,y) == Order.EQ\n",
    "assert ground_kbo(x + z, x + y) == Order.GT\n",
    "assert ground_kbo((x + x) + x, x + (x + x)) == Order.GT\n",
    "assert ground_kbo((z + z) + z, x * x) == Order.GT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this interesting? Well, one reason is that it let's you build an egraph. Completion over ground equations is the same thing as congruence closure. Completion for ground equations is guaranteed to terminate because you only produce terms smaller than the largest one you started with. Eventually you run out of terms.\n",
    "Critical pairs for ground terms is merely finding a one term as a subterm in another.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bits and Bobbles\n",
    "\n",
    "Working over z3 asts I think is a good idea. It let's me bop in and out of z3 and complies with thew current direction I'm going in knuckledragger. Z3 is a vast raft of functionality.\n",
    "\n",
    "\n",
    "https://www.cs.miami.edu/home/geoff/Courses/TPTPSYS/FirstOrder/SyntacticRefinements.shtml knuht bendix is not ground completable?\n",
    "https://www.cs.unm.edu/~mccune/prover9/manual/2009-11A/term-order.html prover 9 term orderings\n",
    "\n",
    " https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7324232/ A Knuth-Bendix-Like Ordering for Orienting Combinator Equations\n",
    "\n",
    " lambda free kbo https://www.isa-afp.org/browser_info/current/AFP/Lambda_Free_KBOs/document.pdf\n",
    " https://arxiv.org/abs/2005.02094"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## The default \"tuple\" ordering on terms.\n",
    "Python's default for tuples is to compare them lexicographically. This corresponds to first comparing head symbols, then recursively comparing the arguments.\n",
    "This ordering is total.\n",
    "It isn't great in that it isn't a simplification ordering. A subterm can be larger than the term itself. Ok, why is that a problem?\n",
    "Well, it's more obviously a problem if terms have variables in them, since to stay consistent we need to be able to substitute in any possible term. If we stick to ground terms though, this is not such a problem.\n",
    "It does run contrary to the idea that \"size\" is a good measure of simplicity.\n",
    "\n",
    "An arbitrary (well founded) ordering is basically ok on ground terms because you don't care as much about things like substitution. The typical union find exploits this to find an arbitrary online ordering that keeps the union find kind of flat.\n",
    "\n",
    "## Path Ordering\n",
    "\n",
    "Harrison points out that LPO is basically constructed to have the substitution property. That is why we do a recursive check first.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Path_ordering_(term_rewriting)\n",
    "\n",
    "```python\n",
    "def ground_lpo(s,t, vars=[]):\n",
    "    if t in vars: #LPO1\n",
    "        return t in s\n",
    "    \n",
    "    f = s.decl().name()\n",
    "    g = t.decl().name()\n",
    "    return any(ground_lpo(si, t) for si in s.children()) or \\ #LPO2a\n",
    "    all(ground_lpo(s, ti) for ti in t.children()) and \\ #\n",
    "    (f > g or # LPO2b no. I need to use Order.EQ Order.LT. This is close though. Oh well.\n",
    "    f == g and any(ground_lpo(si,ti) for si, ti in zip(s.children(), t.children()))) #LPO2c\n",
    "```\n",
    "\n",
    "Ordering modulo theories. Like Hashing modulo theories, if you want to use an AVL tree to store some things that respect some equations, how do you do it? Set orderings and multiset orderings. Again can basically sort them and then use a lexicographic ordering.\n",
    "\n",
    "How do you order alpha equiv terms? trvaversal order labelling or \"tree maps\" a la co de bruijn / hash cons modulo alpha.\n",
    "\n",
    "In this direction lies alpha egraphs. nominal matching.\n",
    "\n",
    "\n",
    "https://link.springer.com/content/pdf/10.1007/3-540-17220-3_4.pdf How to choose the weights in the Knuth Bendix ordering -- ursula martin\n",
    "\n",
    "https://academic.oup.com/comjnl/article/34/1/2/427931 An Introduction to Knuthâ€“Bendix Completion Ajj dick\n",
    "\n",
    "https://link.springer.com/article/10.1007/s10817-006-9031-4  Things to Know when Implementing KBO\n",
    "\n",
    "We have a notion that simplifiers must terminate because terms get simpler until they can't\n",
    "\n",
    "\"size\" is a basic notion of complexity.\n",
    "\n",
    "In order to guarantee a rule with variables gets smaller, we can count the number of symbols in it. The number of pattern variables has to be getting smaller, or else the thing is getting bigger.\n",
    "\n",
    "`(X + 0) -> X` makes the term simpler\n",
    "\n",
    "`foo(bar(1,2,3,4,5,6,X)) -> biz(X,X)` is not definitely a simplification, because X might be huge. In a realistic situation, we might want to guard this rule by `size(X) < 3` or something.\n",
    "\n",
    "A slight extension is to give the symbols in the term weight.\n",
    "\n",
    "Another example is associating to the right. X + (Y + Z) -> (X + Y) + Z. This also obviously terminates. We can give lexicographic preference to left subtrees.\n",
    "\n",
    "\n",
    "Another example is getting flatter. The height is descreasing.\n",
    "\n",
    "distrbutivity is another common case. x*(y + z) -> x*y + x*z. This is getting bigger _and_ duplicating x. That's a big nono for most term orderings. But clearly foiling is terminating.\n",
    "\n",
    "A different system that is obviously terminating is stratified macro expansion. Macros can refer to previously defined macros. The terms however, may get very large and be increasing in size.\n",
    "f(X) -> bar(X,X) is not an issue.\n",
    "Cycles in the dependency graph\n",
    "\n",
    "We don't expect to be able to have a total ordering on terms with variables in them. But we do want our definition to have a well defined\n",
    "\n",
    "A primitive system that is obviously terminating are systems of the form `f(x,y,z,...) -> z` This is a view on the homeomorphic embedding relation.\n",
    "\n",
    "right ground systems\n",
    "\n",
    "Many functional programs are \"obviously\" terminating even if we don't think of them in these terms\n",
    "\n",
    "Negation normal form pushing\n",
    "\n",
    "\n",
    "What about naive breadth first search:\n",
    "Separate into confluent terminating rules\n",
    "and naughty rules. (maude makes this disctinction but do so dynamically)\n",
    "\n",
    "Hash cons only things that are normalized with respect to current rules.\n",
    "run twee for a bit, extract oriented rules, confluence them (?)\n",
    "Use the others for search.\n",
    "\n",
    "TRS/SK90 Joachim Steinbach, Ulrich KÃ¼hler: Check your Ordering - termination proofs and open Problems, Technical Report SR-90-25, UniversitÃ¤t Kaiserslautern, 1990.\n",
    "TRS/D33 Nachum Dershowitz: 33 Examples of Termination, 1995 Proc. French Spring School of Theoretical Computer Science, LNCS 909, http://www.math.tau.ac.il/~nachumd/papers/printemp-print.pdf\n",
    "TRS/AG01 Thomas Arts, JÃ¼rgen Giesl: Termination of term rewriting using dependency pairs, 2000, http://dblp.uni-trier.de/rec/bibtex/journals/tcs/ArtsG00 http://verify.rwth-aachen.de/giesl/papers/ibn-97-46.ps\n",
    "SRS/Zantema 128 string rewriting termination problems collected by Hans Zantema (2004?). They include (as z027 .. z064) a set of one-rule termination problems by Alfons Geser (Habilitationsschrift, TÃ¼bingen, 2001) and possibly Winfried Kurth (Dissertation, Clausthal, 1990)\n",
    "\n",
    "disjunctive normal form example. Makes term bigger.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction Cost is Knuth Bendix Ordering Weights\n",
    "\n",
    "consider ground extraction\n",
    "\n",
    "elpi knuth bendix\n",
    "https://www.cl.cam.ac.uk/~jrh13/atp/ harrison's code\n",
    "\n",
    "Knuth bendix + grobner\n",
    "Can we do it? It'd be cool https://www3.risc.jku.at/publications/download/risc_3528/paper_48.pdf Knuth-bendix procedure and buchbergher - a synthesis\n",
    "\n",
    "\n",
    "\n",
    "naive rewriting search engine. Keep score. \n",
    "Greedy.\n",
    "Then\n",
    "Expansive\n",
    "\n",
    "import swipy\n",
    "import requests\n",
    "r = requests.get('https://www.metalevel.at/trs/trs.pl')\n",
    "\n",
    "swipy.load_module(r.body)\n",
    "\n",
    "`pip install janus_swi`\n",
    "\n",
    "\n",
    "```python\n",
    "%%file /tmp/trs.pl\n",
    "% based on markus triska's trs\n",
    ":- op(800, xfx, ==>).\n",
    "step([L==>R|Rs], T0, T) :-\n",
    "        (   subsumes_term(L, T0) ->\n",
    "            copy_term(L-R, T0-T)\n",
    "        ;   step(Rs, T0, T)\n",
    "        ).\n",
    "\n",
    "normal_form(Rs, T0, T) :-\n",
    "        (   var(T0) -> T = T0\n",
    "        ;   T0 =.. [F|Args0],\n",
    "            maplist(normal_form(Rs), Args0, Args1),\n",
    "            T1 =.. [F|Args1],\n",
    "            (   step(Rs, T1, T2) ->\n",
    "                normal_form(Rs, T2, T)\n",
    "            ;   T = T1\n",
    "            )\n",
    "        ).\n",
    "\n",
    "term_to_list(T, L) :-\n",
    "        (   var(T) -> L = [T]\n",
    "        ;   T =.. [F|Args],\n",
    "            maplist(term_to_list, Args, ArgsL),\n",
    "            L = [F|ArgsL]\n",
    "        ).\n",
    "```\n",
    "\n",
    "    Overwriting /tmp/trs.pl\n",
    "\n",
    "\n",
    "% convert prolog term to lists of lists\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "# https://www.swi-prolog.org/pldoc/doc_for?object=section(%27packages/janus.html%27)\n",
    "import janus_swi as janus\n",
    "janus.consult(\"/tmp/trs.pl\")\n",
    "ans = janus.query_once(\"normal_form([f(x) ==> g(x)], f(x), _Res), term_to_list(_Res,Res).\")\n",
    "ans['Res']\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ['g', ['x']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "https://lpcic.github.io/coq-elpi/tutorial_elpi_lang.html\n",
    "https://github.com/LPCIC/elpi/blob/master/src/builtin.elpi\n",
    "\n",
    "\n",
    "```python\n",
    "%%file /tmp/trs.elpi\n",
    "kind term type.\n",
    "\n",
    "%infixr ==> 50.\n",
    "%type (==>)   tye -> tye -> tye.\n",
    "type fn string -> list term -> term.\n",
    "type lam : (term -> term) -> term.\n",
    "\n",
    "kind rule type.\n",
    "type rule : term -> term -> rule.\n",
    "\n",
    "pred normal : list rule,  term, term.\n",
    "\n",
    "```\n",
    "\n",
    "    Writing /tmp/trs.elpi\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "! elpi -exec 'normal (fn \"a\" []) Ans' /tmp/trs.elpi\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "class Fn():\n",
    "    cost: int\n",
    "    hash: int\n",
    "    head: str\n",
    "    args: List['Fn']\n",
    "\n",
    "\n",
    "def ground_kbo(t1,t2):\n",
    "    if t1.cost < t2.cost:\n",
    "        return True\n",
    "    elif t1.cost > t2.cost:\n",
    "        return False\n",
    "    elif t1.head < t2.head:\n",
    "        return True\n",
    "    elif t1.head > t2.head:\n",
    "        return False\n",
    "    else:\n",
    "        for arg1, arg2 in zip(t1.args ,t2.args):\n",
    "            if arg1 == arg2:\n",
    "                continue\n",
    "            return ground_kbo(arg1, arg2)\n",
    "            \n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "import requests\n",
    "res = requests.get(\"https://www.metalevel.at/trs/trs.pl\")\n",
    "with open(\"/tmp/trs.pl\", \"w\") as f:\n",
    "    f.write(res.text)\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "    ERROR: /tmp/trs.pl:67:\n",
    "    ERROR:    source_sink `library(clpz)' does not exist\n",
    "    Warning: /tmp/trs.pl:67:\n",
    "    Warning:    Goal (directive) failed: user:use_module(library(clpz))\n",
    "    ERROR: /tmp/trs.pl:69:\n",
    "    ERROR:    source_sink `library(dcgs)' does not exist\n",
    "    Warning: /tmp/trs.pl:69:\n",
    "    Warning:    Goal (directive) failed: user:use_module(library(dcgs))\n",
    "    ERROR: /tmp/trs.pl:71:\n",
    "    ERROR:    source_sink `library(iso_ext)' does not exist\n",
    "    Warning: /tmp/trs.pl:71:\n",
    "    Warning:    Goal (directive) failed: user:use_module(library(iso_ext))\n",
    "    ERROR: /tmp/trs.pl:72:\n",
    "    ERROR:    source_sink `library(format)' does not exist\n",
    "    Warning: /tmp/trs.pl:72:\n",
    "    Warning:    Goal (directive) failed: user:use_module(library(format))\n",
    "    ERROR: /tmp/trs.pl:261:23: Syntax error: Operator expected\n",
    "    ERROR: /tmp/trs.pl:269:9: Syntax error: Operator expected\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "let rec termsize tm =\n",
    "  match tm with\n",
    "  | Fn(f,args) -> itlist (fun t n -> termsize t + n) args 1;;\n",
    "    Var x -> 1\n",
    "\"\"\"\n",
    "def termsize(tm):\n",
    "    match tm:\n",
    "        case (\"Var\", x):\n",
    "            return 1\n",
    "        case (\"Fn\", f, args):\n",
    "            return sum(map(termsize, args)) + 1\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "class Term:\n",
    "    pass\n",
    "@dataclass\n",
    "class Fn(Term):\n",
    "    f: str\n",
    "    args: Tuple[Term]\n",
    "\n",
    "@dataclass\n",
    "class Var(Term):\n",
    "    name: str\n",
    "\n",
    "def Function(fn):\n",
    "    def res(*args):\n",
    "        return Fn(fn, args)\n",
    "    return res\n",
    "def Vars(xs):\n",
    "    return [Var(x) for x in xs.split()]\n",
    "def Functions(fns):\n",
    "    return [Function(fn) for fn in fns.split()]\n",
    "\n",
    "import enum\n",
    "\n",
    "# traat\n",
    "# harrison\n",
    "# norvig\n",
    "# eli bendersky\n",
    "\n",
    "def unify(x, y, subst):\n",
    "    \"\"\"Unifies term x and y with initial subst.\n",
    "\n",
    "    Returns a subst (map of name->term) that unifies x and y, or None if\n",
    "    they can't be unified. Pass subst={} if no subst are initially\n",
    "    known. Note that {} means valid (but empty) subst.\n",
    "    \"\"\"\n",
    "    if subst is None:\n",
    "        return None\n",
    "    elif x == y:\n",
    "        return subst\n",
    "    elif isinstance(x, Var):\n",
    "        return unify_variable(x, y, subst)\n",
    "    elif isinstance(y, Var):\n",
    "        return unify_variable(y, x, subst)\n",
    "    elif isinstance(x, Fn) and isinstance(y, Fn):\n",
    "        if x.f != y.f or len(x.args) != len(y.args):\n",
    "            return None\n",
    "        else:\n",
    "            for i in range(len(x.args)):\n",
    "                subst = unify(x.args[i], y.args[i], subst)\n",
    "            return subst\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def unify_variable(v, x, subst):\n",
    "    \"\"\"Unifies variable v with term x, using subst.\n",
    "\n",
    "    Returns updated subst or None on failure.\n",
    "    \"\"\"\n",
    "    assert isinstance(v, Var)\n",
    "    if v.name in subst:\n",
    "        return unify(subst[v.name], x, subst)\n",
    "    elif isinstance(x, Var) and x.name in subst:\n",
    "        return unify(v, subst[x.name], subst)\n",
    "    elif occurs_check(v, x, subst):\n",
    "        return None\n",
    "    else:\n",
    "        # v is not yet in subst and can't simplify x. Extend subst.\n",
    "        return {**subst, v.name: x}\n",
    "\n",
    "def occurs_check(v, term, subst):\n",
    "    \"\"\"Does the variable v occur anywhere inside term?\n",
    "\n",
    "    Variables in term are looked up in subst and the check is applied\n",
    "    recursively.\n",
    "    \"\"\"\n",
    "    assert isinstance(v, Var)\n",
    "    if v == term:\n",
    "        return True\n",
    "    elif isinstance(term, Var) and term.name in subst:\n",
    "        return occurs_check(v, subst[term.name], subst)\n",
    "    elif isinstance(term, Fn):\n",
    "        return any(occurs_check(v, arg, subst) for arg in term.args)\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "assert unify(Var(\"x\"), Var(\"y\"), {}) == {\"x\": Var(\"y\")}\n",
    "assert unify(Var(\"x\"), Var(\"x\"), {}) == {}\n",
    "assert unify(Var(\"x\"), Fn(\"f\", [Var(\"y\")]), {}) == {\"x\": Fn(\"f\", [Var(\"y\")])}\n",
    "assert unify(Fn(\"f\", [Var(\"x\")]), Fn(\"f\", [Var(\"y\")]), {}) == {\"x\": Var(\"y\")}\n",
    "assert unify(Fn(\"f\", [Var(\"x\")]), Fn(\"g\", [Var(\"y\")]), {}) == None\n",
    "\n",
    "# hmm. This is assuming \"x\" and \"x\" in the lhs rhs are the same thing.\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "def critical_pairs(f,g):\n",
    "    if isinstance(f, Var) or isinstance(g, Var):\n",
    "        return\n",
    "    elif f.f == g.f:\n",
    "        subst = unify(f,g,{})\n",
    "        if subst is not None:\n",
    "            yield subst, f, g\n",
    "    # This is double counting\n",
    "    for arg in f.args:\n",
    "        yield from critical_pairs(arg, g)\n",
    "    for arg in g.args:\n",
    "        yield from critical_pairs(f, arg)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "list(critical_pairs(Fn(\"f\", [Var(\"x\")]), Fn(\"f\", [Var(\"y\")])))\n",
    "list(critical_pairs(Fn(\"f\", [Var(\"x\")]), Fn(\"g\", [Var(\"y\")])))\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "def _match(e,pat,subst):\n",
    "    match pat, e:\n",
    "        case (Var(name), _):\n",
    "            if pat.name in subst:\n",
    "                if subst[pat.name] == e:\n",
    "                    return subst\n",
    "                else:\n",
    "                    return None\n",
    "            else:\n",
    "                return {**subst, pat.name: e}\n",
    "        case (Fn(f,args), Fn(f1,args1)):\n",
    "            if f != f1 or len(args) != len(args1):\n",
    "                return None\n",
    "            else:\n",
    "                for a1,a2 in zip(args,args1):\n",
    "                    subst = _match(a1,a2,subst)\n",
    "                    if subst == None:\n",
    "                        return None\n",
    "                return subst\n",
    "        case _:\n",
    "            raise ValueError(f\"Can't match {pat} with {e}\")\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "recexpr\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "let rec lexord ord l1 l2 =\n",
    "  match (l1,l2) with\n",
    "    (h1::t1,h2::t2) -> if ord h1 h2 then length t1 = length t2\n",
    "                       else h1 = h2 & lexord ord t1 t2\n",
    "  | _ -> false;;\n",
    "\"\"\"\n",
    "def lexord(ord, l1, l2):\n",
    "    match (l1, l2):\n",
    "        case ([h1,*t1], [h2, *t2]):\n",
    "            if ord(h1, h2): # yeah I dunno about this. Is this a faithful rep of ord\n",
    "                return len(t1) == len(t2)\n",
    "            else:\n",
    "                return h1 == h2 and lexord(ord,t1, t2)\n",
    "        case _:\n",
    "            return False\n",
    "\n",
    "lexord([1,5], [1,4])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "let rec lpo_gt w s t =\n",
    "  match (s,t) with\n",
    "    (_,Var x) ->\n",
    "        not(s = t) & mem x (fvt s)\n",
    "  | (Fn(f,fargs),Fn(g,gargs)) ->\n",
    "        exists (fun si -> lpo_ge w si t) fargs or\n",
    "        forall (lpo_gt w s) gargs &\n",
    "        (f = g & lexord (lpo_gt w) fargs gargs or\n",
    "         w (f,length fargs) (g,length gargs))\n",
    "  | _ -> false\n",
    "\"\"\"\n",
    "def lpo_gt(w,s,t):\n",
    "    match (s,t):\n",
    "        case (_, (\"Var\", x)):\n",
    "            return s != t and x in fvt(s)\n",
    "        case ((\"Fn\", f, fargs), (\"Fn\", g, gargs)):\n",
    "            return any(lpo_ge(w, si, t) for si in fargs) or \\\n",
    "                all(lpo_gt(w, s, si) for si in gargs) and \\\n",
    "                (f == g and lexord(lambda s,t: lpo_gt(w,s,t), fargs, gargs) or \\\n",
    "                 w(f, len(fargs), g, len(gargs)))\n",
    "        case _:\n",
    "            return False\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "def size(t):\n",
    "    match t:\n",
    "        case Var(x):\n",
    "            return 1\n",
    "        case Fn(f, args):\n",
    "            return 1 + sum(map(size, args))\n",
    "\n",
    "def vars(t):\n",
    "    match t:\n",
    "        case Var(x):\n",
    "            yield x\n",
    "        case Fn(f, args):\n",
    "            for arg in args:\n",
    "                yield from vars(arg)\n",
    "from collections import Counter # a multiset type\n",
    "\n",
    "def var_comp_lte(t1,t2):\n",
    "    # We don't want to have more vars on lhs than rhs\n",
    "    c1 = Counter(vars(t1))\n",
    "    c2 = Counter(vars(t2))\n",
    "    for k in c2:\n",
    "        if c1[k] < c2[k]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "def kbo2a(t1,t2):\n",
    "    if not isinstance(t2, Var):\n",
    "        return False\n",
    "    # check if t1 is iteerated functions symbol f over variable t2\n",
    "    if not isinstance(t1, Fn):\n",
    "        return False\n",
    "    fn = t1.f\n",
    "    def check(t):\n",
    "        if isinstance(t, Var):\n",
    "            return t == t2\n",
    "        elif isinstance(t, Fn):\n",
    "            return t.f == fn and len(t.args) == 1 and check(arg[0])\n",
    "        else:\n",
    "            raise ValueError(\"Not a term\")\n",
    "    return check(t1)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "%%file /tmp/kbo_test.p\n",
    "cnf(test, axiom, f = g).\n",
    "cnf(false, conjecture, true = false).\n",
    "\n",
    "```\n",
    "\n",
    "    Overwriting /tmp/kbo_test.p\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "!twee /tmp/kbo_test.p --precedence g,f\n",
    "```\n",
    "\n",
    "    Here is the input problem:\n",
    "      Axiom 1 (test): f = g.\n",
    "      Goal 1 (false): true = false.\n",
    "    \n",
    "    1. g -> f\n",
    "    \n",
    "    Ran out of critical pairs. This means the conjecture is not true.\n",
    "    Here is the final rewrite system:\n",
    "      g -> f\n",
    "    \n",
    "    RESULT: CounterSatisfiable (the conjecture is false).\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "!twee /tmp/kbo_test.p --precedence f,g\n",
    "```\n",
    "\n",
    "    Here is the input problem:\n",
    "      Axiom 1 (test): f = g.\n",
    "      Goal 1 (false): true = false.\n",
    "    \n",
    "    1. f -> g\n",
    "    \n",
    "    Ran out of critical pairs. This means the conjecture is not true.\n",
    "    Here is the final rewrite system:\n",
    "      f -> g\n",
    "    \n",
    "    RESULT: CounterSatisfiable (the conjecture is false).\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "def kbo(t1,t2):\n",
    "    if not var_comp_lte(t1,t2):\n",
    "        return True\n",
    "    w1 = size(t1)\n",
    "    w2 = size(t2)\n",
    "    if w1 > w2:\n",
    "        return True\n",
    "    elif w1 < w2:\n",
    "        return False\n",
    "    else:\n",
    "         if kbo2a(t1,t2):\n",
    "             return True\n",
    "         elif t2.f < t1.f:   #kbo2b\n",
    "            return True\n",
    "        elif: #kbo2c\n",
    "            \n",
    "        else:\n",
    "            return None\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "%%bash\n",
    "\n",
    "dedu\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "# naive\n",
    "\n",
    "def rw(t):\n",
    "    match t:\n",
    "        case (\"+\", 0, x):\n",
    "            return x\n",
    "        case x: \n",
    "            return (\"+\", 0, x)\n",
    "        case (\"+\", x, 0):\n",
    "            return x\n",
    "        case x:\n",
    "            return (\"+\", x, 0)\n",
    "        case (\"+\", (\"-\", x), x):\n",
    "            return 0\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "def expand(t):\n",
    "    for (lhs,rhs) in rules:\n",
    "        yield from apply(lhs,rhs,t)\n",
    "        yield from apply(rhs,lhs,t)\n",
    "\n",
    "def prove(lhs,rhs):\n",
    "    lhsseen = set(lhs)\n",
    "    rhsseen = set(rhs)\n",
    "    while lhsseen & rhsseen == set():\n",
    "        for t in expand(lhsseen):\n",
    "            if t in rhsseen:\n",
    "                return True\n",
    "        for t in expand(rhsseen):\n",
    "            if t in lhsseen:\n",
    "                return True\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "%%file /tmp/eq.p\n",
    "cnf(axiom1, axiom, a = b).\n",
    "cnf(axiom2, axiom, b = c).\n",
    "cnf(axiom3, axiom, c = d).\n",
    "thf(p_decl, type, p : $i > $i).\n",
    "thf(qdecl, type, q : $i > $i).\n",
    "thf(axiom4, axiom, (^[X: $i] : X) = p).\n",
    "\n",
    "```\n",
    "\n",
    "    Overwriting /tmp/eq.p\n",
    "\n",
    "\n",
    "can turn decl = rewrite rule.\n",
    "maybe the special syntax is kind of nice. I don't like thf that much\n",
    "\n",
    "https://github.com/sneeuwballen/zipperposition/blob/master/examples/ind/list.zf\n",
    "https://github.com/sneeuwballen/zipperposition/blob/master/examples/ind/tree.zf\n",
    "\n",
    "`val[AC] plus : term -> term -> term.`\n",
    "val to declare\n",
    "assert for axioms\n",
    "rewrite\n",
    "data definitions\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "!zipperposition /tmp/eq.p #--dot /tmp/eq.dot --debug 10\n",
    "```\n",
    "\n",
    "    # done 4 iterations in 0.002s\n",
    "    % Final clauses: 4\n",
    "    Clauses:\n",
    "    [a = b*]/id:0/depth:0/penalty:1/red:false\n",
    "    \n",
    "    [b = c*]/id:1/depth:0/penalty:1/red:false\n",
    "    \n",
    "    forall X0. [X0 = p X0*]/id:3/depth:0/penalty:1/red:false\n",
    "    \n",
    "    [b = d*]/id:4/depth:0/penalty:1/red:false\n",
    "    # SZS status Satisfiable for '/tmp/eq.p'\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "%%file /tmp/test.zf\n",
    "val term : type.\n",
    "\n",
    "data nat :=\n",
    "  | z\n",
    "  | s nat.\n",
    "\n",
    "def[infix \"+\"] plus : nat -> nat -> nat where\n",
    "  forall (X:nat). ((plus z X) = X);\n",
    "  forall (X:nat). (forall (Y:nat). ((plus (s X) Y) = (s (plus X Y)))).\n",
    "\n",
    "def[infix \"-\"] minus : nat -> nat -> nat where\n",
    "  forall (X:nat). ((minus X z) = X);\n",
    "  forall X. minus z X = z;\n",
    "  forall (X:nat). (forall (Y:nat). ((minus (s X) (s Y)) = (minus X Y))).\n",
    "\n",
    "def[infix \"<\"] less : nat -> nat -> prop where\n",
    "  forall (X:nat). (less z (s X));\n",
    "  forall X. ~ (less (s X) z);\n",
    "  forall (X:nat). (forall (Y:nat). ((less (s X) (s Y)) <=> (less X Y))).\n",
    "\n",
    "def[infix \"â‰¤\"] leq : nat -> nat -> prop where\n",
    "  forall (X:nat). (leq z X);\n",
    "  forall X. ~ (leq (s X) z);\n",
    "  forall (X:nat). (forall (Y:nat). ((leq (s X) (s Y)) <=> (leq X Y))).\n",
    "\n",
    "```\n",
    "\n",
    "    Overwriting /tmp/test.zf\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "!zipperposition /tmp/test.zf\n",
    "```\n",
    "\n",
    "    # done 0 iterations in 0.003s\n",
    "    % Final clauses: 0\n",
    "    # SZS status GaveUp for '/tmp/test.zf'\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "%%file /tmp/one.zf\n",
    "data unit := One.\n",
    "\n",
    "goal forall (x y: unit). x = y.\n",
    "```\n",
    "\n",
    "    Writing /tmp/one.zf\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "!zipperposition /tmp/one.zf\n",
    "```\n",
    "\n",
    "    # done 2 iterations in 0.003s\n",
    "    # SZS status Theorem for '/tmp/one.zf'\n",
    "    # SZS output start Refutation\n",
    "    \u001b[32;1m*\u001b[0m sk__1 = One(1) by trivial\n",
    "    \u001b[32;1m*\u001b[0m sk_ = One(2) by trivial\n",
    "    \u001b[32;1m*\u001b[0m sk__1 = sk_(3) by simp 'demod' with sk__1 = One(1), sk_ = One(2)\n",
    "    \u001b[32;1m*\u001b[0m goal [file \"/tmp/one.zf\" \"zf_stmt_1\"]\n",
    "        âˆ€ x/45:unit y/46:unit. (x/45 = y/46). by\n",
    "      goal 'zf_stmt_1' in '/tmp/one.zf'\n",
    "    \u001b[32;1m*\u001b[0m negated_goal Â¬ (âˆ€ x/45:unit y/46:unit. (x/45 = y/46)) # skolems: []. by\n",
    "      esa 'cnf.neg'\n",
    "        with goal [file \"/tmp/one.zf\" \"zf_stmt_1\"]\n",
    "               âˆ€ x/45:unit y/46:unit. (x/45 = y/46).\n",
    "    \u001b[32;1m*\u001b[0m sk_ â‰  sk__1(0) by\n",
    "      esa 'cnf'\n",
    "        with negated_goal\n",
    "               Â¬ (âˆ€ x/45:unit y/46:unit. (x/45 = y/46))\n",
    "               # skolems: [].\n",
    "    \u001b[32;1m*\u001b[0m âŠ¥(4) by simp 'simplify_reflect-' with sk__1 = sk_(3), sk_ â‰  sk__1(0)\n",
    "    \n",
    "    # SZS output end Refutation\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "!cat /tmp/eq.dot | dot -Tsvg > /tmp/eq.svg\n",
    "```\n",
    "\n",
    "    cat: /tmp/eq.dot: No such file or directory\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
