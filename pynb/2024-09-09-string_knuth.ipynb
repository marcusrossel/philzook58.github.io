{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: String Knuth Bendix\n",
    "date: 2024-09-09\n",
    "---\n",
    "\n",
    "I've become entranced by all the varieties of the knuth bendix algorithm as of late.\n",
    "\n",
    "Knuth Bendix completion is a kind of equational reasoning algorithm. Given some generators of an equivalence relation like `X + 0 = X` it converts them into a \"good\" rewriting system like `X + 0 -> X`.\n",
    "\n",
    "Knuth bendix does not have to be over terms. It can be over other things that have a notion of overlapping and ordering, graphs, strings, multisets, polynomials, etc. There are ways perhaps of modelling these things as terms, which is a unifying, but there can be computational and mental overhead in doing so.\n",
    "\n",
    "Strings are in particular an interesting example and actually the original thing Knuth and Bendix were considering (I think).\n",
    "\n",
    "When we say strings, we mean the same thing as sequences. There isn't anything intrinsically textual about what we're doing. \n",
    "\n",
    "Having said that, it is neat an intriguing that you can use the commonly available string manipulation libraries to achieve string rewriting needs. String rewriting is the analog of repeatedly applying find/replace rules. \n",
    "\n",
    "Try it out on colab: https://colab.research.google.com/github/philzook58/philzook58.github.io/blob/master/pynb/2024-09-09-string_knuth.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite(s : str, R : list[tuple[str,str]]) -> str:\n",
    "    \"\"\"Fully reduce s by rewrite rules R\"\"\"\n",
    "    s0 = \"\"\n",
    "    while s0 != s:\n",
    "        s0 = s\n",
    "        for lhs,rhs in R:\n",
    "            s = s.replace(lhs,rhs)\n",
    "    return s\n",
    "\n",
    "assert rewrite(\"aaabbbaa\", [(\"aa\", \"a\")]) == \"abbba\" # remove duplicate a\n",
    "assert rewrite(\"aaabbbaa\", [(\"aa\", \"a\"), (\"bb\", \"b\")]) == \"aba\" # remove duplicates\n",
    "assert rewrite(\"aaabbbaa\", [(\"aa\", \"a\"), (\"bb\", \"b\"), (\"ba\", \"ab\")]) == \"ab\" # bubble sort \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But actually for my purposes, I don't want to be restricted to just sequences of characters, so I need to rebuild some functionality we had with python strings using python tuples.\n",
    "\n",
    "# Rewriting Tuples\n",
    "In order to do matching, I need to know when one tuple is a subsequence of another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subseq(s,t):\n",
    "    \"\"\"Return index when s is a subsequence of t, None otherwise\"\"\"\n",
    "    for i in range(len(t) - len(s) + 1):\n",
    "        if s == t[i:i+len(s)]:\n",
    "            return i\n",
    "    return None\n",
    "assert subseq((2,2), (1,1,1,2,2)) == 3\n",
    "assert subseq((2,2), (1,1,1,2,2,2)) == 3\n",
    "assert subseq((), (1,2,3)) == 0\n",
    "assert subseq((3,4), (4,5,3)) is None\n",
    "assert subseq((3,4), (4,5,3,4)) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to reimplement the `replace` functionality of a single replacement rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(s, lhs, rhs):\n",
    "    \"\"\"\"\"\"\n",
    "    i = subseq(lhs, s)\n",
    "    if i is not None:\n",
    "        return s[:i] + rhs + s[i+len(lhs):]\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "assert replace((1,2,3,4), (2,3), (5,6)) == (1,5,6,4)\n",
    "assert replace((1,2,3,4), (2,3), (5,6,7)) == (1,5,6,7,4)\n",
    "assert replace((1,2,3,4), (2,3), (5,6,7,8)) == (1,5,6,7,8,4)\n",
    "assert replace((1,1), (4,4), (2,2)) == (1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can fully reduce tuples now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite(s, R, exclude=-1):\n",
    "    # exclude is useful for simplifying a rule\n",
    "    while True:\n",
    "        s0 = s\n",
    "        for i,(lhs,rhs) in enumerate(R):\n",
    "            if i != exclude:\n",
    "                s = replace(s,lhs,rhs)\n",
    "        if s == s0:\n",
    "            return s\n",
    "\n",
    "assert rewrite((1,2,3,4), [((2,3), (5,6)), ((5,6), (7,8))]) == (1,7,8,4)\n",
    "assert rewrite((1,1,1,1,1,1), [((1,1), ())]) == ()\n",
    "assert rewrite((1,1,1,1,2,1), [((1,1), ())]) == (2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knuth Bendix\n",
    "So we've implemented running our string rewrite system. Nice.\n",
    "\n",
    "The next piece I need is to implement our term ordering. The concept of a term ordering I think is the key point of term rewriting that is both nearly trivial in some sense but also not at all obvious.\n",
    "\n",
    "A key property that is very useful in a rewriting system (or any algorithm) is termination. When we want to talk mathematically about termination, we talk about well founded relations. https://en.wikipedia.org/wiki/Well-founded_relation Well founded transition relations are the ones that are guaranteed to terminate.\n",
    "\n",
    "Orderings are also binary relations and we can cook up all sorts of ways of ordering things based on different intuitions and needs. If we choose an ordering that is well founded, then when the right hand sides of our rewrites are \"smaller\" than our left hand sides, our rewrite system is guaranteed to terminate.\n",
    "\n",
    "Actually implementing the typical orderings (LPO, RPO, KBO) for terms is quite confusing. The basics string orderings are more straightforward.\n",
    "\n",
    "We want short strings, so we can order by length. To tie break, we can use the build in lex ordering. This is called the shortlex ordering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortlex(s,t):\n",
    "    \"\"\" order by length, then tie break by contents lex\"\"\"\n",
    "    if len(s) < len(t):\n",
    "        return t,s\n",
    "    elif len(s) > len(t):\n",
    "        return s,t\n",
    "    elif s < t:\n",
    "        return t,s\n",
    "    elif s > t:\n",
    "        return s,t\n",
    "    else:\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's things that are a bit trickier. In Knuth Bendix we need to find all non trivial overlaps of left hand sides in rules. These are the possible sources of non-confluence aka the rule order mattering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def overlaps(s,t):\n",
    "    \"\"\"critical pairs https://en.wikipedia.org/wiki/Critical_pair_(term_rewriting)\"\"\"\n",
    "    # make len(t) >= len(s)\n",
    "    if len(t) < len(s):\n",
    "        s,t = t,s\n",
    "    if subseq(s,t) is not None:\n",
    "        yield t\n",
    "    # iterate over possible overlap sizes 1 to the len(s) at edges\n",
    "    for osize in range(1,len(s)):\n",
    "        if t[-osize:] == s[:osize]:\n",
    "            yield t + s[osize:]\n",
    "        if s[-osize:] == t[:osize]:\n",
    "            yield s + t[osize:]\n",
    "\n",
    "assert set(overlaps((1,2), (2,3))) == {(1,2,3)}\n",
    "assert set(overlaps((1,2), (3,2))) == set()\n",
    "assert set(overlaps((1,2), (2,1))) == {(1,2,1), (2,1,2)}\n",
    "assert set(overlaps((1,2), (1,2))) == {(1,2)}\n",
    "assert set(overlaps((2,2), (2,2,3))) == {(2,2,3), (2,2,2,3)}\n",
    "assert set(overlaps((), (1,2))) == {(1,2)} # Hmm. Kind of a weird edge case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given all these pieces, knuth bendix is a pretty simple almost brute force way of inferring equations and rules turning a set of equations into a good set of rules.\n",
    "\n",
    "In a big loop we\n",
    "1. For each pair of rules, we try to find a critical pair and add this to E\n",
    "2. Reduce all equations in E with respect to R to remove redundancy\n",
    "3. Orient equations in E to make them rewrite rules in R \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduce(R):\n",
    "    \"\"\"deduce all possible critical pairs from R\"\"\"\n",
    "    for i, (lhs,rhs) in enumerate(R):\n",
    "        for j in range(i):\n",
    "            lhs1,rhs1 = R[j]\n",
    "            for o in overlaps(lhs1,lhs):\n",
    "                x,y = rewrite(o, [(lhs1, rhs1)]), rewrite(o, [(lhs, rhs)])\n",
    "                if x != y:\n",
    "                    yield x,y\n",
    "\n",
    "def KB(E):\n",
    "    E = E.copy()\n",
    "    R = []\n",
    "    done = False\n",
    "    while not done:\n",
    "        done = True\n",
    "        E.extend(deduce(R))\n",
    "        while E:\n",
    "            lhs, rhs = E.pop()\n",
    "            lhs, rhs = rewrite(lhs,R), rewrite(rhs,R)\n",
    "            if lhs != rhs:\n",
    "                done = False\n",
    "                lhs, rhs = shortlex(lhs,rhs)\n",
    "                R.append((lhs, rhs))\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a nice examlpe from https://haskellformaths.blogspot.com/2010/05/string-rewriting-and-knuth-bendix.html . \n",
    "![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXLtSpN3WX0ZQr_HhiXMvJuS5gmRzufJk36492TLrFr_dkyTBYIJA46ROYoVQZkmkIlrWNpJxLQP8w6T2dUu00g-6eXDxEK52yyA8AkXqvNNigY14YuVOVcVhEVBRtorgZUnOLllCW9AO1/s400/squaresyms.GIF)\n",
    "`a` and `b` generate rotations and flips of a square. These eqautions aren't normalizing though as is. We can run them through Knuth Bendix to get a normalizing set of rules.\n",
    "\n",
    "A nice trick is to encode the inverse as a negative. These are opaque identitifiers and I have done nothin special in KB for accounting for inverses, but it is nice from a python syntax perspective.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((-1, 1), ()), ((-2, 2), ()), ((1, 1, 1, 1), ()), ((2, 2), ()), ((1, 1, 1, 2), (2, 1))]\n",
      "[((1, 1, 1, 2), (2, 1)), ((2, 2), ()), ((1, 1, 1, 1), ()), ((-2, 2), ()), ((-1, 1), ()), ((1, 1, 1), (-1,)), ((1, 1, 2), (-1, 2, 1)), ((2,), (-2,)), ((1, -2, 1), (-2,)), ((-1, -2, 1, 1), (1, -2)), ((-1, -2, 1), (-2, 1, 1)), ((-2, 1, -2), (-1,)), ((-2, 1, 1, -2), (-1, -1)), ((1, -1), (-2, -2)), ((-2, -2), ()), ((-1, -2), (-2, 1)), ((1, -2), (-2, -1)), ((1, 1), (-1, -1)), ((-1, -1, -1), (1,))]\n"
     ]
    }
   ],
   "source": [
    "e = 0\n",
    "a = 1 # a is rottate square\n",
    "b = 2 # b is flip square horizontally.\n",
    "E = [\n",
    "    ((-a, a), ()), # inverse -b * b = 1\n",
    "    ((-b,b), ()), # inverse -a * a = 1\n",
    "    ((a,a,a,a), ()), # a^4 = 1\n",
    "    ((b,b), ()), # b^2 = 1\n",
    "    ((a,a,a,b), (b,a)) #a^3 b = ba  \n",
    "]\n",
    "print(E)\n",
    "print(KB(E))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a big pile. Here is a post processing to simplify the redundancies in the rules. A more efficient knuth bendix, like Huet's algorithm, would do this as it goes. The simplification of the left hand side of rules is a bit tricky. I'm not particularly sure I did it right here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((-1, 1), ()),\n",
       " ((2,), (-2,)),\n",
       " ((1, -1), ()),\n",
       " ((-2, -2), ()),\n",
       " ((-1, -2), (-2, 1)),\n",
       " ((1, -2), (-2, -1)),\n",
       " ((1, 1), (-1, -1)),\n",
       " ((-1, -1, -1), (1,))]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simplify(R):\n",
    "    Rnew = []\n",
    "    E = []\n",
    "    for i, (lhs,rhs) in enumerate(R):\n",
    "        # lhs = reduce(Rnew)\n",
    "        lhs1 = rewrite(lhs, R, exclude=i) # L-simplify. nebulous correctness. I might be playing it both ways here. I keep around the old R even though I should have moved it to E?\n",
    "        rhs1 = rewrite(rhs, R) # R-simplify\n",
    "        if lhs1 == lhs:\n",
    "            Rnew.append((lhs,rhs1))\n",
    "        elif lhs1 != rhs1:\n",
    "            E.append((lhs1, rhs1))\n",
    "    return E,Rnew\n",
    "\n",
    "R = KB(E)\n",
    "E,R = simplify(R)\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've deduced that $b^{-1} = b$ for the flips.\n",
    "\n",
    "Just to confirm I haven't lost completeness, I can derive all of the original axioms using my new simplified system R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "(-2, 1) (-2, 1)\n"
     ]
    }
   ],
   "source": [
    "E = [\n",
    "    ((-a, a), ()), # inverse -b * b = 1\n",
    "    ((-b,b), ()), # inverse -a * a = 1\n",
    "    ((a,a,a,a), ()), # a^4 = 1\n",
    "    ((b,b), ()), # b^2 = 1\n",
    "    ((a,a,a,b), (b,a)) #a^3 b = ba  \n",
    "]\n",
    "for x,y in E:\n",
    "    print(rewrite(x, R), rewrite(y, R))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bits and Bobbles\n",
    "\n",
    "Note that GAP already ships a string knuth bendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://doc.sagemath.org/html/en/reference/groups/sage/groups/finitely_presented.html\n",
    "from sage.all import *\n",
    "F = FreeGroup(3)\n",
    "x0,x1,x2 = F.gens()\n",
    "G = F / [x0**2, x1 * x0]\n",
    "G\n",
    "F([1])\n",
    "F.gap()\n",
    "G.relations()\n",
    "k = G.rewriting_system()\n",
    "k.make_confluent()\n",
    "k\n",
    "G.simplification_isomorphism()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "Where I'm going with my next post is to tie in string rewriting to an egraph giving a \"seqeunce egraph\". The \"characters\" of the string are ground terms. Because this egraph embeds string rewriting, it isn't guaranteed to terminate.\n",
    "\n",
    "I want this because programs often have sequence of instructions. Instrinsic associativity in the style of https://www.philipzucker.com/linear_grobner_egraph/ seems useful.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/gap-packages/kbmag\n",
    "\n",
    "- https://www.sciencedirect.com/science/article/pii/S1567832610001025 efficiency issues of kbmag - swan https://www.cs.stir.ac.uk/~kjt/techreps/pdf/TR197.pdf Augmenting Metaheuristics with Rewriting Systems\n",
    "- https://maffsa.sourceforge.net/old_manpages/maf.html MAF\n",
    "- rkbp\n",
    "\n",
    "Useful books:\n",
    "- Epstein book Word Processing in Groups\n",
    "- Handbook of computational group theory\n",
    "- Charles Sims book - Computation with Finitely Presented Groups\n",
    "- Term Rewriting and All That\n",
    "\n",
    "\n",
    "The automata anlge is really intriguing, becayse I'd hope I could use off the shelf high perf stuff to achieve the rewriting. Maybe BurntSushi?\n",
    "\n",
    "Word processing in groups\n",
    "automatic groups https://en.wikipedia.org/wiki/Automatic_group\n",
    "word acceptor. Is this for partiality? the valid stings = {w | accept(w)} ?\n",
    "Could have automata that identifies strings that correspond to particular elements\n",
    "wa=v is the same asa recognizer a=i(w)v\n",
    "\n",
    "\n",
    "What's the deal with the automata stuff\n",
    "\n",
    "Twee sorting.\n",
    "If we specialize commutiativyt, a very bad rule, to the individual guys, it becopmes sorting rules\n",
    "\n",
    "\n",
    "This rewrite system will sort.\n",
    "\n",
    "Computational group theory. Orbits. Caleb's graph hashing. Cayley Graph. Nauty\n",
    "\n",
    "Free monoid\n",
    "I can totalize the groupoid of paths into path fragment sequences?\n",
    "Use string KB a la grobner egraphs. String"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://haskellformaths.blogspot.com/2010/05/string-rewriting-and-knuth-bendix.html\n",
    "\n",
    "\n",
    "![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXLtSpN3WX0ZQr_HhiXMvJuS5gmRzufJk36492TLrFr_dkyTBYIJA46ROYoVQZkmkIlrWNpJxLQP8w6T2dUu00g-6eXDxEK52yyA8AkXqvNNigY14YuVOVcVhEVBRtorgZUnOLllCW9AO1/s400/squaresyms.GIF)\n",
    "\n",
    "shortlex\n",
    "\n",
    "https://math.stackexchange.com/questions/684066/knuth-bendix-completion-algorithm-word-problem\n",
    "\n",
    "https://docs.gap-system.org/pkg/kbmag/doc/manual.pdf\n",
    "\n",
    "Strings are nice mathematically because they bake in an associative operation of concatenation, which we use to model other associative operations like group multiplication. Most typical algebraic thingies have associativity, and cutting out associativty computationally by brute force is awful. We often bake in associativty into our notation by just forgetting to bother with parenthesis, which makes proofs incvolving associativity so trivial as to be beneath notice.\n",
    "\n",
    "Strings can also represent paths.\n",
    "\n",
    "\n",
    "- Strings can be modelled as terms. The string `xyz` in a left string context `A` and a right string context `B`, `AxyzB`, can be modelled in a couple different way: `x(y(z(B)))` or `cons(x,cons(y,cons(z,cons(B)))` or reversedly `z(y(x(A)))`. It's interesting that we tend to make the string contexts A & B implicit, whereas the upper term context is usually implicit but the lower child context is not implicit and instead represented by variables. Food for thought. \n",
    "\n",
    "- It is to my knowledge difficult to shallowly embed term rewriting into string rewriting. It is possible to embed because string rewriting is turing complete, so the brute force solutions is build a turing machine into string rewriting, then write a program to do term rewriting on that turing machine. Yuck.\n",
    "- Ground term rewriting is trivially embeddable into string rewriting. Just flatten / pretty print the term. The difference here is that term rewriting supports (multiple) variables, which is tough in strings. When we look at critical pairs, in ground terms the ONLY way to have non trivial overlap is for on term to be a subterm of the other. There are no interesting partial overlaps.\n",
    "\n",
    "\n",
    "String knuth bendix is much simpler than term knuth bendix because the notion of overlap and reduction is so much simpler. You don't need to go into some side discussion of unification and narrowing, which are topics into their own right.\n",
    "\n",
    "A Interesting trick to if I need bulk rewriting, perhaps concat a special symbol in between my different strings I want reduced `\"$\".join(string_db).replace()`\n",
    "\n",
    "Regular lexicographic order is not well founded b > ab > aab > aaab > ...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "https://www.philipzucker.com/ground_kbo/\n",
    "\n",
    "Charles Sim book has heuristics, suggestions about indexing\n",
    "Automata for indexing?\n",
    "\n",
    "Hyperscan. Burntsushi. Geoff langdale\n",
    "Can their tricks be applied or are those tricks about regex?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way of generating all the normal forms is mentioned for this finite group. Enumerate all strings formed by appending the generators to something else that was a normal form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(), (1,)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def nfs(gens, R):\n",
    "    nf = [()]\n",
    "    seen = 0\n",
    "    while seen < len(nf):\n",
    "        s = nf[seen]\n",
    "        seen += 1\n",
    "        for g in gens:\n",
    "            t = (g,) + s\n",
    "            t1 = rewrite(t, R)\n",
    "            if t == t1:\n",
    "                nf.append(t)\n",
    "    return nf\n",
    "\n",
    "nfs([1,2], KB(E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, 2, 2, 2, 1, 2), (2, 2, 2, 1, 2, 2), (1, 2, 2, 2, 1, 2)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def subseq(s,t):\n",
    "    for i in range(len(t) - len(s)):\n",
    "        if s == t[i:i+len(s)]:\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "def overlaps(s,t):\n",
    "    if len(t) < len(s):\n",
    "        s,t = t,s\n",
    "    # t is bigger than s\n",
    "    #overlap_size == 0\n",
    "    #for i in range(len(t) - len(s)):\n",
    "    #    if s == t[i:i+len(s)]:\n",
    "    if subseq(s,t) is not None:\n",
    "        yield t\n",
    "    for osize in range(1,min(len(s), len(t)) + 1):\n",
    "        if t[-osize:] == s[:osize]:\n",
    "            yield t + s[osize:]\n",
    "        if s[-osize:] == t[:osize]:\n",
    "            yield s + t[osize:]\n",
    "\n",
    "list(overlaps( (2,2,2,1,2), (1,2,2) ))\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "proofs.\n",
    "don't delete anything. Keep the active simplified set though.\n",
    "R = [] # write only\n",
    "explain = []\n",
    "Rmin = {} # simmplified ids or tuples without exaplain\n",
    "\n",
    "R = [((lhs,rhs),explain)]\n",
    "Rmin = {(lhs,rhs)} ...  {lhs:rhs}  (?)\n",
    "\n",
    "A trie / prefix trie seems useful.\n",
    "\n",
    "\n",
    "preallocate numpy (?) for numba jit?\n",
    "np.zeros((N, M))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huet(E,R):\n",
    "    R = set()\n",
    "    unmarked = []\n",
    "    while len(E) > 0 or len(unmarked) > 0:\n",
    "        while E:\n",
    "            (lhs,rhs) = E.pop()\n",
    "            lhs,rhs = rewrite(lhs, R), rewrite(rhs, R)\n",
    "            if lhs == rhs:\n",
    "                continue\n",
    "            lhs,rhs = shortlex(lhs,rhs)\n",
    "            Rnew = [(lhs,rhs)]\n",
    "            unmarked.append((lhs,rhs))\n",
    "            for (g,d) in R:\n",
    "                g1 = replace(g, (lhs,rhs))\n",
    "                if g1 == g:\n",
    "                    d = rewrite(d, R)\n",
    "                    Rnew.append((g,d))\n",
    "                else:\n",
    "                    E.append((g1,d))\n",
    "            R = Rnew\n",
    "        if unmarked:\n",
    "            (lhs,rhs) = unmarked.pop()\n",
    "            for (g,d) in R:\n",
    "                for o in overlaps(g,lhs):\n",
    "                    x = rewrite(o, R)\n",
    "                    y = rewrite(o, [(lhs,rhs)])\n",
    "                    if x != y:\n",
    "                        E.append((x,y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knuth bendix can be made more goal driven. We don't persay need to fully ocmplete the system if we just want to prove some particular equality.\n",
    "\n",
    "Similarly, we can early stop knuth bendix is all we want is a simplified term. It isn't obvious if we can show that a term is globally maximally simplified until KB finishes though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KBgoal(E, goal):\n",
    "    \"\"\"\n",
    "    goal driven KB. \n",
    "    \"\"\"\n",
    "    gl, gr = goal\n",
    "    while len(E) > 0:\n",
    "        if gl == gr:\n",
    "            # We can do early stopping given a particular goal.\n",
    "            return True\n",
    "        E,R = KB1(E, R)\n",
    "        gl, gr = reduce(gl, R), reduce(gr, R)\n",
    "    return False\n",
    "\n",
    "# could also do binary combinations as goals. and / or / not  of = !="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def overlaps(s,t):\n",
    "    if len(s) > len(t):\n",
    "        s,t = t,s\n",
    "    for i in range(1, min(len(s), len(t)) + 1):\n",
    "        if s[-i:] == t[:i]:\n",
    "            yield i\n",
    "    for i in range(len(t) - len(s)):\n",
    "        if s == t[i:i+len(s)]:\n",
    "            return i\n",
    "        \n",
    "        \"\"\"\n",
    "        for i, (lhs,rhs) in enumerate(R):\n",
    "            for j in range(i):\n",
    "                lhs1,rhs1 = R[j]\n",
    "                for o in overlaps(lhs1,lhs):\n",
    "                    E.append((reduce(o, [(lhs1, rhs1)]), reduce(o, [(lhs, rhs)])))\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 0\n",
    "a = 1 # a is rottate square\n",
    "b = 2 # b is flip square horizontally.\n",
    "E = [\n",
    "    ((-a, a), ()), # inverse -b * b = 1\n",
    "    ((-b,b), ()), # inverse -a * a = 1\n",
    "    #((e,a), (a,)),\n",
    "    #((e,b), (b,)),\n",
    "    #((e,-a), (-a,)),\n",
    "    #((e,-b), (-b,)),\n",
    "    ((a,a,a,a), ()),\n",
    "    ((b,b), ()),\n",
    "    ((a,a,a,b), (b,a))\n",
    "]\n",
    "print(E)\n",
    "print(KB(E))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old notes String rewriting systems\n",
    "\n",
    "[Semi-Thue systems](https://en.wikipedia.org/wiki/Semi-Thue_system)\n",
    "[Word problem](https://en.wikipedia.org/wiki/Word_problem_(mathematics))\n",
    "Monoid presentation\n",
    "\n",
    "converting to term rewriting system fff ---> f(f(f(X)))\n",
    "\n",
    "Term Rewriting using linux command line tools\n",
    "\n",
    "The string search and manipulation tools are very powerful and efficient. They compile queries like grep into simple machines and such I think.\n",
    "\n",
    "There's a big difference between ground and non ground terms. They appear subtly different when latexed, but the are way different beats\n",
    "Ground terms equation proving can be done through the e graph efficiently.\n",
    "\n",
    "Ground term rewriting seems to be identical to string rewriting. Just layout serially a traversal of the term.\n",
    "\n",
    "The implicit prefix and suffix are the context of the term\n",
    "\n",
    "```python\n",
    "\n",
    "rules = [\n",
    "  (\"aa\", \"b\"),\n",
    "  (\"b\", \"c\")\n",
    "]\n",
    "\n",
    "def run_rules(s,rules):\n",
    "  old_s = None\n",
    "  while s != old_s:\n",
    "    old_s = s\n",
    "    for lhs,rhs in rules:\n",
    "      s = s.replace(lhs,rhs)\n",
    "  return s\n",
    "\n",
    "print(run_rules(\"ababaaaaccaaa\", rules))\n",
    "\n",
    "def naive_completion(rules):  \n",
    "  old_rules = None\n",
    "  while rules != old_rules:\n",
    "    old_rules = rules.copy()\n",
    "    for (lhs,rhs) in old_rules:\n",
    "        a = run_rules(lhs, rules)\n",
    "        b = run_rules(rhs, rules)\n",
    "        if a == b:\n",
    "          break\n",
    "        if a < b:\n",
    "          rules.add((b,a))\n",
    "        if a > b:\n",
    "          rules.add((a,b))\n",
    "  return rules\n",
    "\n",
    "# an incomplete reduction routine?\n",
    "# Triangular rewrite rules in some sense.\n",
    "# Is this right at all? This is like a chunk of Huet's. I think just moving R -> E might be ok even if not one of listed rules. No, if I could do that ths move + simplfy would give me a more powerful R-simplify.\n",
    "# I might be weakening my rules. That's not so bad imo.\n",
    "def reduce_rules(E):  \n",
    "  # worklist style\n",
    "  R = set()\n",
    "  # Sort E so smallest last probably. Most reduction power.\n",
    "  E = sorted(E, key=lambda k: len(k[0]), reverse=True)\n",
    "  while len(E) > 0:\n",
    "    (a,b) = E.pop()\n",
    "    a = run_rules(a, R) # simplify\n",
    "    b = run_rules(b, R)\n",
    "    print(a,b)\n",
    "    if a == b: #delete\n",
    "      continue\n",
    "    if (len(b), b) > (len(a), a): # len then lex ordering\n",
    "      R.add((b,a))\n",
    "    if (len(a), a) > (len(b), b):\n",
    "      R.add((a,b))\n",
    "  return R\n",
    "\n",
    "\n",
    "\n",
    "rules = {\n",
    "  (\"aaa\", \"a\"),\n",
    "  (\"aaa\", \"c\")\n",
    "}\n",
    "\n",
    "print(naive_completion(rules))\n",
    "\n",
    "\n",
    "rules = [\n",
    "  (\"ffa\", \"a\"),\n",
    "  (\"fa\", \"a\")\n",
    "]\n",
    "\n",
    "print(run_rules(\"ffffffffffffa\", rules))\n",
    "\n",
    "print(reduce_rules(rules))\n",
    "\n",
    "rules = [\n",
    "  (\"12+\", \"21+\"), # an application of comm\n",
    "  (\"23+1+\", \"2+31+\") # an application of assoc\n",
    "]\n",
    "# I am really going to want a notion of indirection or compression here.\n",
    "# Intern strings\n",
    "\n",
    "class RPN():\n",
    "  def __init__(self, s):\n",
    "    self.s = str(s)\n",
    "  def __add__(self,b):\n",
    "    return RPN(self.s + b.s + \"+\")\n",
    "  def __repr__(self):\n",
    "    return self.s\n",
    "\n",
    "b0 = RPN(0)\n",
    "b1 = RPN(1)\n",
    "b2 = RPN(2)\n",
    "b3 = RPN(3)\n",
    "\n",
    "\n",
    "E = [\n",
    "  (b1 + b0, b0 + b1),\n",
    "  (b0 + b1, b1),\n",
    "  (b1 + b2, b3),\n",
    "  (b2, b1 + b1) \n",
    "]\n",
    "E = [\n",
    "  (\"10+\", \"01+\"),\n",
    "  (\"01+\", \"1\"),\n",
    "  (\"12+\", \"3\"),\n",
    "  (\"2\", \"11+\"),\n",
    "  (\"00+\", \"0\"),\n",
    "  (\"00+1+1+1+\", \"3\"),\n",
    "]\n",
    "\n",
    "print(reduce_rules(E))\n",
    "E = reduce_rules(E)\n",
    "print(run_rules(\"00+1+0+1+0+2+\",E))\n",
    "\n",
    "E = [( str(i) + str(j) + \"+\" , str(i + j)) for i in range(4) for j in range(10) if i + j <= 9]\n",
    "print(E)\n",
    "print(reduce_rules(E))\n",
    "print(run_rules(\"00+1+0+1+0+2+\",E))\n",
    "\n",
    "```\n",
    "\n",
    "Ropes\n",
    "\n",
    "We can of course compile a rule set to do better than this. In some sense every string represents a possibly infinite class of strings posible by running rules in reverse\n",
    "\n",
    "String rewriting systems are a bit easier to think about and find good stock library functionality for.\n",
    "\n",
    "string rewriting is unary term rewriting. A variable string pattern \"aaaXbbbb\" is a curious object from that perspective. While simple, it is a higher order pattern. `a(a(a(X(b(b(b(Y)))))))`. You can also finitize a bit. `foo(a)` can be made an atomic character. Or you can partially normalize a term rewriting system to string rewriting form\n",
    "\n",
    "String orderings\n",
    "Lexicographic comparison\n",
    "Length\n",
    "shortlex - first length, then lex\n",
    "symbol counts\n",
    "\n",
    "```python\n",
    "def critical_pairs(a,b):\n",
    "  assert len(a) > 0 and len(b) > 0\n",
    "  if len(b) <= len(a):\n",
    "    a,b = b,a # a is shorter\n",
    "  cps = []\n",
    "  if b.find(a) != -1: # b contains a\n",
    "   cps.append(b)\n",
    "  for n in range(1,len(a)): # check if overlapping concat is possible\n",
    "    if b[-n:] == a[:n]:\n",
    "      cps.append(b + a[n:])\n",
    "    if b[:n] == a[-n:]:\n",
    "      cps.append(a + b[n:])\n",
    "  return cps\n",
    "\n",
    "print(critical_pairs(\"aba\", \"ac\"))\n",
    "print(critical_pairs(\"aba\", \"ca\"))\n",
    "print(critical_pairs(\"abab\", \"ba\"))\n",
    "  \n",
    "'''\n",
    "def reduce_rules(rules): # a very simple reduction, reducing rhs, and removing duplicate rules\n",
    "  rules1 = set()\n",
    "  for (lhs,rhs) in rules:\n",
    "    rhs = run_rules(rhs,rules)\n",
    "    rules1.add((lhs,rhs))\n",
    "  return rules1\n",
    "'''\n",
    "\n",
    "  #run_rules\n",
    "  #reduce_rules\n",
    "```\n",
    "\n",
    "Building a suffix tree might be a good way to find critical pairs.\n",
    "\n",
    "Lempel Ziv / Lzw is the analog of hash consing? Some kind of string compression is. That's fun.\n",
    "\n",
    "<http://haskellformaths.blogspot.com/2010/05/string-rewriting-and-knuth-bendix.html>\n",
    "\n",
    "It seems like named pattern string rewriting and variable term rewriting might be close enough\n",
    "\n",
    "You could imagine\n",
    "\n",
    "((x + 0) + 0 + 0) laid out as + + + x 0 0 0. and the found ground rewite rule + x 0 -> x being applied iteratively.\n",
    "\n",
    "Labelling shifts:\n",
    "f(g(a), b) ->   f +0 g +1 a -1 b -0\n",
    "\n",
    "the pattern\n",
    "f(?x)  -> ?x\n",
    " becoming\n",
    " f +0 <x> -0 -> <x>\n",
    " f +1 <x> -1 -> <x>\n",
    " f +2 <x> -2 -> <x>\n",
    "and so on to some upper limit\n",
    "We could occasionally renormalize maybe if there are no +n -n pairs remaining.\n",
    "then shuffle all the above ones down.\n",
    "Ok but what about something that increases the depth\n",
    "x -> f(x)\n",
    "... hmmm.\n",
    "\n",
    "And if we number from the bottom?\n",
    "\n",
    "f +2 <X> -2 -> <X>\n",
    "\n",
    "and\n",
    "<X> -> ??? Well a raw pattern is pretty shitty\n",
    "f(x) -> f(f(x)) becomes\n",
    "f +n <X> -n -> f +(n+1) f +n <x>\n",
    "Yeah. Numbering from the bottom is better. We don't have to\n",
    "\n",
    "f(stuff,stuff)\n",
    "f +n <X> -n +n <Y> -n\n",
    "\n",
    "even without enumerating\n",
    "plus +<n1> <X> -<n1> +<n2> <Y> -<n2>\n",
    "\n",
    "Oooh. We have to enumerate every combo of possible subterm depths.\n",
    "\n",
    "Hmm. This adjust levels\n",
    "\n",
    "<http://matt.might.net/articles/sculpting-text/>\n",
    "\n",
    "A unique terminator for the subexpression is the point.\n",
    "f +2 (^ -2)* -2\n",
    "\n",
    "could have a counter per symbol. per symbol depth.\n",
    "f1 ( yadayada) \\f1\n",
    "\n",
    "fa1 <X> fb1 <Y> fc1\n",
    "\n",
    "huh. What about the CPP? Won't that basically work?\n",
    "\n",
    "This is horribly inefficent because it'll expand out huge terms.\n",
    "And big backtracking jumps. Or rather big seeks while it tries to find the next spot to go to. The next argument of f.\n",
    "\n",
    "For ground term rewriting it seems actually reasonable and faster than having indirections. We can't have sharing though. That is a bummer.Maybe with zip.\n",
    "Unless our goal is simplifcation.\n",
    "\n",
    "using the rule\n",
    "\n",
    "We could try to use the e-matching strategy where we iteratively instantiate fixed ground rewrite rules into the sed file itself?\n",
    "\n",
    "Instead of using parenthesis, one could use numbered enter level exit level. And then bound the number of levels.\n",
    "Each term rewriting becomes a string rewriting (with named regex holes ) replicated by the number of supported levels.\n",
    "\n",
    "using sed on itself we might be able to implement knuth bendix\n",
    "\n",
    "One could compile into a human unreadable huffman encoded like thing\n",
    "\n",
    "<https://superuser.com/questions/1366825/how-to-recursively-replace-characters-with-sed> looping in sed\n",
    "You can gnu parallel into chunks\n",
    "grep is much faster. If terms are rare, maybe find using grep first?\n",
    "\n",
    "Suffix trees can store every subterm of ground term with efficient query.\n",
    "\n",
    "<https://spencermortensen.com/articles/serialize-a-binary-tree-in-minimal-space/> an interesting perspective on tree serialization\n",
    "catalan numbers. We know size of tree if we give elements.\n",
    "<https://www.educative.io/edpresso/what-is-morris-traversal> woah. Bizarre. It mutates the true as it goes down it and store\n",
    "Kind of reminscent of dancing links in a way\n",
    "\n",
    "f 20 90 190 yada yada yada could desribe links to the approprate spots.\n",
    "This would be the analog of flatterms.\n",
    "\n",
    "There is something like encoding lambda terms with de bruijn. vs unique signifiers.\n",
    "If we could encode the unique signifiers in a way such that they never collide.\n",
    "\n",
    "There is something to that we're kind of converting to rpn.\n",
    "<https://github.com/GillesArcas/numsed>\n",
    "<https://github.com/shinh/sedlisp>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
