{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "date: 2024-05-15\n",
    "title: Proof Search with an LLM Kanren\n",
    "---\n",
    "\n",
    "https://arxiv.org/abs/2402.08147 VerMCTS: Synthesizing Multi-Step Programs using a Verifier, a Large Language Model, and Tree Search\n",
    "\n",
    "\n",
    "Deep learning and in particular LLMs are still hot.\n",
    "\n",
    "Machine learning applied to theorem proving is not a new field. I was watching an interview with Stephen Schulz and he mentioned he was doing a PhD in the nineties on it. Axiom and strategy selection is an important part of top tier solvers. \n",
    "Probably it goes back in various forms all the way to the beginning.\n",
    "\n",
    "To some degree, copilot is already an extremely useful machine learning proof assistant out of the box. I've been finding it mostly fills out reasonable lemma selection and even theorem statements while I've been working on knuckledragger.\n",
    "\n",
    "It now feels almost passe, but the computer world was rocked by alpha go. Monte carlo tree search seems like a natural fit for theorem proving search as well. There is a proof search tree that branches when there are multiple possible proof rules to follow.\n",
    "\n",
    "\n",
    "\n",
    "Kanren is a style of embedding logic programming as a DSL into host language. \n",
    "Logic programming has two big pieces that can be treated separately\n",
    "\n",
    "- Search. A program is given alternative possibilities. This nondeterminism can be modelled as a function `state -> list state` and combinators around this type.\n",
    "- Unification. Unification is two way pattern matching. The bidirectional character of it is what enables the seemingly magical ability for prolog of minikanren to run relations backwards or forwards (for example using a single `append` predicate to append two lists, take a list difference, or generate partitions of a list). The `state` in a basic kanren is the unification state, which is a substitution mapping from logic variables to terms.\n",
    "\n",
    "There are two basic combinators you need for kanren style search, `conj` and `disj` aka `and` and `or`. Conj combines requirements and is the bind operation of a nondeterminism monad. `conj` presents alternative choices and is where the nondeterminism is generated.\n",
    "\n",
    "Different search strategies can be represented by different implementations of `conj` and `disj`\n",
    "\n",
    "\n",
    "The most basic depth first strategy can be written as\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disj(*args): # very similar to chain https://docs.python.org/3/library/itertools.html#itertools.chain\n",
    "    def res(state):\n",
    "        for a in args:\n",
    "            for state in a(state):\n",
    "                yield state\n",
    "    return res\n",
    "\n",
    "def conj(*args): # kind of state threading product https://docs.python.org/3/library/itertools.html#itertools.product\n",
    "    def res(state):\n",
    "        if len(args) == 0:\n",
    "            yield state\n",
    "            return\n",
    "        else:\n",
    "            for state in args[0](state): # expand first requirement\n",
    "                for state in conj(*args[1:])(state): # send it into expansion of next requiresments\n",
    "                    yield state\n",
    "    return res\n",
    "\n",
    "def fail(state):\n",
    "    return\n",
    "\n",
    "def filt(pred):\n",
    "    def res(state):\n",
    "        if pred(state):\n",
    "            yield state\n",
    "    return res\n",
    "\n",
    "# convert ordinary generator into a goal... waaaaaitaminute.\n",
    "def goal(gen):\n",
    "    def res(state):\n",
    "        for i in gen(state):\n",
    "            yield i\n",
    "    return res\n",
    "\n",
    "#def run(goal,n=-1):\n",
    "#    for x in goal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n",
      "7\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "ex1= conj(range, \n",
    "          disj(conj(filt(lambda x: x > 5), \n",
    "               disj(filt(lambda x: x < 8), \n",
    "                    filt(lambda x: x < 10)))))\n",
    "\n",
    "for i in range(20):\n",
    "    for z in ex1(i):\n",
    "        print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, but let's say I want to return a choice of what to expand.\n",
    "\n",
    "I could wrap this in a gym interface maybe.\n",
    "type hints = string * state\n",
    "type nondet (hints, a) = strem (hints * nondet (hints,a)) | done of state\n",
    "type choice = [(hints, () -> choice)] | state\n",
    " -- so I can view the state or a description and choose to expand this branch, or it may be done \n",
    "\n",
    "We need to invert control. Whatever the hell that means\n",
    "I'm ticking the iteratee part of my brain.\n",
    "What about lazy game trees?\n",
    "\n",
    "state -> choice\n",
    "The hints aren't choices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disj\n",
      "6\n",
      "[[6], [6]]\n",
      "disj\n",
      "7\n",
      "[[7], [7]]\n",
      "disj\n",
      "8\n",
      "[[], [8]]\n",
      "disj\n",
      "9\n",
      "[[], [9]]\n",
      "disj\n",
      "10\n",
      "[[], []]\n",
      "disj\n",
      "11\n",
      "[[], []]\n",
      "disj\n",
      "12\n",
      "[[], []]\n",
      "disj\n",
      "13\n",
      "[[], []]\n",
      "disj\n",
      "14\n",
      "[[], []]\n",
      "disj\n",
      "15\n",
      "[[], []]\n",
      "disj\n",
      "16\n",
      "[[], []]\n",
      "disj\n",
      "17\n",
      "[[], []]\n",
      "disj\n",
      "18\n",
      "[[], []]\n",
      "disj\n",
      "19\n",
      "[[], []]\n"
     ]
    }
   ],
   "source": [
    "def disj(*args):\n",
    "    def res(state):\n",
    "        return (\"disj\", state, [arg(state) for arg in args]) # \"disj\"\n",
    "    return res\n",
    "\n",
    "def conj(*args):\n",
    "    def res(state):\n",
    "        if len(args) == 0:\n",
    "            return [state]\n",
    "            #return [(\"success\", state)]\n",
    "        else:\n",
    "            return [state for state in args[0](state) for state in conj(*args[1:])(state)]\n",
    "            #return [(\"choice\", state, conj(*args[1:])(state)) for state in args[0](state)]\n",
    "    return res\n",
    "\n",
    "def filt(pred):\n",
    "    def res(state):\n",
    "        if pred(state):\n",
    "            return [state]\n",
    "        else:\n",
    "            return []\n",
    "    return res\n",
    "\n",
    "def goal(gen):\n",
    "    def res(state):\n",
    "        return (\"disj\", state, gen(state))\n",
    "    return res\n",
    "\n",
    "    \n",
    "ex1= conj(filt(lambda x: x > 5), \n",
    "               disj(filt(lambda x: x < 8), \n",
    "                    filt(lambda x: x < 10)))\n",
    "\n",
    "for i in range(20):\n",
    "    for z in ex1(i):\n",
    "        print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(goal, state0):\n",
    "    goals = [goal(state0)]\n",
    "    while goals:\n",
    "        goal = goals.pop() # pick which goal seems most interesting here.\n",
    "        #print(\"Which goal? \", list(enumerate(goals)))\n",
    "        #goal = goals.remove(input())\n",
    "        match goal:\n",
    "            case \"done\", state:\n",
    "                yield state\n",
    "            case \"more\", state, k:\n",
    "                goals.extend(next(k)) # or instead of extend, we could use a context to keep track or goals could be a tree.\n",
    "\n",
    "def goal(gen):\n",
    "    def res(state):\n",
    "        return [(\"done\", i) for i in gen(state)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Ctx = namedtuple(\"Ctx\", \"up, siblings\")\n",
    "def run(goal, state0):\n",
    "    ctx = Ctx(\"root\", [])\n",
    "    goals = [goal(state0)]\n",
    "    while True:\n",
    "        cmd = input()\n",
    "        match cmd:\n",
    "            case -1:\n",
    "                up, siblings = ctx\n",
    "                siblings.append(goals)\n",
    "                goals = siblings\n",
    "                ctx = up\n",
    "            case n:\n",
    "                goal = goals.pop(n)\n",
    "                match goal:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could perhaps run a version of minikanren to generate the best possible examples and bad examples as prompts.\n",
    "Run it automatically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disj(*args):\n",
    "    def res(state):\n",
    "        for a in args:\n",
    "            for state in a(state):\n",
    "                yield state\n",
    "    return res\n",
    "\n",
    "def conj(*args):\n",
    "    def res(state):\n",
    "        if len(args) == 0:\n",
    "            yield state\n",
    "            return\n",
    "        else:\n",
    "            for state in args[0](state): # expand first requirement\n",
    "                for state in conj(*args[1:])(state): # send it into expansion of next requiresments\n",
    "                    yield state\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pstep(seq):\n",
    "    (ctx, goal) = seq\n",
    "    if goal in ctx: # refl\n",
    "            yield seq\n",
    "    else:\n",
    "        match goal:\n",
    "            case (\"false\"):\n",
    "                yield from fail\n",
    "            case (\"and\", a, b):\n",
    "                yield from conj(pstep((ctx, a)), pstep((ctx, b)))\n",
    "            case (\"or\", a, b):\n",
    "                yield from disj(pstep((ctx, a)), pstep((ctx, b)))\n",
    "            case (\"=>\", a, b):\n",
    "                yield from pstep((ctx + {a}, b))\n",
    "            case (\"not\", a):\n",
    "                yield from pstep((ctx + a, (\"false\",)))\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/2102.13564 Improving ENIGMA-Style Clause Selection While Learning From History\n",
    "\n",
    "Machine learning in proving tutorial\n",
    "Talia's doc\n",
    "\n",
    "https://www.tcs.ifi.lmu.de/mitarbeiter/jasmin-blanchette/axiom_sel.pdf A Survey of Axiom Selection as a\n",
    "Machine Learning Problem\n",
    "\n",
    "\n",
    "relational learning https://link.springer.com/article/10.1007/s10994-013-5392-1\n",
    "\n",
    "https://simons.berkeley.edu/workshops/agenda/theoretical-foundations-satsmt-solving/ml-solvers\n",
    "\n",
    "https://arxiv.org/pdf/2403.04017 Learning Guided Automated Reasoning:\n",
    "A Brief Survey\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arguments=(2,)\n",
      "Source:\n",
      " @self_describe\n",
      "def fact(n):\n",
      "    if n <= 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * fact(n-1)\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m n \u001b[38;5;241m*\u001b[39m fact(n\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mfact\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m, in \u001b[0;36mself_describe.<locals>.res\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marguments=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSource:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, src)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m, in \u001b[0;36mfact\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m n \u001b[38;5;241m*\u001b[39m \u001b[43mfact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "def self_describe(f):\n",
    "    src = inspect.getsource(f)\n",
    "    def res(*args, **kwargs):\n",
    "        print(f\"arguments={args}\")\n",
    "        print(\"Source:\\n\", src)\n",
    "        return f(*args, **kwargs)\n",
    "    return res # returning tuple here didn't work so good. Could return class wrapper wit __call__ and desc.\n",
    "\n",
    "class Goal():\n",
    "    def __call__(self, *args):\n",
    "        return self.f(*args)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@self_describe\n",
    "def fact(n):\n",
    "    if n <= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * fact(n-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llm\n",
    "\n",
    "\n",
    "# llm datalog?\n",
    "\n",
    "# use llm to auto build descriptor?\n",
    "def auto_desc(state):\n",
    "    \n",
    "\n",
    "\n",
    "def conj( *args , desc=None):\n",
    "    def res(state):\n",
    "        for a in args:\n",
    "            for state in a(state):\n",
    "                \n",
    "        return state\n",
    "    return res\n",
    "def disj(*args ): \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def self_describe(x):\n",
    "    x.__code__\n",
    "    prompt = \n",
    "    \"\"\"\n",
    "    This is source code for a search function. There are a couple of choices avaiable.\n",
    "    Pleas\n",
    "    \"\"\"\"\n",
    "\n",
    "@self_describe\n",
    "def foo(x,y,z):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about using the llm to self annoate the code using __code__ attribute. Then any hints could just go in comments. That's nice.\n",
    "\n",
    "Then I can go and get stock minikanren programs and see if it does better.\n",
    "\n",
    "\n",
    "blockworld\n",
    "\n",
    "\n",
    "Supposedly the point of natural deduction is that it translate well (\"naturally\") into the way mathematicians write down proofs in prose. This is also appealing for guiding a search that is based on an LLM trained on probably a massive amount of math literature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of minikanren is that search is modellable as\n",
    "`state -> list state`\n",
    "\n",
    "\n",
    "state is a list of descriptions\n",
    "we could have an automatic description generator as a default\n",
    "\n",
    "\n",
    "We could also probably throw some RL-iness on there. We're doing tree search, so we could have the llm \n",
    "\n",
    "\n",
    "\"What clues would you use to do it faster next time?\"\n",
    "\n",
    "Choices:\n",
    "1. abort\n",
    "2. choice 1\n",
    "3. choice 2\n",
    "\n",
    "https://news.ycombinator.com/item?id=39479478 A* boosting. \"searchformer\"\n",
    "Feed the llm the trace, not just the state.\n",
    "\n",
    "Kind of need a compelling problem. That's tough? Why should that be tough?\n",
    "\n",
    "neural kanren https://github.com/xuexue/neuralkanren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conj(cost_heuristic=None):\n",
    "def disj(cost_heuristic=None):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic minikanren\n",
    "# ignore the unification stuff. Who cares.\n",
    "def conj(a,b):\n",
    "    def res(state0):\n",
    "        for state1 in a(state0):\n",
    "            for state2 in b(state1):\n",
    "                yield state2\n",
    "    return res\n",
    "\n",
    "def disj(a,b):\n",
    "    def res(state0): # unfair interleaving\n",
    "        for state in a(state0):\n",
    "            yield state\n",
    "        for state in b(state0):\n",
    "            yield state\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth limitted search.\n",
    "def disj_depth(a,b,N=10):\n",
    "    def res(state0):\n",
    "        if state0.depth > N:\n",
    "            return\n",
    "        state0.depth += 1 \n",
    "        for state in a(state0):\n",
    "            yield state\n",
    "        for state in b(state0):\n",
    "            yield state\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{score: (state, expand)} # min heap \n",
    "conj : state -> [(score, state)]\n",
    "disj : \n",
    "# take a look at the first orderized minikanren\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say this is a test\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "chat_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def system():\n",
    "    prompt f\"\"\"\n",
    "        This is a minikarnren like search program looking for solutions to X.\n",
    "        I want you to guide it towards good choices.\n",
    "        For example, if you had the state \n",
    "          \n",
    "        Return ONLY the number number of the choice of which state to expand.\n",
    "        {examples}\n",
    "    \"\"\"\n",
    "    {\"role\": \"system\", \"content\" : \"T\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytroch install.\n",
    "https://github.com/ROCm/ROCm/discussions/2932\n",
    "Actually Radeon 680M and 780M are supported by the latest ROCm 6.0, what you need to do is to set HSA_OVERRIDE_GFX_VERSION=10.3.0 for 680M, and HSA_OVERRIDE_GFX_VERSION=11.0.0 for 780M.\n",
    "\n",
    "huggingface-cli for model management\n",
    "\n",
    "`rocminfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HSA_OVERRIDE_GFX_VERSION=10.3.0\n"
     ]
    }
   ],
   "source": [
    "%env HSA_OVERRIDE_GFX_VERSION=10.3.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HSA_OVERRIDE_GFX_VERSION=11.0.0\n"
     ]
    }
   ],
   "source": [
    "%env HSA_OVERRIDE_GFX_VERSION=11.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_ROCM_ARCH=\"gfx1103\"\n"
     ]
    }
   ],
   "source": [
    "%env PYTORCH_ROCM_ARCH=\"gfx1103\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: AMD_LOG_LEVEL=3\n"
     ]
    }
   ],
   "source": [
    "%env AMD_LOG_LEVEL=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: AMD_SERIALIZE_KERNEL=3\n"
     ]
    }
   ],
   "source": [
    "%env AMD_SERIALIZE_KERNEL=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbc47baa94e743398db7f117de0024f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philip/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write me a poem about Machine Learning.\n",
      "\n",
      "Machines, they weave and they learn,\n",
      "From\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", device_map=\"auto\")\n",
    "#model.to(\"cuda\")\n",
    "\n",
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\") #.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write me a poem about Machine Learning.\n",
      "\n",
      "Machines, they weave and they learn,\n",
      "From\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15cf32e036884d0db1d34df165076164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "./gemma-2b-it.gguf:   0%|          | 0.00/10.0G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-89948379-b0a2-46df-b224-c1c75e2c359c',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1711641796,\n",
       " 'model': '/home/philip/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/718cb189da9c5b2e55abe86f2eeffee9b4ae0dad/./gemma-2b-it.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': '\\n\\nI am unable to access external sources or display images, so I am unable to provide a detailed description of the image you have specified.'},\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 33, 'completion_tokens': 28, 'total_tokens': 61}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "# https://github.com/abetlen/llama-cpp-python\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"google/gemma-2b-it\",\n",
    "    filename=\"*gemma-2b-it.gguf\",\n",
    "    verbose=False\n",
    ")\n",
    "llm.create_chat_completion(\n",
    "      messages = [\n",
    "          {\"role\": \"system\", \"content\": \"You are an assistant who perfectly describes images.\"},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"Describe this image in detail please.\"\n",
    "          }\n",
    "      ]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
