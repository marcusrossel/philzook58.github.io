### The Optimal Polynomial

For an error measure, we can find a polynomial that does as well or
better than every other polynomial of the same degree at aproximating a
function. Using the maximum error measure

$$e[f,p]=\max_{x\in[-1,1]}|f(x)-p(x)|$$

$$e[f,p*]=\epsilon$$

Meanse that the optimal polynomial does not ever get farther away from
the function than $\epsilon$ at any point. Typically, the polynomial
$p*$ will snake back and forth undershooting and overshooting the
function and will actually achieve that bound N+2 times.

The ideal sampling points are the points for which this polynomial
crosses the function we are evaluating

### Chebyshev Polynomials

Chebyshev Polynomials are the polynomials that are as flat as possible
for their degree, i.e. they are the optimal polynomials of the constant.

$$e[0,T_{n}]=1$$

This rippliness is very much like $\cos(nx)$, which do oscillate nicely
between 1 and -1. However, the cosine is decidedly not a polynomial of
degree n.

The asnwer turns out to be to reparametize the cosine

$$T_{n}(\cos\theta)=\cos(n\theta)$$

$$x=\cos(\theta)$$

$$T_{n}(x)=\cos(n\arccos x)$$

In this form it is not entirely obvious that it is indeed a polynomial.

$$z=e^{i\theta}=\cos(\theta)+i\sin(\theta)$$

$$z^{n}=e^{in\theta}=\cos(n\theta)+i\sin(n\theta)=(\cos(\theta)+i\sin(\theta))^{n}=\sum_{j=0}^{n}i^{j}\left(\begin{array}{c}
n\\
j
\end{array}\right)\cos(\theta)^{n-j}\sin(\theta)^{j}$$

Important Identities:

The Recurrence Relation

$$T_{k+1}=2xT_{k}-T_{k-1}$$

The Beat Identity

$$T_{m}T_{n}=\frac{1}{2}(T_{m+n}+T_{m-n})$$

$$\cos m\theta\cos n\theta=$$

Using $m=0$, $n=1$ this is also the recurrence relation.

If we place our samples at the zeros of a Chebyshev Polynomial

$$l(x)=\frac{T_{n}(x)}{2^{n-1}}$$

Very peculiarly, there exists a discrete analog of the Chebyshev Series.
The Chebyshev series os just the Foruier series in disguise, so perhaps
this is not that unusual.

Physical Basis of Chebyshev?

Taking a fourier series in $\theta$, then making the replacement

$$x=\cos(\theta)$$

Gives you chebyshev polynomials. In this way, you can find the
equivlaent of a great many things in Fourier theory in Chebyshev theory

Making a fourier series in x and then making the raplcement gives you
chebyshev fnctions of the second kind

Should we find full chebyshev and then economize until generlazition is
achieved? Similar to finding Fourier and low passing until
generallization

Newton Interpolation and Shadow Calculus
----------------------------------------

Shadow caclulus at unequal points would use unequal point inteprolation
formulas. Change of variables?

Lagrange Remainder

$$R(x)=\frac{1}{(n+1)!}f^{(n+1)}(\xi)l(x)$$

As the points get closer together

$$l(x)\rightarrow(x-a)^{n+1}$$

And this results compresses down to the result for Taylor Series.

This also expalins why chebyshev points are good, since they cause

$$l(x)=T_{n+1}(x)$$

Which means at least that factor of the remiander is bounded.

Can we use the sampling data itself to estimate $f^{(n+1)}$? Tempting.
And for unknown functions, difficult to see how you could avoid it.

Newton Series to get equivlant of Integral Representation of Error.

Extend These methods to other vector spaces. The pointwise max error
does not respect unitary tranaformation is the thing. That's why some
bases are better than others. Projection onto subspaces. Or sampling
coefficients in one basis (position), find best approximation in another
basis (polynomials) ideal to some other basis (max in position)

Lebesgue Constants

To Lagrange Inteprolate with more points than minimal, write effective
sampled values $f_{i}$ as linear combos of actual oversampled? No\....
That doesn't make sense.

Still, using the bare vandermonde seems naughty.

How does the condition number of vandermonde evolve with more samples?
If I use unfirom sampling, I can multiply columns by a factor, then
stack.

How do Newton Interp and Lagrange Interpolation play together?

$$[x_{1},x_{2},\ldots]l(x)=0$$

$$[x_{1},x_{2}](x-x_{1})(x-x_{2})=0$$

$$[x_{1},x_{2}]l(x)=$$

$$[x,x_{2}]l_{2}=l_{2}$$

$$[x,x_{1}]l(x)=\frac{l(x)}{x-x_{1}}$$

$$w_{j}=\frac{1}{[x_{j},x_{1}]l(x)}$$

Newton Interpolation as triganulat System
