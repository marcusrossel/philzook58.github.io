Why Wavelength Bounds our Ability to see
----------------------------------------

One argument is that when we scatter light off of

What we see is the Energy in light. Light dumps energy (via the electric
field of the wave doing work) onto the retina. Once sufficent energy has
been dumped the nerve fires and sends a signal into the brain, from
which . I back this picture up with no experimental or anatomic data
whatsoever, except I belive this is the common picture we all have.
Because it takes multiple periods of the wave to dump in sufficient
energy into the eye, the eye does not detect phase of the wave. Like a
bamboo fountain in a zen garden, it collects energy and then dumps
repeatedly (or a drinky bird).

The eye is not sensitive enough to detect single photons. It takes a
couple to trigger. I feel that Quantum Mechanics of the electrodynamic
field is mostly an irrelevant complication a to our discussion and I
shall treat the field classically.

The intensity of the field is equivalent to the time averaged Poynting
Vector or the distribution of the time averaged Poynting Vector. Since
the Poynting vecotr indicates work done by the electromagnetic field,
intensity is the thing that we measure with our eye.

A simpler model might be measurement with a large number of thermometers
attached to a screen. As the field does work, it heats up the screen,
and thus we may indrectly detect light.

Is it truly fundamental that we may only see Intensity? No. We may build
detectors that are sensitive to phase. Classically speaking, there is no
distinction at all between RF scattering and light scattering except for
a trivial scaling. The scattering of RF off a 10m ball is the same as
red light scattering off a 500nm ball. Even the Field in a wire, which
is easily measured by an oscilloscope, is more or less just a scalings
difference from Light. Or light in an optical fiber compared to
mircowaves in a metallic waveguide.

Nevertheless in most ordinary situations, Intensity is the quantity of
interest.

What we need to see depends on our model of what we can possibly see. At
the extreme, if we knew exactly what we'll see, we don't need to look at
all. At the other end, if we allow any image to be present to our eyes,
we'll never be able to decrypt it in its entirety with the ultimately
fninte amount of data God gives us.

What we are able to discern also depends on how noisy our measurments
are. Even if an ideal camera could

If we need to only discern between one gaussian lump and two, we may be
able to do this even when the lumps are quite overlapped. This is a very
simple model two make. If we need to pick one out of 1000 possible
function to fit the data, then we may need more data and a clearer
signal.

$$Discernment=\frac{Datapoints-Noise}{ModelPossibilities}$$

Once we allow noise into consideration, then we're woorking
porbabilistically, so your discernement also depends on how often you're
willing to be wrong. I doubt you'll ever get to 50-50 on yes-no
questions

We deevelop of a model of everything we oculd possibly know, then we run
it through a grinder, chopping off limbs. Maybe we square so we only
have amplitudes, or stick ourselves outside the reigon of interest.

Thermodynmaics is ultimately a very simple model for some very complex
behavior with a massive amount of data points. That might be the origin
of its success.

The first order in the Born Approximation of the scattering of a wave is
given by

$$<f|V|i>$$

This is the same as Fermi's Golden Rule.

If we use plane waves as our incoming and outgoing, we get

$$\int d^{3}xV(x)e^{i(k-k')\cdot x}$$

If the scattering is elastic, then we have the condition

$$|k|=|k'|$$

$$q=(k-k')$$

It is clear then that (by Cauchy-Schwartz probably)

$$|q|\le2|k|$$

If we fix the incoming wavevector in the $\hat{z}$ direction, q
describes a circle of radius $2|k|$ resting on the $k_{z}$ axis.

If we allow the dirction to vary, we could in thoery by arranging the
incoming radation and outgoing detector determine the fourier coeffcient
of the potential for any $|q|<2|k_{0}|$. This means we achieve a banded
dtermination of the potential.

The Sampling Theorem
--------------------

There are a couple of preliminary results that are quite useful.

First off, in the vector space of periodic functions, the identity is
given by a complete set of states

$$I=\sum_{n=-\infty}^{\infty}|k_{n}><k_{n}|$$

A basis is a set of vectors $\{e_{n}\}$ that spans a space. A basis in
one space may not be a basis for a larger space. For example the
functions $e^{i2\pi nx}$ are a basis for functions in the interval
$[0,1]$, or for the set of functions with period 1, but not for any
function from $(-\infty,\infty)$. An orthonormal basis means that
$e_{n}^{\dagger}e_{m}=\delta_{mn}$. From any old basis we can rearrange
them into an orthonormal one if we desire.

### Dirac Comb

We can find a fourier series for a $\delta$ function at the origin.

$$f(x)=\sum_{n}a_{n}e^{i2\pi nx}$$

$$a_{n}=\int_{0}^{1}dxf(x)e^{-i2\pi nx}$$

$$a_{n}=\int_{0}^{1}dx\delta(x)e^{-i2\pi nx}=1$$

If we extend the domain to outside the interval $[0,1]$ what we insetad
have is the periodic extension of the Dirac Delta, the Dirac comb

$$\amalg(x)=\sum_{n=-\infty}^{\infty}e^{i2\pi nx}=\sum_{j=-\infty}^{\infty}\delta(x-j)$$

Wha about the Fourier Transfrom of this comb?

$$\tilde{\amalg}(\omega)=\int dxe^{-i2\pi x}\sum_{j=-\infty}^{\infty}\delta(x-j)=\sum_{j=-\infty}^{\infty}e^{-i2\pi jx}=\amalg(\omega)$$
We have the remarkable fact that the Dirac Comb is the fourier transform
of itself. Or in general by the scaling theorem, the trnasform of a
scaled dirac comb is given by an anti-scaled dirac comb

$$\mathsf{\mathrm{\mathscr{F}[\amalg(ax)]=\frac{1}{a}\amalg(\frac{\omega}{a})}}$$

### The Fourier transform of a box function

$$\Pi(x)=\begin{cases}
1 & -1<x<1\\
0 & \text{otherwise}
\end{cases}$$

$$\tilde{\Pi}(\omega)=\int e^{-i2\pi\omega x}\Pi(x)dx$$

$$=\frac{1}{-i2\pi\omega}e^{-i2\pi\omega x}]_{-1}^{1}=\frac{\sin(2\pi\omega)}{\pi\omega}$$

The Fourier Transform of a box is a sinc function.

### Convolution Theorem

### Sampling

By multiplying a function by our dirac comb, we leave behind a ssampling
of sorts by what is sometimes called the sifting property of the delta
function

$$f(x)\amalg(x)=f(x)\sum_{j=-\infty}^{\infty}\delta(x-j)=\sum_{j=-\infty}^{\infty}f(j)\delta(x-j)$$

Now we do not have any of the information left on our function between
our sample points.

Let us go to the Fourier domain. By the convolution theorem

$$f(x)\amalg(x)\Rightarrow\tilde{f}(\omega)*\tilde{\amalg}(\omega)$$

However

$$\tilde{\amalg}(\omega)=\amalg(\omega)$$

$$$$

### The ridiculous p\[arts of the theorem

No physical sampling is likely to take exact point-like samples. Nor are
signal expected to be bandlimitted often.

### Generalized Sampling

Appromxation in the context of vector spaces usually means least square
minimization, doing the best you can with what you're given. For
exmpale, if you're constrained to a subspace spanned by a fixed number
of vectors, you can pack them into A. Then multiplying A times a vector
of coeffcients will reconstitue your vector like orange juice.

$$A=\left[\begin{array}{cccc}
| & | & | & \ldots\\
a_{1} & a_{2} & a_{3} & \ldots\\
| & | & | & \ldots
\end{array}\right]\left[\begin{array}{c}
\lambda_{1}\\
\lambda_{2}\\
\lambda_{3}\\
\ldots
\end{array}\right]=\sum a_{n}\lambda_{n}$$

$$Lu=j$$

This can come from a minimization problem

$$\text{min}(Lu-j)^{2}$$

But now if we want constraint into the subspace spanned by A

$$\text{min}(LA\lambda-j)^{2}$$

One method for considering generalzied sampling is to look at the
interplay of vector subspaces.

Multiplying by the comb function is a projection onto a comb-like
subspace. Convolution by the dirac comb is an idenity operation on the
bandlmittied subspace. The subspace in particular is the bandlimitted
subspace. Mutlpiplciation by the box function in the Fourier domain is a
porjetion onto bandlmittied subspace.
