Error Bound for matrix or error bound for vector

$$|u-u_{h}|=e\le\left[\begin{array}{c}
f''(\xi_{1})\\
\\
\\
\\
\end{array}\right]$$

Or

$$|L-L_{h}|\le$$

$$|G-G_{h}|\le$$

What space to work in? Can project G into finite space or interpolate Gh
into total inifnite space. Or some combo thereof.

residual error (proably easier to work with) The so called error norm.

$$|Lu-f|=|r|\le\left[\begin{array}{c}
f''(\xi_{1})\\
\\
\\
\\
\end{array}\right]$$

HOw does the relationship go?

$$r=Le$$

So condition number tells us how well relative residual bounds relate to
error bounds.

$$\frac{r}{f}=\frac{Le}{Lu}$$

$$\frac{e}{u}=$$

We need error to be nondimensional to be physical.

Early stopping in born series, conjugate gradient, asymptotic series.
Perhaps at some point in asymptotic series, we get weiner filter
condition (amplify signal with minimal noise amplification), but
afterwards we flip. I'm not super duper sure this is a thing in a
iterative linear solution. Best basis is noise in one subspace and
signal in another (as much as possible anyhow). SVD used usually to find
it.

Bounds of derivatives from bounds on fourier coefficients? something
like if k\>k0 $f(k)\le c\frac{1}{k^{p}}$ then $f'(x)\le c+k_{0}f_{max}$.
In the squared sense. Spectral optimiszation.
