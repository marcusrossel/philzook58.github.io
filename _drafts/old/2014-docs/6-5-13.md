Log Infromation matrix times possible interval vector sqyared= bits

If we want to make an informed choice on the next experimental step we
should take, use conditional distriubtion on already taken sample.
F=$E[\partial_{\theta}^{2}L(x|x_{0},\theta)]$. Somehow include fisher
information matrix that we already have
$\partial_{\theta}^{2}L(x_{0}|\theta)$. Could use current sample as a
prior on

$I_{2}I_{1}^{-1}$infromation gain? $trI_{2}I_{1}^{-1}$. I think this
works. Since we can wraparound eigenespansions. Yes. Bits gained =
$\log trI_{2}I_{1}^{-1}$ . Strange that this forumula does not cascade
right. $\sum\log trI_{i+1}I_{i}^{-1}\ne\log trI_{N}I_{1}^{-1}$. If we
have experiment options, we could use this to find direction of maximal
gain. No. use det.

It is called D-optimality in optimal diesgin. Not quite actually.

Optimizing prediction and optimizing estimates may not be the same
thing?

eigenvectorts are sum of paramters we know best,worst.

Could be considered as encoding interval or a bayesian prior
distribution varaince

Hyoptheisis questions information matrix - fractions of bit

Encode floating point in prability exp and mantissa

On Binary expnasion, use percetpron, logistic, kernel trick. estimae
probability with paramter. Single rela number encoded as infitnite
discrete questons. can then use discrete learnng theory and VC
dimension. Calculate bits of accuracy from 10 Calculatre infromation
boost insetad of total infromation.

Typically we know the high digits and don't know low digits,
oscillatory, until end where probability falls to 1/2. Enevelopped
oscillation function. This looks like a logistic function. Logitstic
regression on a ln(x) scale.

Connection between Fisher infromation and Condition Number. Well, fisher
matrix may be modified by a double jacobian. This gives a factor of
absolute condition number squared possibly.

So the infromation on j is given by

$$e^{-(\phi-Gj)^{2}\Sigma^{-1}}$$

We could use An upsampled j, $Uj$, to reduce the number of parameters.
Or include smoothing term to make G not quite so harsh.

Information matrix $G\Sigma^{-1}G$. That seems fine? Roughly constant
info at and below wavelnegth, decays afterwards. maximal info at
wavelength. decays afterwards at $n^{-4}$. Then lesson seems to be that
you can do well below k with little precision, but after k you need more
lots more precision to do well.

Need to examine prediction estimates then. Even though the estimates of
j might be okay, they might go radically astrary.

Value inaccruacy is easy to deal with, but what about spatial
inaccruacy? (1,0,0,0,0) and (0,1,0,0,0) might be far vectorially, but
they are very close experimentally. Not quite so bad seeming if the lump
is more distributed.

The necessity of only receiving data on the exterior of the region
(boundary or perhaps volumetric. Volumetric might be overkill, but we
should be able to handle the degenreacy)

Okay. So it seems obvious that given complete field perfectly, we can
find complete j minus the cokernel of L. Wait\... maybe not. if the
field has vanishingly smal but nonzero $\Sigma$, the fast that G
Explodes with a pole might still kill all infromation.

What is the equivaent of the Helmhotlz kirchoff integral for matrix
version of wave equation?

The clebscjh gordon coeffcicents is the equivalent of the beat equations
for spehrical harmonics (rexpression products back into linear). Its
just that legendre polynomials are shitty functions of cosine. We could
reqrite legendre as a fourier series. Clebsch gordon come as
combinatorics for a bunch of uses of fundamental beat combos. fourier
series/taylor series on $CP^{1}$

### Sufficient Statistics

You can find a function of the samples that gives as much information
about the

I think the obvious analystic choice is the linear one, defining a fat
matrix S

$$t=Sx$$

for example, the matrix S that will construct the mean is

$$\frac{1}{N}\left[\begin{array}{ccc}
I & I & I\end{array}\right]$$

With all your samples packed into a single vector x, each vector
$x_{i}$being from sample i

$$x=\left[\begin{array}{c}
x_{1}\\
x_{2}\\
x_{3}
\end{array}\right]$$

Then if the smapples are i.i.d., the total joint probability
distribution is

$$\exp(-\frac{1}{2}(x-m)^{T}\left[\begin{array}{ccc}
\Sigma^{-1} & 0 & 0\\
0 & \Sigma^{-1} & 0\\
0 & 0 & \Sigma^{-1}
\end{array}\right](x-m))$$

Since we think that that there is really only one sample worth of
parameters, we can upsmple with a thin matrix

$$m=\uparrow\mu$$

$$\uparrow=\left[\begin{array}{c}
A\\
A\\
A
\end{array}\right]$$

Now we can easily use correlated samples using the off diagonals.

The information matrix for this is

$$F=\uparrow^{T}\left[\begin{array}{ccc}
\Sigma^{-1} & 0 & 0\\
0 & \Sigma^{-1} & 0\\
0 & 0 & \Sigma^{-1}
\end{array}\right]\uparrow$$

If we do some block summation over samples, we get 1 factor from each
sample giving n in total.

$$F=nA^{T}\Sigma^{-1}A$$

So to list the sizes at play:

n is the number of samples

j is the number of things measured per sample, the size of a vector
$x_{i}$

nj is the size of vector x, m

r is the number of parameters, the size of vector $\mu$

The matrix A is $j/r$

The matrix $\uparrow$ is $nj/r$

The matrix S will be $q/nj$

For polynomial regression

n is number of points evaluated at

j is 1, the value of the function at that point

r is the type of polynomial used.

If we also randomly pick the evalutation points, then this becomes a
mess.

So the objective is to pick S such that we maxmimize the new information
matrix in some manner. We can build a joint distribution
$P(x,t|\mu)=P(x|\mu)\delta(t-Sx)$. Then we will marginalize out x. We
hsould write that delta function as a gaussian.

$$\exp(-\frac{1}{2}(x-m)^{T}\left[\begin{array}{ccc}
\Sigma^{-1} & 0 & 0\\
0 & \Sigma^{-1} & 0\\
0 & 0 & \Sigma^{-1}
\end{array}\right]\left[\begin{array}{c}
s\\
x
\end{array}\right])$$

$$\exp(-\frac{1}{2}(x-m)^{T}\left[\begin{array}{ccc}
\Sigma^{-1} & 0 & 0\\
0 & \Sigma^{-1} & 0\\
0 & 0 & \Sigma^{-1}
\end{array}\right](x-m)+\alpha(t-Sx)^{2})$$

Rewrite this as a complete square. Then do schur complement trickery to
marginalize out x. We'll get a matrix something like this

$$\left[\begin{array}{cccc}
\Sigma^{-1} & 0 & 0 & |\\
0 & \Sigma^{-1} & 0 & \alpha S^{T}\\
0 & 0 & \Sigma^{-1} & |\\
- & \alpha S & - & \alpha I
\end{array}\right]$$

I'm guessing that the S will mostly be projecting operators. and that
essentially we want singular vectors of the A?
