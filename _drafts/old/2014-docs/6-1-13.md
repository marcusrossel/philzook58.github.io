Curve Fitting anf Interpolation
-------------------------------

The fitting of curves to expiermental data and the interpollation of
numerical data have more in common than they do differences.
Experimental data will be cloaked in the language of porbability, while
numerics are discussed in error bounds. In both cases though, you have
to infer the best you can from sampled values. You may only perform a
finite number of experiments and compute values for a finite number of
function positions.

### Least Squares

First of, we have to be specific about what kind of error we want to
optimize. The classic is the least squares error. We compare our
estimate function evaluated at our sample points$g(x_{i})$ with the
values it should ideally return $y_{i}$

$$E_{LS}=\sum_{i}(y_{i}-g(x_{i}))^{2}$$

### Chebyshev

Picking the points. Quadrature. Minimax. Miniripple

### What is Linear?

### Vandermonde

Consider the polynomials

$$y(x)=\sum a_{n}x^{n}$$

The coeffcients $\{a_{n}\}$ can be packed into a vector. This makes
sense in some ways, since we may add polynomials and perform operations
n polymoials that end up being linear in the coeffcients (such as
differentiation), and multpiplying polynomials is the same and
convolving their coeffcicents. Actually, the coefficients of polynomials
is one of the canonical examples of a vector space.

We could evaluate this polynomial at a set of point $\{x_{n}\}$ by using
a rather strange matrix

$$\left[\begin{array}{c}
y_{1}\\
y_{2}\\
y_{3}\\
y_{4}\\
y_{5}
\end{array}\right]=\left[\begin{array}{ccccc}
1 & x_{1} & x_{1}^{2} & x_{1}^{3} & x_{1}^{4}\\
1 & x_{2} & x_{2}^{2} & x_{2}^{3} & x_{2}^{4}\\
1 & x_{3} & x_{3}^{2} & x_{3}^{3} & x_{3}^{4}\\
1 & x_{4} & x_{4}^{2} & x_{4}^{3} & x_{4}^{4}\\
1 & x_{5} & x_{5}^{2} & x_{5}^{3} & x_{5}^{4}
\end{array}\right]\left[\begin{array}{c}
a_{0}\\
a_{1}\\
a_{2}\\
a_{3}\\
a_{4}
\end{array}\right]This$$ matrix is called the Vandermonde matrix. We can
rewrite this in vector notation

$$y=Va$$

This is quite a nice matrix since we can use it to find coefficients for
an N-degree polynomial from N sample points.

$$a=V^{-1}y$$

Even better, we can find fitting polynomials. Using the least squares
error

$$E_{LS}=\sum_{i}(y_{i}-P(x_{i}))^{2}$$

We can rewrite this in vectr form as

$$E_{LS}=(y-Va)^{T}(y-Va)$$

Note that V is now not square. It is tall and skinny. We may
differentiate with respect to a to find the minimum

$$V^{T}Va=V^{T}y$$

This is called the Normal equation. We solve it

$$a=(V^{T}V)^{-1}V^{T}y$$

The combination on the right hand side is called the pseudo-inverse of
the matrix.

$$V^{+}=(V^{T}V)^{-1}V^{T}$$

It is indeed the left inverse of the rectangular matrix in the sense
that

$$V^{+}V=(V^{T}V)^{-1}V^{T}V=I$$

In many rectangular matrix situations, you can blindly replace what
would ordinarily be the inverse with the pseudo inverse and get what you
wanted, hence its conceptual utility.

The simple monomial basis is not the only basis for polynmials. We could
also work with the Chebyshev polynomials, or Legendre Polynmoials, or
what have you. To estimate the coefficients in these other basis, they
are related to the coefficents in the monomial basis by linear
transfromations.

The Vandermonde matrix is a numerical shit pie however.

The eigenvalues of the Vandermonde matrix are

The determinant of the Vandermonde matrix

Lagrange polynomials

We can easily find a polynomial that has zeros at a set of points. If we
want a polynomial that is zero at all our sample points except $x_{j}$we
get

$$\tilde{l}_{j}(x)=(x-x_{1})(x-x_{2})(x-x_{3})\ldots(x-x_{j-1})(x-x_{j+1})\ldots$$

This function will have some wacky value at $x_{j}$ itself. We can
normalize it by the constant $\tilde{l}_{j}(x_{j})$

$$l_{j}(x)=\frac{\tilde{l}_{j}(x)}{\tilde{l}_{j}(x_{j})}$$

Now we have

$$l_{j}(x_{i})=\delta_{ij}$$

So, if we have

$$P(x)=\sum_{i}y_{i}l_{i}(x)$$

This polymmoial does fit exactly at the sample points

$$P(x_{j})=\sum_{i}y_{i}l_{i}(x_{j})=\sum_{i}y_{i}\delta_{ij}=y_{j}$$

$$l(x)=(x-x_{1})(x-x_{2})(x-x_{3})\ldots$$

$$\tilde{l}_{j}(x)=\frac{l(x)}{x-x_{j}}$$

$$w_{j}=\frac{1}{\tilde{l}_{j}(x_{j})}$$

We get the barycentric formula

$$P(x)=\sum w_{j}y_{j}\frac{l(x)}{x-x_{j}}$$

Newton Series

Barycentric algorthim

Matrix Properties

Sylvester Matrix

$V^{-1}$has monomial coefficents of Largange basis $VV^{-1}=I$

### Overfitting

Stability of error/intepolation. Why does more data points make SVD of
data more stable? Update formula for SVD, Recursive least squares

### Taylor Series Error

We start with s simple identity

$$f(x)=f(a)+\int_{a}^{x}f'(t)dt$$

We can integrate by parts.

$$u=f'(t)$$

$$dv=dt$$

$$v=t-x$$

$$du=f''(t)dt$$

$$f(x)=f(a)+f'(t)(t-x)]_{a}^{x}-\int_{a}^{x}f''(t)(t-x)dt$$

$$f(x)=f(a)+f'(a)(x-a)-\int_{a}^{x}f''(t)(t-x)dt$$

We can do so again

$$u=f''(t)$$

$$dv=(t-x)dt$$

$$v=\frac{1}{2}(t-x)^{2}$$

$$du=f'''(t)dt$$

$$f(x)=f(a)+f'(a)(x-a)-f''(t)\frac{1}{2}(t-x)^{2}]_{a}^{x}+\int_{a}^{x}f'''(t)\frac{1}{2}(t-x)^{2}dt$$

$$f(x)=f(a)+f'(a)(x-a)+\frac{1}{2}f''(a)(x-a)^{2}+\frac{1}{2}\int_{a}^{x}f'''(t)(t-x)^{2}dt$$

And so on. Clearly, we are developing the Taylor expansion, but what is
interesting is the error integral

$$E_{n}(x)=\frac{1}{n!}\int_{a}^{x}f^{(n+1)}(t)(t-x)^{n}dt$$

This term contains the difference between the function and the truncated
Taylor series $T_{n}(x)$. If tthis integral is directly evaluatable,
then approximations are silly. Its use is in bounding the error. Any
bound on this integral becomes a bound on the error of the Taylor
series. We could look at the pointlike error

$$|f(x)-T_{n}(x)|=|E_{n}(x)|<C$$

Or we could look at some kind of average square error

$$\int_{a}^{b}|f(x)-T_{n}(x)|^{2}dx=\int_{a}^{b}|E_{n}(x)|^{2}<C$$

A very simple bound of any integral is given by

$$\int_{a}^{b}g(x)dx\le\int_{a}^{b}\max_{t}g(t)dx=\max_{t}g(t)(b-a)$$

The integrand is always less than its maximum value, so if we replace
the function with its maximum value inside the integral, we get a bound.

For example, a bound on the error for $f(x)=e^{x}$,
$T_{2}(x)=1+x+\frac{1}{2!}x^{2}$ on the interval $[0,1]$ is given by

$$E_{n}(x)=\frac{1}{2!}\int_{0}^{x}e^{t}(t-x)^{2}dt\le\frac{1}{2!}\int_{0}^{1}e^{t}(t-x)^{2}dt\le\frac{1}{2!}\int_{0}^{1}e^{1}(1-x)^{2}dt=\frac{e}{2}(1-x)^{2}$$

### Lagrange Remainder

The mean value theorem says there is a place where the slop of the
secant matches the slope of a tangent somewhere inside the secant
region. Try it.

For some $a\le\xi\le b$

$$\frac{f(b)-f(a)}{b-a}=f'(\xi)$$

We can also constyruct the taylor series from the bottom up.

$$f^{(n-1)}(x)=f^{(n-1)}(a)+\int_{a}^{x}f^{(n)}(t)dt$$

$$f^{(n)}(x)-f^{(n)}(a)=(x-a)f^{(n+1)}(\xi)$$

$$f^{(n)}(x)=f^{(n)}(a)+(x-a)f^{(n+1)}(\xi)$$

$$$$

$$f^{(n-1)}(x)=f^{(n-1)}(a)+\int_{a}^{x}f^{(n)}(a)+(t-a)f^{(n+1)}(\xi)dt$$

We can do both of these integrals immediately

$$f^{(n-1)}(x)=f^{(n-1)}(a)+f^{(n)}(a)(x-a)+\frac{1}{2!}(x-a)^{2}f^{(n+1)}(\xi)$$

And so on until we get all the way back to the beginning

$$f(x)=f(a)+f^{1}(a)(x-a)+\frac{1}{2!}(x-a)^{2}f^{(2)}(a)+\ldots+\frac{1}{n!}f^{(n)}(a)(x-a)^{n}+\frac{1}{(n+1)!}(x-a)^{n+1}f^{(n+1)}(\xi)=T_{n}(x)+\frac{1}{(n+1)!}(x-a)^{n+1}f^{(n+1)}(\xi)$$

The point of knowing that $\xi$ exists is to find a bound

$$f(x)-T_{n}(x)=E_{n}(x,\xi)\le\max_{\xi}E_{n}(x,\xi)$$

### Regularization and Cross-Validation

We do not trust the computer, or at least you should not. Occasionally,
it spits back out somethng that you just know is total bullcrap, a
result that can't be right. Then you go back thorugh your code and see
where you went awry. This is an indication that we don't really go into
the process blind. We always have some kind of checks and balances, some
kind of rough idea what answer we're looking for, even if only a range
of magnitudes.

There are some interperations of this. The Bayesians say that you should
quantify your prior expectations as well as you can and include them in
your analysis. Then the computer will feel some weight to not defy them.

Another intepretation is that it is not that our intuition needs to be
included, but that our intuition stems from a principle of
generalization. Over the years, we get a feeling for when the amount of
data we have is sufficient to generalize outside of the data.
