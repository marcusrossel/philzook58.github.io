A field of absolute conics, parameter estimation required for all of
them dscribes GR. Or dsicrete metric field

Assume we camputre enetire helmholtz field at noise level
$|\Delta\phi|<\epsilon$, spatisal accuracy $\Delta x$? Perhaps go point
by point estimates and regularize field by putting prior on it that
litted it going nuts. Since ntropy goes as logN, ratio of limit to
accuracy, adding a little to limit doesn't matter much. Also cross
validation. Maybe we could do analysis scaling $\Delta x$ down and
hitting a crossover somwhere. I would like to avoid requring spatial
accuracy. Then we have a bit content to field (via kullback-liebler)? To
determine j we apply L. Condition number of L subtracts off bits of
info. If we make L with lots of entires (finely spaced), condition
number gets worse and worse.

The question is, where does wavelnegth matter? Is it because we image
externally, or is does it occur even if we had all the marbles.

Is cross validation, regularization and such the way to save continuos
entropy? Bayesian wise, we can talk about entropy gain if we had prior.
But from just letting data talk for itself,

$\kappa(N),\kappa(\Delta x),\kappa(\Lambda)$ are quantities of interest
and their asymptotic behavir as a function of dimension and such.
$\kappa(\Delta x,L,k^{2})=\frac{\lambda_{\frac{L}{\Delta x}}}{\lambda_{0}}$.

$\lambda_{0}=-(\frac{\pi}{L})^{2}+k^{2}$

If you hit a pole in the spectrum at some point in increasing your
resolution, that would cap out the condition number.

$\kappa\le||G||||L||$

$\kappa=||L||\frac{||x||}{||Lx||}$

The number of entries in our vectors will be
$N=(\frac{L}{\Delta x})^{d}$

$||L||\propto N^{2/d},(\frac{\Delta x}{L})^{2}$

$||G||=pole/resonance$

$\frac{1}{-(\frac{N\pi}{L})^{2}+k^{2}}$

The less than equal is unconvincing though.

The Helmholtz equation kind of looks exactly like tikhonov
regularization with regulariuzation parameter $k^{2}$.

Helmholtz only solves for one $\omega$ component of source
$j(\omega,x)$. Should I work with full time varying wave equation?

why condition number is is inofrmation decay. Fisher infromation?

Relative Fisher information $I*\theta^{2}$$I*\theta$?
$I*\sigma_{\theta}^{2}$? estimates how close you are the rao bound.
bsically paramrtization invariant.

Could put floating point bounds in porbabilsitic language. A compact
support distribution of error. Markov bound type things.

what about taking just a simple derivative?

born series and decay of inoformation. The longer the chain of cause
effect, the tougher? GGG each time compounds condition number. If
condition number is really a physical thing.

How far should a series go?

Multiscale KL divergence. Use estimate from prior scale and see how much
more data is needed. Might cap out at $\lambda$scale.

The eikonal approximation fails for small feautre sizes. We could take
this as an indiciation. But it does not mean things can't be seen, maybe
we just need to change our analysis.

Condition Number
----------------

Fisher Information
------------------

The goal is to estimate a parameter in a probability distribution. For
example, the gaussian

$$N(\mu,\sigma^{2})$$

We could take samples from the distribution and then

Or even simpler, pulling blue and red balls from a box, estimate the
proportion of blue to red balls. The obvious estimate is that the
proptiopn of balls in the box is going to be close to the proprtion we
actually pull out and see. We intuitively feel that our estimate will
get better and better the more balls we pull.

If the ratio of red balls is

$$p=\frac{\text{\# Red Balls}}{\text{\# Total Balls}}$$

Then the porbability of picking a red ball is p and the porbability of
picking a blue ball is $1-p$. The probability of picking r red balls and
b blue balls is $p^{r}(1-p)^{b}$

### The Likelihood Function

The likelihood function is the porability of getting the data you get,
given as a function of the paramter you don't know. In our ball case

$$L(r,b|p)=p^{r}(1-p)^{b}$$

The classic method of parameter estimation is the maximum likelihood
approach. We fix r and b to the values we actually found and maximize
with respect to p. This makes sense as a principle, since it is good to
assume that what happened was likely to happen, but it is certainly not
proof of anything really. We often take the lograithm of the likelihood,
since the maximum of a log is the same as the maximum of the argument,
and typically likeihoods are composed of a large amount of
multiplicative factors.

$$\frac{\partial}{\partial p}\ln L(r,b|p)=0$$

$$\frac{r}{p}+\frac{b}{1-p}=0$$

$$p=\frac{r}{b+r}$$

We get exactly what our intutive guess of the estimate should have been.

If we consider r and b to be random variables, then we see that p is
also a random variable. We can therefore find the variance of p and
other stiastical porperties

### Information

How reliable is the estimate given by the maximum likelihood principle?
We use the maximum, but it is possible that this maximum is very steep
or shallow. If the function is very shallow, then a particular choice of
p may not be all that much better than another choice. The maximum
likeihood principle feels much safer when one choice dominates the other
ones. If not, then maybe a slightly different sample or a slightly
different estimation principle might give radically different results.

The steepness at the point of estimation is given by
$$\frac{\partial^{2}}{\partial p^{2}}\ln L$$

Since this is the term in the taylor expansion about that point

$$L\approx\ln L(p_{0})+0\Delta p+\frac{1}{2}(\Delta p)^{2}\frac{\partial^{2}}{\partial p^{2}}\ln L$$

If we want to conisder all the possibitites we might have had to deal
with, we can look at the average sharpness of the Log-Likelihood

$$I(p)=E_{r,b}[\frac{\partial^{2}}{\partial p^{2}}\ln L|p]$$

If we consider $\hat{p},$the estimator considered as a random variable,
then the Fisher Infromation is approximately given by

$$I(p)\sim\frac{1}{\sigma_{p}^{2}}$$

This statement is exactly true for the estimate of the mean parameter in
the gaussian drisribution. It is a statement for rough estimation and
understanding of the Fishera information, very similar to the identity

$$\Delta x\sim\frac{\hbar}{\Delta p}$$

Whereas the latter is dignified by assuming the best with the Heisneberg
Uncertasinty inequality, the first assume the best out of the Cramer-Rao
inequality.

If the samples are indepedant, then the infromation is proportional to
the number of samples. Thisd is the main vidnication to using the
Log-lieklihood instead of liekhoold.

$$I\propto N$$

### Data Reduction

Numerical Analysis
------------------

In the continuous case, it is not intrtsiniscally clear what doing a
good job entails.

If you want to work with relative quantities, use $\ln x$. Then an
overall scalling does not effect an interval size.
$\Delta\ln x=\Delta\ln\alpha x$

### Conditioning

The condition number gives a bound on how bad error can be amplified by
an operation. You may measure error however you want. You may work with
relative or absolute error or squared error or any cockammie system you
like.

An error in x gets multiplied by the Jacobian.

$\frac{\partial\ln f}{\partial\ln x}=\frac{\partial f}{\partial x}\frac{x}{f}$

What about a multivariable function?

### Stability

The definition of continuity is a little game. A stranger gives you a
little interval of size $\delta$ in around $y_{0}$ in the output of a
function. YOu have to provide a little range $\epsilon$ centered around
$x_{0}$that you can gurantee will map into his interval, regardless how
small. In other words, your job is to find a function$\epsilon(\delta)$
that will work.
