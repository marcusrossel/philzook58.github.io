### Orthogonal Polynomials

Ordinary orthogonal function theory wokrs on continuous intervals. For
exmaple, when we rothognalize the polynomials on the interval $[-1,1]$
with the inner product

$$<f,g>=\int_{-1}^{1}f*(x)g*(x)dx$$

We get the Legendre polynomials.

This is the equivlanet of QR factorization.

Weighted Inner Product is needed to produce certain types of series
(Hermite, Chebyshev, etc.)

$$<f,g>=\int_{-1}^{1}f*(x')g*(x)w(x,x')dx'dx$$

One could consider the sampled point inner product as letting
$w(x,x')=\delta(x-x')\sum_{i}\delta(x-x_{i})$

We can define orthogonal polymmials on the domain of points from the QR
of the Vandermonde matrix

$$\left[\begin{array}{ccccc}
1 & x_{1} & x_{1}^{2} & x_{1}^{3} & x_{1}^{4}\\
1 & x_{2} & x_{2}^{2} & x_{2}^{3} & x_{2}^{4}\\
1 & x_{3} & x_{3}^{2} & x_{3}^{3} & x_{3}^{4}\\
1 & x_{4} & x_{4}^{2} & x_{4}^{3} & x_{4}^{4}\\
1 & x_{5} & x_{5}^{2} & x_{5}^{3} & x_{5}^{4}
\end{array}\right]=V=QR$$

The columns of Q give the values at the point, but more important, the
$R^{-1}$matrix will take us from V to Q. The continuous domain
equlivaent of the Vandermonde matrix is a $\infty/5$ matrix.

$$\left[\begin{array}{ccccc}
| & | & | & | & |\\
1 & x & x^{2} & x^{3} & x^{4}\\
| & | & | & | & |
\end{array}\right]R^{-1}=\text{Something}$$

In this way, a set of points can be mapped to a set of polynomials.

The Lagrange polynomials are the delta function for finite domains

This domain extension is the same as using a fourier series on an
interval and then using the periodic extension outside the interval.

### Lebesgue Constant

It is strange, but the choice of sample points changes the behavior of
the apporximating polynomial drastically

### Chebyshev Points

### Least Squares from Probability

Let's say we want to determine the mean value of a random vector from
sampling. We assume a Gaussian Distribution

$$P(x)=\frac{1}{\sqrt{(2\pi)^{N}\det\Sigma}}e^{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)}$$

Where $\Sigma$ is the covairnace matrix

$$\Sigma=\left[\begin{array}{cccc}
\sigma_{1}^{2} & \sigma_{1}\sigma_{2} & \sigma_{1}\sigma_{3} & \ldots\\
\sigma_{2}\sigma_{1} & \sigma_{2}^{2} & \sigma_{2}\sigma_{3} & \ldots\\
\sigma_{3}\sigma_{1} & \sigma_{3}\sigma_{2} & \sigma_{3}^{2} & \ldots\\
\ldots & \ldots & \ldots & \ldots
\end{array}\right]$$

The Lieklihood function is

$$L(\{x_{i}\}|\mu,\Sigma)=\prod_{i}\frac{1}{\sqrt{(2\pi)^{N}\det\Sigma}}e^{-\frac{1}{2}(x_{i}-\mu)^{T}\Sigma^{-1}(x_{i}-\mu)}$$

$$\ln L(\{x_{i}\}|\mu,\Sigma)=\sum_{i}\frac{-N}{2}\ln2\pi-\frac{1}{2}\ln\det\Sigma-\frac{1}{2}(x_{i}-\mu)^{T}\Sigma^{-1}(x_{i}-\mu)$$

$$\frac{\partial}{\partial\mu}\ln L(\{x_{i}\}|\mu,\Sigma)=0$$

This will lead to minimization of the least squares looking quantity

$$\sum_{i}\frac{1}{2}(x_{i}-\mu)^{T}\Sigma^{-1}(x_{i}-\mu)=\sum_{i}\frac{1}{2}(x_{i}^{T}\Sigma^{-1}x_{i}+\mu{}^{T}\Sigma^{-1}\mu-2\mu{}^{T}\Sigma^{-1}x_{i})$$

The first factor is irrelevent since it cannot be minimized

We can pack our samples into a matrix for convenience

$$X=\left[\begin{array}{cccc}
| & | & | & \ldots\\
x_{1} & x_{2} & x_{3} & \ldots\\
| & | & | & \ldots
\end{array}\right]$$

$$\sum_{i}\Sigma^{-1}\mu-\Sigma^{-1}x_{i}$$

$$\mu=\frac{1}{n}\sum_{i}x$$

The best guess of the mean is indeed what we usually think of as the
mean. Good.

But a slightly change in the analysis makest hings more interesting.
Instead of using just the bare means $\mu$let us insetad use the means
linearly parametized by a smaller vector $A\mu$

$$P(x)=\frac{1}{\sqrt{(2\pi)^{N}\det\Sigma}}e^{-\frac{1}{2}(x-A\mu)^{T}\Sigma^{-1}(x-A\mu)}$$

$$\sum_{i}\frac{1}{2}(x_{i}-\mu)^{T}\Sigma^{-1}(x_{i}-\mu)=\sum_{i}\frac{1}{2}(x_{i}^{T}\Sigma^{-1}x_{i}+\mu{}^{T}A^{T}\Sigma^{-1}A\mu-2\mu{}^{T}A^{T}\Sigma^{-1}x_{i})$$

$$\sum_{i}A^{T}\Sigma^{-1}A\mu-A^{T}\Sigma^{-1}x_{i}=0$$

$$nA^{T}\Sigma^{-1}A\mu=\sum_{i}A^{T}\Sigma^{-1}x_{i}$$

$$\mu=\frac{1}{n}\sum_{i}(A^{T}\Sigma^{-1}A)^{-1}A^{T}\Sigma^{-1}x_{i}$$

In a curve fitting situation, the matrix A will be our tall and skinny
Vandermode Matrix and the vector $\mu$ will be our polynomial
coefficients.

### Estimation of $\Sigma$

### Correlated Samples

### The Fisher Information

From the definition of Fisher information, we get the result that

$$I(\mu)=E[\partial_{\mu}^{2}\ln L(\{x_{i}\}|\mu)|\mu]=E[\sum_{i}A^{T}\Sigma^{-1}A]=nA^{T}\Sigma^{-1}A$$

### Recursive Least Squares

### Moments of Gaussians

### Solving Gaussians

Gaussian Elimination and Diagonilzation

The Gaussian is a linear algebra engine.

### Marginals of Gaussians

Generating function
