### Points

Points in projective space are given by 1-d subspaces. This means any
multiple of the same vector gives the same point. For example

$$\left[\begin{array}{c}
1\\
3
\end{array}\right]$$

$$\left[\begin{array}{c}
2\\
6
\end{array}\right]$$

give the same point.

In the pinhole camera model this refers to the fact that a point of
light on the 2-d screen corrspeonds to a ray of possible points in 3-d
space

### Lines

There are two main ways to specify objects in space, implicit and
explicit (parametization). In the explicit version, which people are
typically more comfortable with, the coordinates of a point in the
n-dimensional object are given as a function of n parameters. You build
upward to higher dimensional objects by adding more parameters. For
example

$$x=\cos(t)$$

$$y=\sin(t)$$

gives the 1-d arc of a circle in 1 parameter, t. The system

$$x=u^{3}v^{2}$$

$$y=u^{7}$$

$$z=u+3v^{2}$$

Gives some cockamamie 2-d surface parametized by two parameters u,v. The
explicit method is often preferable if you want to easily produce points
rather than check if a point is in the set you're looking at.

The implicit method instead uses a system of constraints to specify an
object. It whittles down the space until you're left with what you want.
In an overall N dimensional space, k constraints gives you an object of
dimension N-k. Implicit methods make it very easy to check if a guess
point is within or near the set you're looking at. It is often
convectional to write the constraints As equations set to zero. The
system

$$\phi_{1}(x,y,z)=x^{2}+y^{2}+z^{2}-1=0$$

$$\phi_{2}(x,y,z)=z-\frac{1}{2}=0$$

Specifies the points where the plane $z=\frac{1}{2}$ cuts the unit
sphere in 3-d. This is a little 1-d loop,
since$3-2\text{ Constraints}=1$

One can specify a line in $R^{2}$ implicitly by the equation

$$\phi(x,y)=ax+by+c=0$$

Any point $(x,y)$ which satisifies this equation lies on the line.

One can also specify it explicitly by

$$x=x_{0}+mt$$

$$y=y_{0}+nt$$

or in vector form

$$\vec{x}=\vec{x}_{0}+\vec{m}t$$

$\vec{m}$is a vecotr which points in the same direction as the line, and
$\vec{x}_{0}$is a point lying on the line.

$$\vec{m}=(\vec{x}_{1}-\vec{x}_{0})$$

Going back to our standard connection between $RP^{2}$ and $R^{2}$

$$\left[\begin{array}{c}
\frac{x}{w}\\
\frac{y}{w}
\end{array}\right]\Longleftrightarrow\left[\begin{array}{c}
x\\
y\\
w
\end{array}\right]$$

We find that the implicit version reads

$$\phi(x,y,w)=ax+by+cw=0$$

Defining a new row vector $$l^{T}=\left[\begin{array}{ccc}
a & b & c\end{array}\right]$$

We can reqrite this condition as

$$l^{T}x=0$$

A point lies on the line if its inner product with the homogenous row
vector associated with the line is 0.

The explicit version also gets translated.

$$\left[\begin{array}{c}
x\\
y\\
w
\end{array}\right]=\mu\left[\begin{array}{c}
x_{0}\\
y_{0}\\
w_{0}
\end{array}\right]+\lambda\left[\begin{array}{c}
x_{1}\\
y_{1}\\
w_{1}
\end{array}\right]$$

Describing Subspaces
--------------------

One definition of an n-dimensional subspace is the set of all possible
linear combinations (also called the span) of a set of n linearly
independant basis vectors, so one possible way to specify a subspace is
to give a set of basis vectors. The thing is that this is an arbitrary
specificaiton of the subspace. We can choose a great deal of basis
vectors that span the same subspace.

### Determinants

See: Linear Algebra by Gil Strang, Advanced Math II: Geometry by Klein

Determinants test the independancy of a full set of N vectors. Pop the
vectors into the columns of a matrix, take the Determinant, and if it
gives 0, they are not independant.

They may also be conisdered as a test for the solvability of a system of
linear equations. Pop the coefficients into a matrix, place it inside a
determinant, and if it gives nonzero, the system has a unique solution.

There are 3 fundamental rules of Determinants

1.  The determinant of the identity matrix is 1. The columns of the
    indentiy matrix are clearly independant, so no problem there.

2.  The determinant is linear in each individual row or column at a
    time. It is NOT linear in all of the rows at once
    $$|\left[\begin{array}{c}
    -\alpha a-\\
    -b-\\
    -c-
    \end{array}\right]|=\alpha|\left[\begin{array}{c}
    -a-\\
    -b-\\
    -c-
    \end{array}\right]|$$ $$\left[\begin{array}{c}
    -u+v-\\
    -b-\\
    -c-
    \end{array}\right]|=|\left[\begin{array}{c}
    -u-\\
    -b-\\
    -c-
    \end{array}\right]|+|\left[\begin{array}{c}
    -v-\\
    -b-\\
    -c-
    \end{array}\right]|$$ $$|\left[\begin{array}{c}
    -a+a'-\\
    -b+b'-\\
    -c+c'-
    \end{array}\right]|\ne|\left[\begin{array}{c}
    -a-\\
    -b-\\
    -c-
    \end{array}\right]|+|\left[\begin{array}{c}
    -a'-\\
    -b'-\\
    -c'-
    \end{array}\right]|$$ This also makes sense as regards to its job as
    a test of linear independancy.

3.  Row swapping flips the sign of the determinant. This is the most
    unexpected of the rules. Certainly it is not inconsistent with our
    hope of using the determinant as a depdnancy test, but it is not
    clear that it is necessary

From these three rules, you can determine all the properties of the
determinant

Why are determinants volumes?

A parallelogram can be specified by giving the set of vectors of its
sides.

You can slide the sides of a parrellogram without changing its volume.
The operation of sliding is adding a multiple of one side to another
side. The Geometric version of QR decomposition AKA the Gram-Schmidt
process is sliding the sides under you have right-anglicized the
paraellogram. The volume of a rectangular box is the product of its
sides by definition.

### Minors

What about lower dimensional parralegrams embedded in higher dimesional
spaces? How do we find the area of a parallogram in 3-d space? We have
only two side vectors, each of which has 3 components, so we cannot pack
that into a square matrix, we can only pack it into a skinny $n/N$
matrix.

$$A=\left[\begin{array}{cc}
| & |\\
a & b\\
| & |
\end{array}\right]$$

One suspects that a determinant like operation must be necessary.

The solution is

$$\det A^{T}A$$

The Grammian Matrix is the matrix of all the possible inner products

$$\left[\begin{array}{cc}
a^{T}a & a^{T}b\\
b^{T}a & b^{T}b
\end{array}\right]$$

This matrix is a natural matrix to look at during the Gram-Schmidt
process.

If the parallelogram is degenrate, then its sides are not linearly
independant.

The minors of a matrix are subdeterminants. You find a little square
subsets of a larger matrix and take the determinant.

Cauchy-Binet

Barycentric Coordinates
-----------------------

One of the original motivations for homogenous coordinates was the idea
of a barycenter. This is the center of gravity of a figure. We pick some
reference triangle. To match it up to our Cartesian ideas, a good
reference triangle is one with vertices at

$$\left[\begin{array}{c}
1\\
0
\end{array}\right],\left[\begin{array}{c}
0\\
1
\end{array}\right],\left[\begin{array}{c}
0\\
0
\end{array}\right]$$

If we pretend we place a bunch of weight on each of thes vertices, we
can find the location on the plane where we could balance the plane on
our finger. The total amount of weight is irrelevant. What does matter
is the relative amount of weight.

$$\left[\begin{array}{c}
x\\
y
\end{array}\right]=\frac{1}{m_{1}+m_{2}+m_{3}}(m_{1}\left[\begin{array}{c}
1\\
0
\end{array}\right]+m_{2}\left[\begin{array}{c}
0\\
1
\end{array}\right]+m_{3}\left[\begin{array}{c}
0\\
0
\end{array}\right])$$

We will allow the weights to go negative, because otherwise our balance
point would never get outside the triangle and we want to describe the
whole plane. This actually is physically possible, for example if we
tied balloons to the vertices instead of hanging weights on them.

We could also consider these to be spring coordinates. If we imagine a
bunch of springs with dialable spring constants connected to the
vertices of the triangle., The position of equilbirum will also be at
the barycenter

We can make a vector out of these weights, and pack the corresponding
vertices into a matrix

$$\left[\begin{array}{c}
x\\
y
\end{array}\right]=\frac{1}{m_{1}+m_{2}+m_{3}}\left[\begin{array}{ccc}
1 & 0 & 0\\
0 & 1 & 0
\end{array}\right]\left[\begin{array}{c}
m_{1}\\
m_{2}\\
m_{3}
\end{array}\right]$$

$$\left[\begin{array}{c}
x'\\
y'\\
m_{1}+m_{2}+m_{3}
\end{array}\right]=\left[\begin{array}{ccc}
1 & 0 & 0\\
0 & 1 & 0\\
1 & 1 & 1
\end{array}\right]\left[\begin{array}{c}
m_{1}\\
m_{2}\\
m_{3}
\end{array}\right]$$

This makes it clear why subspaces are lines

### Dual Barycenter Coordinates

Instead of weighting the vertices, instead we could weight the edges of
the triangle.

### Mandelstahm Variables

Continued Fractions
-------------------

Using $RP^{1}$ you have an extension of what operations can be
represented as linear or matrix-like. As before multiplication is a
matrix

$$a\times\Longleftrightarrow\left[\begin{array}{cc}
a & 0\\
0 & 1
\end{array}\right]$$

But now addition is also a matrix

$$a+\Longleftrightarrow\left[\begin{array}{cc}
1 & a\\
0 & 1
\end{array}\right]$$

And most interestingly inversion is also now a matrix.

$$\frac{1}{-}\Longleftrightarrow\left[\begin{array}{cc}
0 & 1\\
1 & 0
\end{array}\right]$$

So now any operation that can be split up into a sequence of these basic
operations can be written as a matrix. This gives an interesting
approach to conitnued fractions for example

$$\frac{1}{2+\frac{1}{2+\frac{1}{2+\ldots}}}$$

What does this ultimately become? The answer is that it converges to the
eigenvector of the corresponding matrix

$$2+\Longleftrightarrow\left[\begin{array}{cc}
1 & 2\\
0 & 1
\end{array}\right]$$

$$\frac{1}{2+}\Longleftrightarrow\left[\begin{array}{cc}
0 & 1\\
1 & 0
\end{array}\right]\left[\begin{array}{cc}
1 & 2\\
0 & 1
\end{array}\right]=\left[\begin{array}{cc}
0 & 1\\
1 & 2
\end{array}\right]$$

$$\frac{1}{2+\frac{1}{2+\frac{1}{2+\ldots}}}=\left[\begin{array}{cc}
0 & 1\\
1 & 2
\end{array}\right]\left[\begin{array}{cc}
0 & 1\\
1 & 2
\end{array}\right]\left[\begin{array}{cc}
0 & 1\\
1 & 2
\end{array}\right]\ldots=\left[\begin{array}{cc}
0 & 1\\
1 & 2
\end{array}\right]^{N}$$

The eigenvectors and eigenvalues of this matrix are
$$\lambda_{1}=1+\sqrt{2},\left[\begin{array}{c}
\sqrt{2}-1\\
1
\end{array}\right]$$

$$\lambda_{2}=1-\sqrt{2},\left[\begin{array}{c}
-\sqrt{2}-1\\
1
\end{array}\right]$$

Method of Images Spherical Inversion
