{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# operation = (name, out, ins)\n",
    "prog = [\n",
    "    (\"mov\", \"z\", []),\n",
    "    (\"add\", \"x\", [\"y\", \"z\"]),\n",
    "    (\"sub\", \"w\", [\"w\"])\n",
    "]\n",
    "\n",
    "\n",
    "def run(prog):\n",
    "    regs = {}\n",
    "    for op, out, ins in enumerate(prog):\n",
    "        if op == \"mov\":\n",
    "            regs[out] = 0\n",
    "        elif op == \"add\":\n",
    "            regs[out] = sum(regs[r] for r in ins)\n",
    "        elif op == \"sub\":\n",
    "            regs[out] = regs[ins[0]]\n",
    "    return regs\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of interesting little subproblems in compilation.\n",
    "\n",
    "I like constraint solvers. I like them because it's kind of fun to figure out how to mold your problem into the form they need.\n",
    "\n",
    "It isn't all roses. Sometimes mangling your problem.\n",
    "\n",
    "They can be slower than a bespoke heuristics.\n",
    "\n",
    "In compilation, there are multiple IRs to speak of.\n",
    "\n",
    "There's usually some kind of SSA IR like LLVM IR.\n",
    "Then there may be instructions filled in but logical variables instead of registers.\n",
    "And what is given to the assembler isn't fully concretized. You still have abstract labels for memory addresses and program addresses.\n",
    "Even after you run that, linking or dynamic linking can still occur, so there are relocations describing the ways to fix up the binary.\n",
    "\n",
    "\n",
    "\n",
    "It's nice to traverse a block backwards. This is because you can perform a livness analysis at the same time you are making your constraints.\n",
    "\n",
    "One can push different pieces into the metalevel (python) or the object languages (z3). You can push the liveness ana;lysis into the solver, or the instruction selection, or the assembler.\n",
    "\n",
    "It all gets more complicated.\n",
    "\n",
    "The more powerful compile time techniqyes seem to deliver fairly maginal gains. One problem is that the model of cpu operation isn't that clear. There's a lot of internal complexity to the CPU (caches, pipelining, [out of order](https://en.wikipedia.org/wiki/Out-of-order_execution) execution). Considering these gives rules of thumb.\n",
    "\n",
    "More straightforward objectives are things like code size. Smaller is always better (well, unless you also care about performance. It is conceivable larger code performs better.).\n",
    "\n",
    "It is conceivable to have crazier constraints. Like maybe you're writing shellcode that can't have certain bytes in it.\n",
    "\n",
    "\n",
    "Register allocation is often modelled as a graph coloring. This isn't quite true because it can be the case that if you can get your resgiters right, your peephole optimizer can reduce your code a little bit. The canonical example is a `mov x,y` instruction. If you can select registers such that `x=y`, then \n",
    "\n",
    "\n",
    "[Instruction scheduling](https://en.wikipedia.org/wiki/Instruction_scheduling) is the problem of reshuffling your instructions according to your dependencies. It is good to get your memory accesses as early as possible. They are often going to be slower than more computational register to register functions. It's kind of like firing off an async request.\n",
    "\n",
    "Memory models\n",
    "\n",
    "Instruction Selection is a combination of problems. Perhaps it is the richest. Here are some declarative specs of instruction selectors. Go, cranelift.\n",
    "Instruction selection finds patterns.\n",
    "The source can be modelled as Trees, DAGs, or Graphs. See  https://www.amazon.com/Instruction-Selection-Principles-Methods-Applications/dp/3319816586\n",
    "\n",
    "Then it selects a sufficient set of these patterns to actually cover the required function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#[(op,outs, ins)]\n",
    "\n",
    "\n",
    "Expr = Tuple[Op, Any]\n",
    "\n",
    "def simp(expr):\n",
    "    \n",
    "def isel_stmt(stmt):\n",
    "\n",
    "def isel_expr(expr):\n",
    "    match expr:\n",
    "        case (\"add\", x, y):\n",
    "        case (\"mul\", x, y):\n",
    "        case (\"neg\", x):\n",
    "        case (\"load\", x):\n",
    "        \n",
    "def eval(env, block):\n",
    "    for (op, outs, ins) in reversed(block):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://bernsteinbear.com/blog/ddcg/ destination driven\n",
    "https://github.com/tekknolagi/pyddcg\n",
    "https://github.com/tekknolagi/ddcg\n",
    "https://legacy.cs.indiana.edu/~dyb/pubs/ddcg.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "One of my big regrets on the VIBES project was not writing up a checkpoint on the ideas and state of the constraint compiler aspect of it. There were a couple of reasons for this:\n",
    "\n",
    "1. It was not clear the degree to which it is allowed for me to write about work projects\n",
    "2. The state is and was highly imperfect\n",
    "\n",
    "Nevertheless, it is quite a cool approach and even in it's imperfect state it is interesting\n",
    "\n",
    "# Unison\n",
    "\n",
    "A major touchpoint for compiling with constraints is the [Unison compiler](https://unison-code.github.io/).\n",
    "\n",
    "Unison is an alternative LLVM backend that solves the register allocation problem and instruction selection problem in \"unison\" using a constraint solving backend.\n",
    "\n",
    "Unison supported multiple architectures, but one, [Hexagon](https://en.wikipedia.org/wiki/Qualcomm_Hexagon) was a [very long instruction word](https://en.wikipedia.org/wiki/Very_long_instruction_word) (VLIW) architecture. My 10cent discussion of VLIW is that the instruction set allows explicit specification of parallelism / data dependencies. This enables perhaps a more direct model of the actual parallel interworkings of the cpu at the cost of significant compiler complexity. It is here that perhaps constraint compilation carries its weight.\n",
    "\n",
    "This paper is particular is useful [Combinatorial Register Allocation and Instruction Scheduling])<https://arxiv.org/abs/1804.02452>). Also very interesting was the [manual](https://unison-code.github.io/doc/manual.pdf).\n",
    "\n",
    "LLVM has multiple IRs. One of them is [Machine IR](https://llvm.org/docs/MIRLangRef.html) (MIR), which is an IR that has more of the information filled in. Unison takes this output, fiddles with it, and then performs solving. This reuses the frontend, middle end passes, and instruction selection of llvm, which is huge.\n",
    "\n",
    "The actual unison minizinc model is [here](https://github.com/unison-code/unison/blob/master/src/solvers/multi_backend/minizinc/code-generation.mzn). I spent a good month probably studying this and trying to pare this down. Ultimately, I decided to write it from scratch again, adding in pieces as we understood we needed them.\n",
    "\n",
    "This is an example of a unison IR taken from the manual:\n",
    "\n",
    "```\n",
    "function: factorial\n",
    "b0 (entry, freq: 4):\n",
    "    o0: [t0:r0,t1:r31] <- (in) []\n",
    "    o1: [t2] <- A2_tfrsi [1]\n",
    "    o2: [t3] <- C2_cmpgti [t0,0]\n",
    "    o3: [] <- J2_jumpf [t3,b2]\n",
    "    o4: [] <- (out) [t0,t1,t2]\n",
    "b1 (freq: 85):\n",
    "    o5: [t4,t5,t6] <- (in) []\n",
    "    o6: [t7] <- A2_addi [t5,-1]\n",
    "    o7: [t8] <- M2_mpyi [t5,t4]\n",
    "    o8: [t9] <- C2_cmpgti [t5,1]\n",
    "    o9: [] <- J2_jumpt [t9,b1]\n",
    "    o10: [] <- (out) [t6,t7,t8]\n",
    "b2 (exit, return, freq: 4):\n",
    "    o11: [t10,t11] <- (in) []\n",
    "    o12: [] <- JMPret [t11]\n",
    "    o13: [] <- (out) [t10:r0]\n",
    "adjacent:\n",
    "    t0 -> t5, t1 -> t6, t1 -> t11, t2 -> t4, t2 -> t10, t6 -> t6, t6 -> t11,\n",
    "    t7 -> t5, t8 -> t4, t8 -> t10\n",
    "```\n",
    "\n",
    "# Minizinc\n",
    "\n",
    "Given that we already had experience using z3 and that smt solvers are not that different from constraint programming systems, why go for minizinc?\n",
    "\n",
    "- Optimization\n",
    "- Useful bits\n",
    "\n",
    "- text file vs bindings. Speed, flexibility, readability, serialization (cost and boon)\n",
    "\n",
    "# The model\n",
    "\n",
    "# Was it worth it?\n",
    "\n",
    "# Ideas on improvements\n",
    "\n",
    "\n",
    "## Declarative Code Gen\n",
    "\n",
    "- [Relational Processing for Fun and Diversity](https://personal.utdallas.edu/~hamlen/lundquist19minikanren.pdf) minikanren\n",
    "- [Denali - a goal directed super optimizer](https://courses.cs.washington.edu/courses/cse501/15sp/papers/joshi.pdf) egraph based optimization of assembly\n",
    "- [PEG](https://cseweb.ucsd.edu/~lerner/papers/popl09.pdf) egraph cfg\n",
    "- [RVSDG](https://github.com/egraphs-good/egg/discussions/106)\n",
    "- [minimips minikanren mips assembler/disassembler](https://github.com/orchid-hybrid/minimips)\n",
    "- [Parsing and compiling using Prolog](https://dl.acm.org/doi/abs/10.1145/22719.22946) There is also a chapter in the art of prolog\n",
    "\n",
    "- <https://www.cs.tufts.edu/~nr/pubs/zipcfg.pdf> zgraph\n",
    "- hoopl <https://hackage.haskell.org/package/hoopl-3.7.7.0/src/hoopl.pdf> a framework for transformations\n",
    "\n",
    "### Unison\n",
    "\n",
    "- [Unison](https://unison-code.github.io/)\n",
    "\n",
    "[diversification](http://www.diva-portal.org/smash/get/diva2:1232129/FULLTEXT01.pdf) make many versions of binary to make code reuse attacks harder. disunison\n",
    "\n",
    "Toy Program:\n",
    "\n",
    "If you do liveness analysis ahead of time, it really does become graph coloring, with an edge between every temporary that is live at the same time.\n",
    "\n",
    "You cannot do liveness ahead of time if you integrate instruction scheduling with allocation. It needs to be internalized.\n",
    "\n",
    "If you do SSA ahead of time, you have more flexibility to change colors/register at overwrite points\n",
    "\n",
    "How to communicate to minizinc:\n",
    "\n",
    "- Serialized files or C bindings\n",
    "- Parameters or constraints. In some sense, you a writing a constraint interpreter over the parameters. Why not cut out the middleman? 1: less clear what the structure is. 2. It forces your hand with the bundling of different pieces. Many things need to be bundled into the `insn` predicate unless you reify the `insn` predicate to a variable, in which case you are rebuilding the parameter version.\n",
    "\n",
    "There is a spectrum of more or less complex models you can use.\n",
    "\n",
    "### Fixed Instruction Order\n",
    "\n",
    "This makes a DSL in minizinc that looks like a somewhat reasonable IR. It uses a predicate function `insn` that takes in the lhs and rhs temporaries. It assigns a register to each temporary such that it never clobbers a live variable.\n",
    "\n",
    "I could do the liveness analysis completely statically, but I choose to internalize it into the model for ease.\n",
    "\n",
    "```minizinc\n",
    "enum reg_t = {R0, R1, R2, R3};\n",
    "enum temp_t = {T0, T1, T2, T3, T4, T5};\n",
    "int : MAXID = 5;\n",
    "set of int : operation_t = 0..MAXID;\n",
    "\n",
    "array[temp_t, operation_t] of var bool : live_in;\n",
    "array[temp_t] of var reg_t : reg;\n",
    "\n",
    "predicate insn(operation_t : id, list of temp_t : lhs, string : opcode, list of temp_t : rhs) = \n",
    "  % https://en.wikipedia.org/wiki/Live_variable_analysis\n",
    "  forall(t in temp_t)(\n",
    "    if (t in rhs) % in gen set\n",
    "      then live_in[t, id] = true\n",
    "    elseif (t in lhs) % not in gen set, in kill set\n",
    "      then live_in[t,id] = false\n",
    "    else % propagate\n",
    "      live_in[t,id] <- live_in[t, id + 1] \n",
    "    endif) /\\\n",
    "  % Assignments need to go to different registers than live variables of next instruction.\n",
    "  forall(t1 in lhs)(\n",
    "    forall(t2 in temp_t where t1 != t2)(\n",
    "      live_in[t2,id+1] -> reg[t1] != reg[t2]\n",
    "  ));\n",
    "\n",
    "% Nothing is live at end of block\n",
    "constraint forall(t in temp_t)( live_in[t, MAXID] = false);\n",
    "\n",
    "constraint \n",
    "  insn(0, [T1], \"mov\", [T0])     /\\\n",
    "  insn(1, [T2], \"add\", [T0, T1]) /\\\n",
    "  insn(2, [T3], \"sub\", [T0, T1]) /\\\n",
    "  insn(3, [T4], \"mul\", [T1, T2]) /\\\n",
    "  insn(4, [T5], \"inc\", [T4]);\n",
    "\n",
    "\n",
    "%reg = [T0: R2, T1: R0, T2: R1, T3: R2, T4: R0, T5: R0];\n",
    "%live_in = \n",
    "%[|         0:     1:     2:     3:     4:     5: \n",
    "% | T0:  true,  true,  true, false, false, false\n",
    "% | T1: false,  true,  true,  true, false, false\n",
    "% | T2: false, false,  true,  true, false, false\n",
    "% | T3: false, false, false, false, false, false\n",
    "% | T4: false, false, false, false,  true, false\n",
    "% | T5: false, false, false, false, false, false\n",
    "% |];\n",
    "%----------\n",
    "\n",
    "% if we're not in ssa, maybe \n",
    "% array[temp_t, id] of var reg_t; \n",
    "% since register can change as reuse site.\n",
    "\n",
    "% Registers don't allocate to same spot\n",
    "%constraint forall (id in operation_t)(\n",
    "%  forall(t1 in temp_t)(\n",
    "%    forall(t2 in temp_t)(\n",
    "%      (live_in[t1,id] /\\ live_in[t2,id] /\\ t1 != t2) ->\n",
    "%      reg[t1] != reg[t2]\n",
    "%    )));\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "How do you want to talk about the solution space.\n",
    "\n",
    "- a next(id1,id2) matrix\n",
    "- live[id,t] matrix vs start end cycle integers.\n",
    "\n",
    " % since we don't record the gen kill sets we need to do this in here.\n",
    "`% next[i,j]` where you see `id + 1`\n",
    "I was assuming SSA, but maybe it can handle non ssa? Noo. It probably can't.\n",
    "\n",
    "### Scheduling and Allocation\n",
    "\n",
    "We can also use a next[i,j] matrix or change live to a start end cycle parameter.\n",
    "\n",
    "### Multiple Blocks\n",
    "\n",
    "### Register Packing\n",
    "\n",
    "Using the rectangle packing constraint for register modelling\n",
    "\n",
    "## Rewrite Rules\n",
    "\n",
    "peephole optimization\n",
    "[cranelift isle](https://github.com/bytecodealliance/wasmtime/blob/918671316301306d653345cc3486f0a15de2aa50/cranelift/docs/isle-integration.md)\n",
    "[Verifying and Improving Halide’s Term Rewriting System with Program Synthesis](https://dl.acm.org/doi/pdf/10.1145/3428234)\n",
    "\n",
    "See: e-graphs\n",
    "[scheduling using unimodular modelling](https://twitter.com/taktoa1/status/1531386684876632064?s=20&t=-IHVNfpCMKlhva0T8ctWXA)\n",
    "\n",
    "## Instruction Selection\n",
    "\n",
    "[Automatically Generating Back Ends Using Declarative Machine Descriptions](https://www.cs.tufts.edu/~nr/pubs/gentileset-abstract.html) dias ramsey <https://www.cs.tufts.edu/~nr/pubs/tiler-abstract.html>\n",
    "\n",
    "[Hoopl](https://www.cs.tufts.edu/~nr/pubs/hoopl-abstract.html)\n",
    "\n",
    "Maximal munch parsing\n",
    "<http://www.cs.cmu.edu/afs/cs/academic/class/15745-s07/www/lectures/lect9-instruction_selection_745.pdf>\n",
    "Like parser generators / libraries, you can make instruction selection libraries / generators. Bottom up vs top down\n",
    "\n",
    "- TWIG BURG BEG bottom up generate instruction selectors\n",
    "-\n",
    "\n",
    "[iburg](https://old.reddit.com/r/Compilers/comments/edgx5s/successors_to_iburg/) <https://github.com/drh/iburg> a code generator generator\n",
    "\n",
    "[Synthesizing an Instruction Selection Rule Library from Semantic Specifications](https://pp.ipd.kit.edu/uploads/publikationen/buchwald18cgo.pdf)\n",
    "\n",
    "Subgraph isomorphism problem\n",
    "VF2 algorithm\n",
    "Very similar to \"technology mapping\" in the vlsi community.\n",
    "\n",
    "type aexpr = Mul | Add\n",
    "\n",
    "Macro expansion\n",
    "\n",
    "- procede bottom up\n",
    "Maximal Munch\n",
    "\n",
    "Instruction selection is taking a program and figuriing out which instructions can be used to implement this. Typically this leaves behind still some problems to solve, in particular register allocation and instruction scheduling. Presumably, everything in the program needs to be done. We have some notion of correspondence between the program representation and the available instructions. The exact nature of this correspondence depends on how we represent our program.\n",
    "\n",
    "- Sequence\n",
    "- Tree - One representation of a program might be as a syntax tree, say `(x + (y * z))`.\n",
    "- DAG - consider `(x + y) * (x + y)`. Really we want to note common shared computation and not recompute `x+ y` twice. DAGs and the technique of hash consing can be useful here.\n",
    "- Tree-like DAGS\n",
    "- CFG - A different representation might be to separate out blocks and control edges between them. Blocks consist of a sequence of statements.\n",
    "\n",
    "If the statements are purely for assignment, assignment can be inlined. The block is nearly purely functional in some sense. It can be compressed into a functional form like the DAG or Tree by inlining. The block could also itself be considered as a graph, as there is often more then one equivalent way of sequencing the instructions.\n",
    "\n",
    "The simplest case to consider is that of the tree. We can enumerate patterns in the tree that we know how to implement using instructions. The relationship between tree patterns and instructions can be many-to-many. We should understand how to implement every node in the tree `(?a + ?b)`, `(?a * ?b)` with a pattern consisting of a sequence of instructions for completeness (ability to compile any tree). We also should try to figure out the tree patterns that correspond to a single assembly instruction like `load reg [reg+n]` because these will often be efficient and desirable.\n",
    "\n",
    "There are two distinct and often separable problems here:\n",
    "\n",
    "1. Find pattern matches\n",
    "2. Pick which matches to actually use aka pattern selection\n",
    "\n",
    "A direct approach to describing patterns is to develop a datatype of patterns. This datatype will be basically the datatype of your AST with holes. This is clearly duplication and becomes painful the more constructors your language has, but whatever.\n",
    "\n",
    "```ocaml\n",
    "type ast = Add of ast * ast | Var of string\n",
    "type ast_pat = Hole of string | PAdd of ast_pat * ast_pat | PVar of string\n",
    "\n",
    "pmatch : ast_pat -> ast -> (string * ast) list option\n",
    "```\n",
    "\n",
    "Alternatively, we can note that the main point of a pattern is to pattern match and use a church-ish/finalish representation\n",
    "\n",
    "```ocaml\n",
    "type ast_pat' = ast -> (string * ast) list option\n",
    "let convert_pat' : ast_pat -> ast_pat' = pmatch\n",
    "```\n",
    "\n",
    "```ocaml\n",
    "type var = string\n",
    "type stmt = Ass of var * expr\n",
    "type expr = Add of var * var | Var of var\n",
    "\n",
    "type blk = stmt list\n",
    "\n",
    "let inline : blk -> (var * ast) list\n",
    "type insn = Mov of reg * reg | Add of reg * reg | Add2 of reg * reg * reg\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "A novelty of the Blindell et al work is the notion of universal function (UF) graph. There is both the functinal repsentation of data values, but also cfg is represented as opaque nodes. The correspondence of where values are defined and where computations happen is left up to the constraint solver.\n",
    "\n",
    "What is the input language? Is it a pure expression langage? A side effectful imperative language? We can convert between these.\n",
    "\n",
    "I have directly gone to effectful assembly from pure expression language above.\n",
    "\n",
    "I understand enough to have many questions. What is the input language over which one is pattern matching. Perhaps language is already the wrong word since language tends to imply something tree-like. Is it a pure language or an imperative language. Is it represented as a sequence of IR instructions, a tree, tree-like dag, dag,  a graph, or something else. Is represented too weak a word for this question which seems to be very important? \"BIL\" represented as a sequence vs as a graph might as well be nearly entirely different things. It seems totally possible to translate between pure and imperative, and between the representations and yet it matters so much. What is the output language. It structurally isn't concrete assembly in many ways. It is definitely un-register allocated and probably unsequenced. Sometimes it feels like tree-like quasi assembly, where the node represents an \"output\" register even though assembly is really just a sequence of effects. Is there freedom to choose any N^2 combination of structural representations between input and output languages, purity and impurity?  None of this even starts to touch control flow. None of this touches what does \"overlapping\" of patterns mean and what should be allowed\n",
    "\n",
    "Sequenced representation: Patterns may need to stretch over bits / reorderings. The sequence of the input language does not at all have to be the sequence of the output. Restricting yourself in this way\n",
    "\n",
    "You can often macro repeat patterns in ways to undo any arbitrary choices made by the representation. Some kind of quotienting. If we have an order free representation, we could aebitrary sequence it, and then sequence the patterns into all possible sequencings. Then you end up with baically the same thing. You can't go the other way in general.\n",
    "There is something that feels galois connection-y here.\n",
    "\n",
    "What is the output of pattern matching? Typically I would consider the output of a pattern match to be just pattern variable bindings. But in this case, really we may need full identification between pattern nodes and pattee nodes since this defines the covering.\n",
    "\n",
    "There are different axes upon which to consider graph variations\n",
    "\n",
    "- input/output Edges ordered or unordered / have identity are interchangeable. AST tree have identity. Consider the example of a power or any non commutative operation. Edges with identities may want to be considered to be attached to \"ports\"\n",
    "- Zero/many input output edges (trees)\n",
    "- Labels on vertices and or edges\n",
    "\n",
    "Different kinds can be embedded in each other.\n",
    "Trees can represent graphs if you are allowed to indirectly refer to nodes via labels.\n",
    "Hash cons dags can have many input and output edges. However the output edges of the hash cons are unported, whereas the input edges are ported. The symmettry can be improved by using projection/port nodes connected to the output. In some sense the output of the original is then\n",
    "Operads\n",
    "\n",
    "You could take a relational perspective on operations, having neither input not output.\n",
    "\n",
    "## Register Allocation\n",
    "\n",
    "[iterated register coalescing - appell and george](https://c9x.me/compile/bib/irc.pdf)\n",
    "move edges are considered special because the can be coalesced (collapse the node in the interference graph)\n",
    "nodes with less than number of register nighbors can be removed and you can construct a coloring\n",
    "conservative coalescing -\n",
    "constants can be spilled cheaply via rematerialization\n",
    "\n",
    "[regallo2 design doc](https://github.com/bytecodealliance/regalloc2/blob/main/doc/DESIGN.md)\n",
    "\n",
    "[flambda reg alloc](https://github.com/ocaml-flambda/flambda-backend/pull/678) points to an [appell paper - iterated register coalescing](https://dl.acm.org/doi/abs/10.1145/229542.229546) and tiger book\n",
    "<https://arxiv.org/abs/1804.02452>\n",
    "\n",
    "[cranelift regalloc](https://cfallin.org/blog/2022/06/09/cranelift-regalloc2/) great blog post\n",
    "\n",
    "[Register Allocation: What Does the NP-Completeness Proof of Chaitin et al. Really Prove? Or Revisiting Register Allocation: Why and How](https://link.springer.com/chapter/10.1007/978-3-540-72521-3_21#:~:text=Abstract,graph%20associated%20to%20the%20variables.)\n",
    "\n",
    "[The Solid-State Register Allocator](https://twitter.com/impraxical/status/1577321303400452100?s=20&t=UJrepWvNkFpXFRNY8yoWDA) <https://www.mattkeeter.com/blog/2022-10-04-ssra/>\n",
    "Belady's OPT algorithm page faults\n",
    "\n",
    "[The Power of Belady’s Algorithm in Register Allocation for Long Basic Blocks](http://polaris.cs.uiuc.edu/publications/guo-2003-old.pdf)\n",
    "[efficient global register allocation](https://arxiv.org/abs/2011.05608)\n",
    "\n",
    "[linear scan register allocation](http://web.cs.ucla.edu/~palsberg/course/cs132/linearscan.pdf)\n",
    "\n",
    "The typical starting point of register allocation is support you've been given as assembly program that doesn't have registers filled in like\n",
    "\n",
    "```assembly\n",
    "# input v1 v2 v3\n",
    "mov v1, v2\n",
    "add v1, v1\n",
    "mul v3, v1\n",
    "# output v3\n",
    "```\n",
    "\n",
    "The interference graph has an edge between any two variables that are live at the same time.\n",
    "Live means that the variable has been made and someone still needs to use it now or later.\n",
    "In this example, if we assume v1 v2 & v3 are live at the beginning, v1 is live for all 3 instructions, v3 is live for all three and at the output, but v2 is only live at the first instruction since it is never used again.\n",
    "\n",
    "[dsatur graph coliring heurisitc](https://en.wikipedia.org/wiki/DSatur)\n",
    "\n",
    "[RL4ReAl: Reinforcement Learning for Register Allocation](https://twitter.com/johnregehr/status/1513561374873464833?s=20&t=NBROMONLYyqlU8uerfss0A)\n",
    "Compiler gym\n",
    "\n",
    "## Instruction Scheduling\n",
    "\n",
    "The pure instruction scheduling problem might occur even at the IR level. We can imagine an imperative IR. Certain operations commute and others don't. We may want to minimize the liveness time of variables for example. This would make sense as a pre-processing step to a sequence input language to an instruction selector.\n",
    "\n",
    "Instruction scheduling can be parametrized as:\n",
    "\n",
    "1. an embedding into actual time (cycle issue time probably). This is important if you are optimizing for runtime and can get estimates of how long each instruction takes.\n",
    "2. a ranking as integers\n",
    "3. next(i,j) relation which is basically integers. Allows for partial order. after(i,j) :- next(i,k), after(). after is path connected in temporal dag. Possibly this is mappable into a lattice notion of time (i,j,k,etc)?\n",
    "\n",
    "## SuperOptimizers\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Superoptimization>\n",
    "\n",
    "[Superoptimizer -- A Look at the Smallest Program](https://web.stanford.edu/class/cs343/resources/superoptimizer.pdf) Massalin\n",
    "<https://news.ycombinator.com/item?id=25196121> discussion\n",
    "\n",
    "- Souper <https://github.com/google/souper> <https://arxiv.org/pdf/1711.04422.pdf>\n",
    "- STOKE <https://cs.stanford.edu/people/eschkufz/docs/asplos_13.pdf>\n",
    "- TOAST an ASP based one? <https://purehost.bath.ac.uk/ws/portalfiles/portal/187949093/UnivBath_PhD_2009_T_Crick.pdf>\n",
    "- GSO gnu superoptimizer\n",
    "-\n",
    "\n",
    "<https://twitter.com/kripken/status/1564754007289057280?s=20&t=KWXpxw5bjeXiDnNeX75ogw> Zakai binaryen superopitmizer\n",
    "\n",
    "\n",
    "## Compiling with Constraints I\n",
    "\n",
    "\n",
    "Hey Hey! We've open sourced our project [VIBES](https://github.com/draperlaboratory/VIBES) (Verified, Incremental Binary Editing with Synthesis).\n",
    "\n",
    "A compiler can be a complicated thing. Often we break up and approach the problem by breaking it into pieces. The front end takes text and converts it to a tree. In turn it is often converted to some kind of intermediate representation, say a control flow graph. The middle end performs rearrangements and analyses on this graph. The back end is in charge of taking this intermediate representation and bringing it to executable machine code.\n",
    "\n",
    "The backend of a compiler has a number of different problems\n",
    "\n",
    "1. Instruction Selection - Take `x := a + b` and turn it into `mov x, a; add x,b` \n",
    "2. Register Allocation - Take `add x,a` and turn it into `add R0, R1`\n",
    "3. Instruction Scheduling - Rearrange instructions to maximize throughput of CPU. Internally cpu has pipelines and resources, store and loads to memory are slow, you want to make sure these are chugging along and interleaved well.\n",
    "\n",
    "Typically, these will be run one after the other in some order, with greedy-ish algorithms and hand tuned code. This works pretty well and is fast to compile.\n",
    "It does leave performance and size of the resulting code on the table.\n",
    "The Unison approach is trying to build a non toy compiler that uses declarative constraint programming to solve these in concert.\n",
    "Now any optimization is only as good as our modelling of the situation. This is one of the common fallacies of godhood people get when they think about applying math to problems. If your model of the cpu is coarse, then perfectly optimizing with respect to this . Ultimately, every ahead of time model is bad because it must be unable to know information that is readily accessible at runtime. This is at least one reason why without experimentation and measurement, you really can't tell whether crass online optimization can deliver superior results to thorough ahead of time optimization. See for example JITs, branch prediction, VLIW architectures, out of order execution. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Can I write CFG programs directly in minizinc in a good way?\n",
    "Would avoid 1 indirection or going in and out of a language\n",
    "\n",
    "Declarative Solving for Compiler Backends using Minizinc\n",
    "Unison-style Compiler backends\n",
    "\n",
    "Declarative compilation\n",
    "\n",
    "[Combinatorial Register Allocation and Instruction Scheduling](https://arxiv.org/pdf/1804.02452.pdf)\n",
    "\n",
    "Schulte course on constraint programming\n",
    "https://kth.instructure.com/courses/7669/files/folder/lectures?\n",
    "on compilers\n",
    "https://kth.instructure.com/courses/6414/files\n",
    "\n",
    "\n",
    "\n",
    "# Graph Coloring and Register Allocation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In any case we are embedding a liveness analysis into minizinc.\n",
    "live(t,i) = true/false\n",
    "next(i1,i2) = true/false\n",
    "next forms total order.\n",
    "\n",
    "\n",
    "[after(i,k)  <- next(i,j), after(j,k) for i,j,k in insns]\n",
    "[after(i,k) <-> exists(j in insn)( next(i,j) /\\  after(j,k)) \\/ next(i,k) forall i,k ]\n",
    "[live[insn, temp] <- ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Instruction Scheduling\n",
    "\n",
    "The pure instruction scheduling problem might occur even at the IR level. We can imagine an imperative IR. Certain operations commute and others don't. We may want to minimize the liveness time of variables for example. This would make sense as a pre-processing step to a sequence input language to an instruction selector.\n",
    "\n",
    "\n",
    "Instruction scheduling can be parametrized as:\n",
    "1. an embedding into actual time (cycle issue time probably). This is important if you are optimizing for runtime and can get estimates of how long each instruction takes.\n",
    "2. a ranking as integers\n",
    "3. next(i,j) relation which is basically integers. Allows for partial order. after(i,j) :- next(i,k), after(). after is path connected in temporal dag. Possibly this is mappable into a lattice notion of time (i,j,k,etc)?\n",
    "\n",
    "\n",
    "\n",
    "Instruction matching as an SMT problem\n",
    "\n",
    "\n",
    "Why is there such an impulse to model these problems in a purely functional style? Invariably we need to introduce a state threading.\n",
    "\n",
    "\n",
    "Dataflow analysis with Datalog.\n",
    "I should lay out what I understand of the unison compiler.\n",
    "\n",
    "\n",
    "Register Packing\n",
    "Instruction Scheduling1\n",
    "\n",
    "The box model\n",
    "\n",
    "\n",
    "Christian Schulte course\n",
    "\n",
    "\n",
    "```minizinc\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "simple\n",
    "```minizinc\n",
    "include \"alldifferent.mzn\";\n",
    "include \"all_equal.mzn\";\n",
    "include \"diffn.mzn\";\n",
    "\n",
    "set of int : reg_t = 1..4;\n",
    "set of int : temp_t = 1..10;\n",
    "set of int : instr_t = 1..10;\n",
    "array[temp_t] of var reg_t : reg;\n",
    "array[temp_t] of var instr_t : def;\n",
    "array[temp_t] of var instr_t : death;\n",
    "/*\n",
    "There is a possible off by one error\n",
    "is death at the last use site, or is it the instruction after\n",
    "Let's say it is the last use site.\n",
    "Or rather should we be thinking of program points between instructions\n",
    "\n",
    "\n",
    "*/\n",
    "\n",
    "\n",
    "\n",
    "%constraint alldifferent(def);\n",
    "\n",
    "predicate insn(array[int] of var temp_t : defs, string: operator,  array[int] of var temp_t : uses) =\n",
    "    all_equal([def[t] | t in defs ] )\n",
    "    /* \n",
    "         forall(t2 in defs)(\n",
    "             def[t1] = def[t2]\n",
    "        ) */\n",
    "        /\\\n",
    "        forall( t1 in defs)(\n",
    "        forall (t2 in uses) (\n",
    "            def[t1] > def[t2] /\\ death[t2] >= def[t1]\n",
    "        )\n",
    "    );\n",
    "\n",
    "% redundant?\n",
    "constraint forall(t in temp_t)(\n",
    "    def[t] <= death[t]\n",
    ");\n",
    "\n",
    "%output [\"foo\" | t in temp_t];\n",
    "constraint diffn( reg, def , [1 | t in temp_t], [ death[t] - def[t] | t in temp_t ] );\n",
    "%constraint diffn( reg, def ,  reg, [ death[t] - def[t] | t in temp_t ] );\n",
    "/* \n",
    "We can't do it this way because alldifferent takes a compile time array.\n",
    "We could do this if we had a set of dummy values,\n",
    "Or if we ahead of time fixed the instruction ordering.\n",
    "If wwe did that, liveness analysis might need to be done out of minizinc.\n",
    "\n",
    "constraint forall( i in instr_t)(\n",
    "    alldifferent(  [ reg[t]   | t in temp_t where i < death[t] /\\  def[t] < i ]    )\n",
    "); */\n",
    "\n",
    "constraint insn( [3], \"add\", [1,2] );\n",
    "constraint insn( [4], \"sub\", [1,3] );\n",
    "\n",
    "solve minimize card(array2set(reg));\n",
    "```\n",
    "\n",
    "\n",
    "ordered\n",
    "```minizinc\n",
    "include \"alldifferent.mzn\";\n",
    "/*\n",
    "\n",
    "This file shows register allocation if we prefix the instruction order.\n",
    "\n",
    "*/\n",
    "set of int : reg_t = 1..4;\n",
    "set of int : temp_t = 1..4;\n",
    "%set of int : instr_t = 1..10;\n",
    "array[temp_t] of var reg_t : reg;\n",
    "%array[temp_t] of var instr_t : def;\n",
    "%array[temp_t] of var instr_t : death;\n",
    "enum inout = {Out, In};\n",
    "set of int : instr_t = 1..4;\n",
    "array[instr_t,inout] of set of temp_t : instrs =\n",
    "   [|\n",
    "    {1,2}, {}  |\n",
    "    {3}, {1,2} |\n",
    "    {4}, {2,3} |\n",
    "    {},  {1,4}\n",
    "   |];\n",
    "\n",
    "array[temp_t] of int : def =\n",
    "   [ min([ i  | i in instr_t where t in instrs[i,Out] ])  | t in temp_t ] ;\n",
    "\n",
    "array[temp_t] of int : death =\n",
    "   [ max([ i  | i in instr_t where t in instrs[i,In] \\/ t in instrs[i,Out]  ])  | t in temp_t ];\n",
    "\n",
    "constraint forall( i in instr_t)(\n",
    "    alldifferent(  [ reg[t]   | t in temp_t where i <= death[t] /\\  def[t] < i ]    )\n",
    ");\n",
    "```\n",
    "\n",
    "Something like a finally tagless style encoding of block?\n",
    "But I'd also like the same of insn\n",
    "Side effectful functions?\n",
    "Ok. So I want\n",
    "\n",
    "```minizinc\n",
    "\n",
    "predicate block(string : name, set[temp_t] : ins, set of [temp_t] : outs, array[int] of insn : data, array[int of insns : data] : ctrl)\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
