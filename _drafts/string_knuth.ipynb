{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bits and Bobbles\n",
    "https://www.philipzucker.com/ground_kbo/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old notes String rewriting systems\n",
    "\n",
    "[Semi-Thue systems](https://en.wikipedia.org/wiki/Semi-Thue_system)\n",
    "[Word problem](https://en.wikipedia.org/wiki/Word_problem_(mathematics))\n",
    "Monoid presentation\n",
    "\n",
    "converting to term rewriting system fff ---> f(f(f(X)))\n",
    "\n",
    "Term Rewriting using linux command line tools\n",
    "\n",
    "The string search and manipulation tools are very powerful and efficient. They compile queries like grep into simple machines and such I think.\n",
    "\n",
    "There's a big difference between ground and non ground terms. They appear subtly different when latexed, but the are way different beats\n",
    "Ground terms equation proving can be done through the e graph efficiently.\n",
    "\n",
    "Ground term rewriting seems to be identical to string rewriting. Just layout serially a traversal of the term.\n",
    "\n",
    "The implicit prefix and suffix are the context of the term\n",
    "\n",
    "```python\n",
    "\n",
    "rules = [\n",
    "  (\"aa\", \"b\"),\n",
    "  (\"b\", \"c\")\n",
    "]\n",
    "\n",
    "def run_rules(s,rules):\n",
    "  old_s = None\n",
    "  while s != old_s:\n",
    "    old_s = s\n",
    "    for lhs,rhs in rules:\n",
    "      s = s.replace(lhs,rhs)\n",
    "  return s\n",
    "\n",
    "print(run_rules(\"ababaaaaccaaa\", rules))\n",
    "\n",
    "def naive_completion(rules):  \n",
    "  old_rules = None\n",
    "  while rules != old_rules:\n",
    "    old_rules = rules.copy()\n",
    "    for (lhs,rhs) in old_rules:\n",
    "        a = run_rules(lhs, rules)\n",
    "        b = run_rules(rhs, rules)\n",
    "        if a == b:\n",
    "          break\n",
    "        if a < b:\n",
    "          rules.add((b,a))\n",
    "        if a > b:\n",
    "          rules.add((a,b))\n",
    "  return rules\n",
    "\n",
    "# an incomplete reduction routine?\n",
    "# Triangular rewrite rules in some sense.\n",
    "# Is this right at all? This is like a chunk of Huet's. I think just moving R -> E might be ok even if not one of listed rules. No, if I could do that ths move + simplfy would give me a more powerful R-simplify.\n",
    "# I might be weakening my rules. That's not so bad imo.\n",
    "def reduce_rules(E):  \n",
    "  # worklist style\n",
    "  R = set()\n",
    "  # Sort E so smallest last probably. Most reduction power.\n",
    "  E = sorted(E, key=lambda k: len(k[0]), reverse=True)\n",
    "  while len(E) > 0:\n",
    "    (a,b) = E.pop()\n",
    "    a = run_rules(a, R) # simplify\n",
    "    b = run_rules(b, R)\n",
    "    print(a,b)\n",
    "    if a == b: #delete\n",
    "      continue\n",
    "    if (len(b), b) > (len(a), a): # len then lex ordering\n",
    "      R.add((b,a))\n",
    "    if (len(a), a) > (len(b), b):\n",
    "      R.add((a,b))\n",
    "  return R\n",
    "\n",
    "\n",
    "\n",
    "rules = {\n",
    "  (\"aaa\", \"a\"),\n",
    "  (\"aaa\", \"c\")\n",
    "}\n",
    "\n",
    "print(naive_completion(rules))\n",
    "\n",
    "\n",
    "rules = [\n",
    "  (\"ffa\", \"a\"),\n",
    "  (\"fa\", \"a\")\n",
    "]\n",
    "\n",
    "print(run_rules(\"ffffffffffffa\", rules))\n",
    "\n",
    "print(reduce_rules(rules))\n",
    "\n",
    "rules = [\n",
    "  (\"12+\", \"21+\"), # an application of comm\n",
    "  (\"23+1+\", \"2+31+\") # an application of assoc\n",
    "]\n",
    "# I am really going to want a notion of indirection or compression here.\n",
    "# Intern strings\n",
    "\n",
    "class RPN():\n",
    "  def __init__(self, s):\n",
    "    self.s = str(s)\n",
    "  def __add__(self,b):\n",
    "    return RPN(self.s + b.s + \"+\")\n",
    "  def __repr__(self):\n",
    "    return self.s\n",
    "\n",
    "b0 = RPN(0)\n",
    "b1 = RPN(1)\n",
    "b2 = RPN(2)\n",
    "b3 = RPN(3)\n",
    "\n",
    "\n",
    "E = [\n",
    "  (b1 + b0, b0 + b1),\n",
    "  (b0 + b1, b1),\n",
    "  (b1 + b2, b3),\n",
    "  (b2, b1 + b1) \n",
    "]\n",
    "E = [\n",
    "  (\"10+\", \"01+\"),\n",
    "  (\"01+\", \"1\"),\n",
    "  (\"12+\", \"3\"),\n",
    "  (\"2\", \"11+\"),\n",
    "  (\"00+\", \"0\"),\n",
    "  (\"00+1+1+1+\", \"3\"),\n",
    "]\n",
    "\n",
    "print(reduce_rules(E))\n",
    "E = reduce_rules(E)\n",
    "print(run_rules(\"00+1+0+1+0+2+\",E))\n",
    "\n",
    "E = [( str(i) + str(j) + \"+\" , str(i + j)) for i in range(4) for j in range(10) if i + j <= 9]\n",
    "print(E)\n",
    "print(reduce_rules(E))\n",
    "print(run_rules(\"00+1+0+1+0+2+\",E))\n",
    "\n",
    "```\n",
    "\n",
    "Ropes\n",
    "\n",
    "We can of course compile a rule set to do better than this. In some sense every string represents a possibly infinite class of strings posible by running rules in reverse\n",
    "\n",
    "String rewriting systems are a bit easier to think about and find good stock library functionality for.\n",
    "\n",
    "string rewriting is unary term rewriting. A variable string pattern \"aaaXbbbb\" is a curious object from that perspective. While simple, it is a higher order pattern. `a(a(a(X(b(b(b(Y)))))))`. You can also finitize a bit. `foo(a)` can be made an atomic character. Or you can partially normalize a term rewriting system to string rewriting form\n",
    "\n",
    "String orderings\n",
    "Lexicographic comparison\n",
    "Length\n",
    "shortlex - first length, then lex\n",
    "symbol counts\n",
    "\n",
    "```python\n",
    "def critical_pairs(a,b):\n",
    "  assert len(a) > 0 and len(b) > 0\n",
    "  if len(b) <= len(a):\n",
    "    a,b = b,a # a is shorter\n",
    "  cps = []\n",
    "  if b.find(a) != -1: # b contains a\n",
    "   cps.append(b)\n",
    "  for n in range(1,len(a)): # check if overlapping concat is possible\n",
    "    if b[-n:] == a[:n]:\n",
    "      cps.append(b + a[n:])\n",
    "    if b[:n] == a[-n:]:\n",
    "      cps.append(a + b[n:])\n",
    "  return cps\n",
    "\n",
    "print(critical_pairs(\"aba\", \"ac\"))\n",
    "print(critical_pairs(\"aba\", \"ca\"))\n",
    "print(critical_pairs(\"abab\", \"ba\"))\n",
    "  \n",
    "'''\n",
    "def reduce_rules(rules): # a very simple reduction, reducing rhs, and removing duplicate rules\n",
    "  rules1 = set()\n",
    "  for (lhs,rhs) in rules:\n",
    "    rhs = run_rules(rhs,rules)\n",
    "    rules1.add((lhs,rhs))\n",
    "  return rules1\n",
    "'''\n",
    "\n",
    "  #run_rules\n",
    "  #reduce_rules\n",
    "```\n",
    "\n",
    "Building a suffix tree might be a good way to find critical pairs.\n",
    "\n",
    "Lempel Ziv / Lzw is the analog of hash consing? Some kind of string compression is. That's fun.\n",
    "\n",
    "<http://haskellformaths.blogspot.com/2010/05/string-rewriting-and-knuth-bendix.html>\n",
    "\n",
    "It seems like named pattern string rewriting and variable term rewriting might be close enough\n",
    "\n",
    "You could imagine\n",
    "\n",
    "((x + 0) + 0 + 0) laid out as + + + x 0 0 0. and the found ground rewite rule + x 0 -> x being applied iteratively.\n",
    "\n",
    "Labelling shifts:\n",
    "f(g(a), b) ->   f +0 g +1 a -1 b -0\n",
    "\n",
    "the pattern\n",
    "f(?x)  -> ?x\n",
    " becoming\n",
    " f +0 <x> -0 -> <x>\n",
    " f +1 <x> -1 -> <x>\n",
    " f +2 <x> -2 -> <x>\n",
    "and so on to some upper limit\n",
    "We could occasionally renormalize maybe if there are no +n -n pairs remaining.\n",
    "then shuffle all the above ones down.\n",
    "Ok but what about something that increases the depth\n",
    "x -> f(x)\n",
    "... hmmm.\n",
    "\n",
    "And if we number from the bottom?\n",
    "\n",
    "f +2 <X> -2 -> <X>\n",
    "\n",
    "and\n",
    "<X> -> ??? Well a raw pattern is pretty shitty\n",
    "f(x) -> f(f(x)) becomes\n",
    "f +n <X> -n -> f +(n+1) f +n <x>\n",
    "Yeah. Numbering from the bottom is better. We don't have to\n",
    "\n",
    "f(stuff,stuff)\n",
    "f +n <X> -n +n <Y> -n\n",
    "\n",
    "even without enumerating\n",
    "plus +<n1> <X> -<n1> +<n2> <Y> -<n2>\n",
    "\n",
    "Oooh. We have to enumerate every combo of possible subterm depths.\n",
    "\n",
    "Hmm. This adjust levels\n",
    "\n",
    "<http://matt.might.net/articles/sculpting-text/>\n",
    "\n",
    "A unique terminator for the subexpression is the point.\n",
    "f +2 (^ -2)* -2\n",
    "\n",
    "could have a counter per symbol. per symbol depth.\n",
    "f1 ( yadayada) \\f1\n",
    "\n",
    "fa1 <X> fb1 <Y> fc1\n",
    "\n",
    "huh. What about the CPP? Won't that basically work?\n",
    "\n",
    "This is horribly inefficent because it'll expand out huge terms.\n",
    "And big backtracking jumps. Or rather big seeks while it tries to find the next spot to go to. The next argument of f.\n",
    "\n",
    "For ground term rewriting it seems actually reasonable and faster than having indirections. We can't have sharing though. That is a bummer.Maybe with zip.\n",
    "Unless our goal is simplifcation.\n",
    "\n",
    "using the rule\n",
    "\n",
    "We could try to use the e-matching strategy where we iteratively instantiate fixed ground rewrite rules into the sed file itself?\n",
    "\n",
    "Instead of using parenthesis, one could use numbered enter level exit level. And then bound the number of levels.\n",
    "Each term rewriting becomes a string rewriting (with named regex holes ) replicated by the number of supported levels.\n",
    "\n",
    "using sed on itself we might be able to implement knuth bendix\n",
    "\n",
    "One could compile into a human unreadable huffman encoded like thing\n",
    "\n",
    "<https://superuser.com/questions/1366825/how-to-recursively-replace-characters-with-sed> looping in sed\n",
    "You can gnu parallel into chunks\n",
    "grep is much faster. If terms are rare, maybe find using grep first?\n",
    "\n",
    "Suffix trees can store every subterm of ground term with efficient query.\n",
    "\n",
    "<https://spencermortensen.com/articles/serialize-a-binary-tree-in-minimal-space/> an interesting perspective on tree serialization\n",
    "catalan numbers. We know size of tree if we give elements.\n",
    "<https://www.educative.io/edpresso/what-is-morris-traversal> woah. Bizarre. It mutates the true as it goes down it and store\n",
    "Kind of reminscent of dancing links in a way\n",
    "\n",
    "f 20 90 190 yada yada yada could desribe links to the approprate spots.\n",
    "This would be the analog of flatterms.\n",
    "\n",
    "There is something like encoding lambda terms with de bruijn. vs unique signifiers.\n",
    "If we could encode the unique signifiers in a way such that they never collide.\n",
    "\n",
    "There is something to that we're kind of converting to rpn.\n",
    "<https://github.com/GillesArcas/numsed>\n",
    "<https://github.com/shinh/sedlisp>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
