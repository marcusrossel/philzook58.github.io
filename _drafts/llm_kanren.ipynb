{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llm\n",
    "\n",
    "\n",
    "# llm datalog?\n",
    "\n",
    "# use llm to auto build descriptor?\n",
    "def auto_desc(state):\n",
    "    \n",
    "\n",
    "\n",
    "def conj( *args , desc=None):\n",
    "    def res(state):\n",
    "        for a in args:\n",
    "            for state in a(state):\n",
    "                \n",
    "        return state\n",
    "    return res\n",
    "def disj(*args ): \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def self_describe(x):\n",
    "    x.__code__\n",
    "    prompt = \n",
    "    \"\"\"\n",
    "    This is source code for a search function. There are a couple of choices avaiable.\n",
    "    Pleas\n",
    "    \"\"\"\"\n",
    "\n",
    "@self_describe\n",
    "def foo(x,y,z):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about using the llm to self annoate the code using __code__ attribute. Then any hints could just go in comments. That's nice.\n",
    "\n",
    "Then I can go and get stock minikanren programs and see if it does better.\n",
    "\n",
    "\n",
    "blockworld\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of minikanren is that search is modellable as\n",
    "`state -> list state`\n",
    "\n",
    "\n",
    "state is a list of descriptions\n",
    "we could have an automatic description generator as a default\n",
    "\n",
    "\n",
    "We could also probably throw some RL-iness on there. We're doing tree search, so we could have the llm \n",
    "\n",
    "\n",
    "\"What clues would you use to do it faster next time?\"\n",
    "\n",
    "Choices:\n",
    "1. abort\n",
    "2. choice 1\n",
    "3. choice 2\n",
    "\n",
    "https://news.ycombinator.com/item?id=39479478 A* boosting. \"searchformer\"\n",
    "Feed the llm the trace, not just the state.\n",
    "\n",
    "Kind of need a compelling problem. That's tough? Why should that be tough?\n",
    "\n",
    "neural kanren https://github.com/xuexue/neuralkanren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conj(cost_heuristic=None):\n",
    "def disj(cost_heuristic=None):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic minikanren\n",
    "# ignore the unification stuff. Who cares.\n",
    "def conj(a,b):\n",
    "    def res(state0):\n",
    "        for state1 in a(state0):\n",
    "            for state2 in b(state1):\n",
    "                yield state2\n",
    "    return res\n",
    "\n",
    "def disj(a,b):\n",
    "    def res(state0): # unfair interleaving\n",
    "        for state in a(state0):\n",
    "            yield state\n",
    "        for state in b(state0):\n",
    "            yield state\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth limitted search.\n",
    "def disj_depth(a,b,N=10):\n",
    "    def res(state0):\n",
    "        if state0.depth > N:\n",
    "            return\n",
    "        state0.depth += 1 \n",
    "        for state in a(state0):\n",
    "            yield state\n",
    "        for state in b(state0):\n",
    "            yield state\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{score: (state, expand)} # min heap \n",
    "conj : state -> [(score, state)]\n",
    "disj : \n",
    "# take a look at the first orderized minikanren\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say this is a test\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "chat_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def system():\n",
    "    prompt f\"\"\"\n",
    "        This is a minikarnren like search program looking for solutions to X.\n",
    "        I want you to guide it towards good choices.\n",
    "        For example, if you had the state \n",
    "          \n",
    "        Return ONLY the number number of the choice of which state to expand.\n",
    "        {examples}\n",
    "    \"\"\"\n",
    "    {\"role\": \"system\", \"content\" : \"T\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytroch install.\n",
    "https://github.com/ROCm/ROCm/discussions/2932\n",
    "Actually Radeon 680M and 780M are supported by the latest ROCm 6.0, what you need to do is to set HSA_OVERRIDE_GFX_VERSION=10.3.0 for 680M, and HSA_OVERRIDE_GFX_VERSION=11.0.0 for 780M.\n",
    "\n",
    "huggingface-cli for model management\n",
    "\n",
    "`rocminfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HSA_OVERRIDE_GFX_VERSION=10.3.0\n"
     ]
    }
   ],
   "source": [
    "%env HSA_OVERRIDE_GFX_VERSION=10.3.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HSA_OVERRIDE_GFX_VERSION=11.0.0\n"
     ]
    }
   ],
   "source": [
    "%env HSA_OVERRIDE_GFX_VERSION=11.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_ROCM_ARCH=\"gfx1103\"\n"
     ]
    }
   ],
   "source": [
    "%env PYTORCH_ROCM_ARCH=\"gfx1103\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: AMD_LOG_LEVEL=3\n"
     ]
    }
   ],
   "source": [
    "%env AMD_LOG_LEVEL=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: AMD_SERIALIZE_KERNEL=3\n"
     ]
    }
   ],
   "source": [
    "%env AMD_SERIALIZE_KERNEL=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbc47baa94e743398db7f117de0024f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philip/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write me a poem about Machine Learning.\n",
      "\n",
      "Machines, they weave and they learn,\n",
      "From\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", device_map=\"auto\")\n",
    "#model.to(\"cuda\")\n",
    "\n",
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\") #.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write me a poem about Machine Learning.\n",
      "\n",
      "Machines, they weave and they learn,\n",
      "From\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15cf32e036884d0db1d34df165076164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "./gemma-2b-it.gguf:   0%|          | 0.00/10.0G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-89948379-b0a2-46df-b224-c1c75e2c359c',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1711641796,\n",
       " 'model': '/home/philip/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/718cb189da9c5b2e55abe86f2eeffee9b4ae0dad/./gemma-2b-it.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': '\\n\\nI am unable to access external sources or display images, so I am unable to provide a detailed description of the image you have specified.'},\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 33, 'completion_tokens': 28, 'total_tokens': 61}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "# https://github.com/abetlen/llama-cpp-python\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"google/gemma-2b-it\",\n",
    "    filename=\"*gemma-2b-it.gguf\",\n",
    "    verbose=False\n",
    ")\n",
    "llm.create_chat_completion(\n",
    "      messages = [\n",
    "          {\"role\": \"system\", \"content\": \"You are an assistant who perfectly describes images.\"},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"Describe this image in detail please.\"\n",
    "          }\n",
    "      ]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
