---
author: philzook58
date: 2020-12-06
layout: post
title: "EGraphs Part III "
---


- [Applications and Ideas](#applications-and-ideas)
  - [Defaults](#defaults)
  - [Egraphs and Terms at the same times](#egraphs-and-terms-at-the-same-times)
  - [CHC](#chc)
  - [refinement typing](#refinement-typing)
  - [Gam |- e => A    A = B](#gam---e--a----a--b)
  - [Interval arithmetic](#interval-arithmetic)
  - [Destructive Rewriting](#destructive-rewriting)
  - [Macro System](#macro-system)
  - [Proof provenance = extraction?](#proof-provenance--extraction)
  - [Proof](#proof)
  - [ASP for ILP extraction?](#asp-for-ilp-extraction)
  - [the chase](#the-chase)
  - [Prolog v Datalog v Egglog](#prolog-v-datalog-v-egglog)
  - [Egglog vs SMT](#egglog-vs-smt)
  - [Egglog vs Flix vs IncA](#egglog-vs-flix-vs-inca)
  - [Gogi](#gogi)
  - [Related systems](#related-systems)
  - [Lattices as egraphs](#lattices-as-egraphs)
  - [Harrop Clauses](#harrop-clauses)
  - [Top down evaluation](#top-down-evaluation)
  - [Constraint handling rules CHR](#constraint-handling-rules-chr)
  - [CAS](#cas)
  - [Quoting out of egraph, reflection](#quoting-out-of-egraph-reflection)
  - [Bisimulation finest partition](#bisimulation-finest-partition)
  - [Typeclasses and Rust Chalk](#typeclasses-and-rust-chalk)
  - [provenance](#provenance)
  - [Semiring smenatics](#semiring-smenatics)
  - [Lambda](#lambda)
    - [Extract and do stuff](#extract-and-do-stuff)
    - [Yinhong let binings](#yinhong-let-binings)
  - [Scoped union find](#scoped-union-find)
  - [Higher order rules](#higher-order-rules)
  - [Semantics of Egraphs](#semantics-of-egraphs)
  - [/](#)
  - [NE graph](#ne-graph)
  - [Egraphs over programs. Program analyiys](#egraphs-over-programs-program-analyiys)
  - [Metatheory and EGraphs](#metatheory-and-egraphs)
    - [Bits and Bobbles](#bits-and-bobbles)
- [A More Naive EGraph](#a-more-naive-egraph)



# Applications and Ideas
Applications I guess
- staged egg
- higher order egglog, skolemization
- A = C - B :- A + B = C. A = Lit(1/C) * B :- Lit(C) * A = B, C != 0 (* Hmm. Recipricating C over and over probably isn't good. *)
- Coq tactic
- Dependent programming proof relevant union find
- Applications: Category theory proving, solving systems of equations, patching compiler
- Lambda terms / binding forms
- efficient encoding to souffle

- instruction matching modulo equality
- xor linked list pointer analsyis
- cos^2 + sin^2 = 1+ interval analysis. Some expression better than others. Tighter interval bounds/
- `pos(x), sqrt(x**2) -> x, neg(x), sqrt(x**2) -> -x`
- x * x -> beter upper bound. Sharing problem

- instruction matching over egraph of cfg. Maybe just a single block
- knot diagrams as braiding algebra? Prove unknot

- Rewrite query to equivalent query for a database. Instruction matching modulo equality.
- any application of the chase. Data migration.

- Carette inspired guarded rewriting. Datalog style fixpoint analysis makes sense
- Reified sets using SMT style union axioms and such.
- Reified lattices in similar way. Lattices defined via generators quotiented by relations

- refinement typechecking a la liquid haskell?
- The "Sub" rule of bidirectional type checking. We can do typechecking modulo equality.

LADDDER benchmakr

Remember my point about Ocaml Set data structure, except `compare` is changing under the hood
Union Find dict.
Lattice ranged dict.

Tuples of union find

https://www.swi-prolog.org/pldoc/man?predicate=dif/2 prolog supports `dif` constraint. Pretty interesting duality to egglog that only supports = constraint
ephemeral union find - how to support provenance?

Could there be an example where egglog is superior to formulog with respect to symbolic execution? In some sense, egglog is a much tighter integration of a piece of smt + datalog. Maybe equational reasoning could see that different paths are equivalent at join points?

[composing data analysis and trasnformation - sorin lerner](https://cseweb.ucsd.edu/~lerner/UW-CSE-01-11-01.pdf)


Dynamic rules and macro based compilation aren’t necessarily mutually exclusive. I think you could call rustc and then dynamically link in new rule. A somewhat complex architecture though https://docs.rs/libloading/latest/libloading/

Incremental egglog. We could perhaps use one of the non union find based encodings?

## Defaults

do we want to have complete lattices.
What is the default value

Default value could be metavariable

I've been tinkering with a prolog/CHR encoding of egglog and in that case defaults being unification variables is by far the easiest thing to do and not costly since the host language has them. But it does seem like a pain to do it in the rust implementation

If unification variables can point to constants then we don't need the `Num` constructor.



## Egraphs and Terms at the same times
Max: Logg table and materialized 

You need both terms and egraphs. Without the egraph union find action, we just have a hash cons. During extraction, we'll want to be using the term version.
If you used a min lattice for example rather than "spooky" union find, that is a hash cons, where we just ignore the version that has a high id for some reason (because of failure to check properly probably.)

I'm kind of thinking that when we define "datatypes", we need to generate two different things. Egraph like tables and hash-cons like tables add_egraph : eid, eid -> eid  vs add_hash : int, int -> minlattice Hash-cons tables would let us talk about terms, which could be attached to eclasses via lattice mappings. Another way of looking at hash cons tables is that their output ids use min-lattice rather than spooky union find "lattice".  Another thing that I think is useful is to "downcast" eclass ids to regular ints, like how the max-lattice could be downcast to an int. I suspect but do not see clearly that with that capability, it is possible to record the equivalence proofs using a regular table instead of a fancy union find (upon every discovery of a new equivalence, record that tuple. This is basically what gets recorded in proof producing union find anyway. Reconstructing the actual proof path seems like it could be expressed as a datalog query itself since UF is used for connected components, proof of connected components is a path, and _the_ classic datalog query is for paths) , avoiding my concern about yihong's "no global union find" version.

It might be desirable for the hash cons table to be garbage collected in some sense. It might also be better to just use a regular hash cons 

The hash-cons / egraph table distinction fell out naturally in the souffle encoding where add(x,y,z)  was the egraph table, whereas $Add(x,y)=z . Hmm. This is not right. We put eclass ids into $Add().

There does have to be an injection of terms into eclassid
inj(tid , eid)
There is a natural table injecting term_ids into eclass_ids, and in principle nice versa, although the opposite direction easily becomes an infinite table

Another way of talking about this is that in ordinary datalog you can make an eq relation
eq(x,y) :- eq(y,x).
eq(x,z) :- eq(x,y), eq(y,z).
Asserting any equality to this relation then gets completed with transitivity and symmetry. Instead you could model this in "edge path" style by asserting discovered equations to eq_edge
eq(x,y) :- eq_edge(x,y).
In souffle and in egglog,eq is done with special data structures, but it is still possible to keep eq_edge for the purposes of recording data.




## CHC
If constraints are egraph constaints.
You need it to satruate though. Wouldn't this alrady be the obvious thing for an smt solver to do?

reach() :- reach()

equational theory over state.
Why would you have this?
Boolean operations I guess.

Why would loops ever terminate?

## refinement typing
I have another possible application related to the hindley milner one yihong did. I've been encoding bidirectional typing into souffle and I suspect something interesting could be done using egglog in the ever mysterious chk-syn rule. Page 3 https://arxiv.org/pdf/1908.05839.pdf This is also the rule where refinement typing puts the smt subtyping query, so there is some relationship there. https://arxiv.org/pdf/2010.07763.pdf pg 15
Gam |- e => A    A = B
-----------------------
    Gam |- e <= B

Maybe refinement typing could be done directly. Not sure. The resulting verification conditions are horn clauses i think? Not sure if that is good enough.


infertype as a partial function
I have been considering using option to represent the binding patterns ofmagic set.
infertype
checktype

Hmm. Doesn't really match does it. No, maybe it does


## Interval arithmetic
Does anyone have an example where rewriting makes an interval more precise?
white_check_mark
eyes
raised_hands



The really simple one was cos^2(x) + sin^2(x) -> 1 takes [0,2] to [1,1]
yeah, that's good. just looking for some easy tests
For that matter x * x -> x^2  also works. if x was in [-1,1] the lhs is [-1,1] whereas the right is [0,1] if you consider squaring as a primitive interval 
You want to rewrite to fused ops you have primitives for (edited) 
ah that's a good one
But it isn't clear how to get your way there, hence egraphs
you dont even have to fuse, you can just have a different rule for lo(x*x)
And the same pieces might be parts of two different fused expressions (I think) (edited) 
Hmm. I don't think I understand the lo(x*x) thing
Oh...
Interesting
you don't need a square ast node at all
I see.
That's cool
you can just have a rule for lo(x * y) and lo(x * x)
Very cool.

S U B S U M P T I O N
Also maybe  div(y,x), posnegzero(x)  could have a good rule

You could also maybe encode hi(x) = sqrt(n) :- hi(x*x) = n (edited) 
Back driving expressions

And so on hi(x) = a - b :- hi(x + y) = a, hi(x) = b

https://juliaintervals.github.io/pages/tutorials/tutorialConstraintProgramming/
https://juliaintervals.github.io/pages/tutorials/tutorialConstraintProgramming/
Hmm. I wonder if we could make a lattice out of taylor models. That'd be fancy
I'd guess the join of two upper bounding polynomials would be the polynomial that stays above both at all points (of the domain of interest) and minimizes some objective. some variant of a sum of squares optimization (edited) 


An example where there isn't a best thing to do in regards to rewriting?
For assoc and comm you'd want them all. You should just group together like terms.

(x >> 2)

(a * 2) / 2




What about floating point reasoning?


Ok I might have an example where it starts to get hairy to arrange a destructive rewrite simplifier to do the best thing every time
white_check_mark
eyes
raised_hands



consider (a + b)(c + d). Should it be expanded?
(a + b)(a + d) should if we can recognize  a*a as a special sharing case. There is not sharing in the factored form (edited) 
(a + b)(b + a) shouldn't. We can commute, and then recognize a sharing at a larger level , which gives better bounds
Ok so maybe the rule should be canonicalize every factor, pair up, and then expand the pieces that don't share up. But that rule is messy to express and questionable. You might just need to keep patching it
I think running these with all variables as [-1,1] is sufficient to demonstrate the difference
Also partial expansions. (d + (b + e))(d + e). Finding the sharing here with destructive rewriting is hard


Herbie Use rationals

.decl exact(e : Expr, v : rational)
.decl approx(e : Expr, samplenum : unsigned, value : float)
approx(e1, i, v1) <= approx(e2,i, v2) :- eq(e1,e2), exact(e1, v), abs(v - v2) <= abs(v - v1).

.type rational = [n : number, d : ]
Yeah. Not particularly gonna be better than floats.



## Destructive Rewriting 
CHR
Linear Logic programming


"pruning", destroying entire eclasses? Or never put stuff into "dead" ecalsses. You don't want to both making num(x,a) equal to anything else. It's a really good term.

## Macro System
https://stackoverflow.com/questions/14459647/is-it-possible-to-generate-and-execute-rust-code-at-runtime

Relatedly : user defined functors.

Prolog style macro system. We could use egglog itself to compile egglog programs? Kind of makes sense.


Use guile? Seems kind of an insane dependency.
Maybe having the master program be racket or whatever and just expose a C api everyone can use.
Then you could use normal racket macros.

Prior datalog API:
- Souffles C api. Pretty basic. There are relations.

It's really hard to have good coupling to rust features but also be portable.


Martin mentionedFortress rewrite rules and Rel uses higher order compile time stuff inspired by hilog?
Flix has its own language.
Embedded datalogs of any kind can construct programs
Souffle has CPP

Many transformations are embedding this or that concept to stock datalog features. If you want that to be a macro system so be it.

That the souffle _compiler_ is written in C++ seems kind of insane. But they would have a 2 language problem.
A bytecode intepreted datalog.

## Proof provenance = extraction?
initiaitng the egraph
var("x",1).
var("y",2).
add(1,2,3) :- var("x",1), var("y",2).
add(3,1,4) :- var("x",1), add(1,2,3).

Consider the following methodology for initiating the egraph. In some sense we're only allowed the make add(1,2,3) because 1 and 2 are already in this egraph.
var("x",1).
var("y",2).
add(1,2,3) :- var("x",1), var("y",2).
add(3,1,4) :- var("x",1), add(1,2,3).

The souffle proof of add(3,1,4) would exactly mirror the "term" that we put in eclass 4
I suppose what I'm doing is associating a grounded rule with every node in the original term.
rather than just a grounded fact.

proof of smallest cost analysis = term. If you ignore the parts of th proof of how the terms got there

magic set for top down extraction.

rel height(x,h : lat) | x -> h.
needheight(x), needheight(y) :- needheight(z), add(x,y,z).
height(z, min(hx,hy) + 1) :- needheight(z), add(x,y,z), height(x,hx), height(x,hy).


// needheight ~ reduciable
needheight(fx) :- app(f,x,fx), lam(v,b,f).
action@insert(t) :- redexp(fx), extract(fx, t).


Grounded rules are proof steps.
That's why the provenance thing worked. A rule name + missing variables plus what was in the head.
You could also make a new rule to record the rule. This is perhaps somewhat wasteful however.
rule1(a,b,c) :- body1().
head1(a) :- body1().

Or! Any usage of head transform into one of the usagees of rule
foo(x) --->  { rulefooheahd() ; rulefoohead() ; rulefoohead() } 
Then you don't need structures. But then you need...

These rules are enough to construct the grounding of the program?
Well, the tables were also in the absence of negation.

## Proof
(i,j) tuples for each equality

explain(i,j).
```
$Eq(i,j)
.decl explain(i,j)
.decl eq(i,j) eqrel
```
eq is closed under trasnitivty
explain isn't
```
eq(i,j) :- explain(i,j, r).
explain(i,j, $AccoL()) :- 
explain(i,j, $AssocR()) :-
explain(i,j, $Comm()) :-

explain(i,j,$Cong()) :- // congreunce 

```
eq is bottom up, but then upon solution, we can extract explain top down.


## ASP for ILP extraction?
I've become more intrigued by the plausibility of using ASP solvers for extraction. Deeply integrating ASP techniques or semantics takes us wildly out of scope, although might be possible. But instead, if extraction is stratified after eqsat, one could print out the normalized egraph database directly into an input file for clingo.
```
num(4.8, 0).
num(754.3, 1).
add(1,2,4, analysis_res_1, analysis_res2, ...).
% and so on
```
And then write the clingo constraint program that operates over these base facts. Alternatively instead of printing this file could use rust bindings: https://github.com/potassco/clingo-rs . See for a flavor of the splitting between the program clauses and the incoming data this graph coloring example https://stackoverflow.com/questions/41191313/asp-clingo-splitting-graph-to-n-cliques .
ASP is a hyper expressive language for optimization problems involving constraints for things like graph coloring, cliques, subset cover or hamiltonian cycles and other things and it feels like it would be a fast way to write strange custom extractions. There seems like relatively low impedance mismatch between the egglog and clingo compared to other optimization methodologies like CSP or IP because they are both rooted in logic programming and relations. Having said all that, I don't know to what size clingo scales, I don't have an example where such sophisticated extraction is warranted, and I'm not 100% sure I know what an extraction encoding would look like.


Kind of related thing one could do is take the grounded eqsat database and encode it into a sat solver. We're not trying to compete in the same domain as sat solvers though. And what do I really mean by this? What kind of boolean logical structure am I considering?
  horn clauses 
+ quantified boolean formula without equality? forall x, f() \/ f() = f() \/ \/ \/ 
It has a flavor of ackermannization. Could only ever return unsat?

Maybe this is just a bad SMT solver.
It's like reversed though. SMT solvers are SAT with theories laid on top. This is EqSat with SAT laid on top.

Remy points out that MIP solvers are awesome. No objections here.

How does ILP work? You get an integer variable for every enode? You make linear constraints that... something?

## the chase
Would it be fair to say that ordinary datalog itself is the chase over full dependencies?
https://dl.acm.org/doi/pdf/10.1145/1514894.1514897 Datalog+- a datalog framework supporting TGD and EGD. A theory, not implementation paper?
A slightly more recent talk http://people.scs.carleton.ca/~bertossi/talks/introAlgsDat+-(16)Pruned.pdf
A thesis on the chase of datalog programs I haven't found the full version of https://www.proquest.com/openview/665c3cbe7753bef1a57a2ccc7bbdb775/1?pq-origsite=gscholar&cbl=18750&diss=y
The chase of datalog programs - ProQuest
Explore millions of resources from scholarly journals, books, newspapers, videos and more, on the ProQuest Platform.
What do you mean by full dependency? Datalog is a chase that only supports tgds
I was reading the Alice book and it was terminology I haven't seen anywhere else yet. Full dependency was rule without existential in head.
Chapter 10 http://webdam.inria.fr/Alice/
Also very interesting that they mention the EPR class of formula
https://twitter.com/SandMouth/status/1464685169214951426?s=20&t=faU6J4XGjpXRLQNoQeJx1Q@SandMouth
Is EPR kind of sort of datalog? The lack of function symbols is pretty similar
Yihong replied he had asked the same question
My impression of what is considered "datalog classic" is that it didn't include TGDs, but any actual datalog with a counter, or integers, or adts probably does.
Is the chase an operation over the actual tables, the schema, queries, or all of them depending on the context?
The chase for Egds involves picking one variable and substituting it into all occurences. This is the naive implementation of unification?
And by variable maybe i mean "labelled null"?
Amusingly, the only way the chase is making sense to me is basically saying "the chase is egglog" rather than "egglog is the chase"
Given my relatively poor database background


So for ephemeral union find, how do we track proof. When we do chase step, we must record it somewhere for provenance

graal https://graphik-team.github.io/graal/papers/graal-ruleml2015.pdf The graph of rules. You can check if rules can unify?


## Prolog v Datalog v Egglog
What makes 

prolog is structural unification g(_) = f(_) can never succeed
Modeling egglog using CLP, adding constraints of equality to and egraph can never fail.

Max has been limitting in the head.

## Egglog vs SMT
Is egglog jyst a bad smt solver wihout dijsunction?

## Egglog vs Flix vs IncA

union find is lattice
join() = parent if in same 

lattice that's changing


lattice in input position? Because eclass id are in input position

dynamic lattices - dpeendent lattice

forall,  -> L a dependent function
A dependent relation?

A lattice that depends on a table.

Hmmm. 

the lattice type also move monotonically

other latic

on the query side flix goes both ways


chase = 

join is spooky
meet is findparent

body vs head
fd output vs input

Why is sort allowed in both, but lattice allowed in 

partial lattice


subsumption semantics

"Everything is a pointer to a pointer to an element of the powerset lattice!!"

"dynamic typeclass"
You could do this in ocaml. This was my point about Set.t with a global thing.

typeclass Lat L {
  join(x,y,z) :- parent(x,y).
}

flix + trees

Is partitioning 


We could probably build a union find using lattices. Of course. Yes.
Remy points out that this does not normalize enodes however.
So this "lattice in negative position" thing is meaningful.

parent()

```
parent(number, maxlattice).
parent(i,k), parent(k,i) :- parent(i,j), parent(j,k).
parent(i,j), parent(j,i) :- findsomethingstbeequal(i,j).
```

I actually thingk the lattice concept is a red herring.


Dependent prolog datapoint - Twelf.
Dependent vs not and lambda vs not seem like distinct axes. Given that lambda prolog has lambda but is not dpeendeltly typed.
A datalog twelf?

type("parent", symbol, symbol).
constructor were named rules


Calling something a lattice macro expands a join relationin the right spots.
This join relation could be intertwined. And really has to be because of demand drivenness



## Gogi

we can have exists, but ONLY in functinally dpenednet position (at least one)
A subclass of TGDs




## Related systems
See datalog notes
datafun, flix, rel
DDlog 
propagators
lvars?



## Lattices as egraphs
Datalog for Things without canonical forms. Then egglog is key.
What don’t have obvious canonizing rewrite systems? Groups. Lattices. Given as generators module equations. Why would I do this?
Oh that’s cute. If we want to internalize lattices egglog might be good? but the lattices we talk about aren’t equalitional presentations.


This might be interesting. Hash-consed points-to sets https://yuleisui.github.io/publications/sas21.pdf , I think they are saying that storing all the indexed points to sets as relations is highly redundant, when really you want to store a foreign key to a points-to-set table, memoizing the sets. This foreign key is kind of a reified set identifier. I think one could consider an uninterpreted function based representation of sets (union s1 s1), (sing x), etc  https://z3prover.github.io/api/html/ml/Z3.Set.html https://stackoverflow.com/questions/17706219/defining-a-theory-of-sets-with-z3-smt-lib2  and giving egraph rewriting axioms for union associativity, commutativity  etc. These union egglog enode tables are very similar (identical?) to their operation memoization tables. It's kind of an interesting angle to reifying sets as objects in datalog. Although I have some other ideas of how to do this that don't really require egraphs (I'm currently very tickled by the idea of hash consing patricia tries).

Stack OverflowStack Overflow
Defining a Theory of Sets with Z3/SMT-LIB2
I'm trying to define a theory of sets (union, intersection etc.) for Z3 using the SMTLIB interface. Unfortunately, my current definition hangs z3 for a trivial query, so I guess I'm missing some ...


Martin Bravenboer  33 minutes ago
We tried that too in Doop but it didn't perform well at the time. It's super performance sensitive. I think it'd be better to implement compression in the underlying database system, which would also be more declarative.

Martin Bravenboer  32 minutes ago
It's similar to the idea behind the bddbddb points-to analysis work, which used BDDs for compressing the points-to sets.

Martin Bravenboer  31 minutes ago
I don't recall what method we used, sorry  (>10 years ago ...). It was not sophisticated so maybe it can be done better yeah.

(speculative shadowy idea) Maybe one could write BDDs also using egraph rewriting powers. It'd be interesting to have a system that has both relational and bdds available (if bddbddb didn't already have this. I'm not familiar with it's details)

Martin Bravenboer  28 minutes ago
It's very sensitive to variable ordering, so for it didn't work great for representing all relations (as in bddbddb). But some controlled usage may be fine yeah

There is something pleasing about the user level simplicity of defining their own set representation equationally even if it may not be as performant as a custom extension of some kind. One could say something similar of datalog based analyses in general.
In a related meta direction, one could consider defining a lattice equationally rather than as functions (join is an uninterpreted function that is associative, commutative, absorptive, and obeys individual lattice specific equations). I don't have an example in pocket of where this would be useful (e
But in principle, lattices, like many algebraic structure, are sometimes defined by generators quotiented by equivalence classes. In particular non finite lattices
I wonder if it would be useful semantically to attempt to consider lattices as a subcase of egraph mechanisms rather than egraph union finds as a subcase of lattices. I don't know exactly what I mean, but it smells plausible. (edited) 
I suppose you'd have to know that to fix a functional dependency, you need to apply a join uf to the two eclasses. This idea is of some relation to materializing the join function as a table.



## Harrop Clauses
See also higher order rules
Gensym for quantified stuff. But you can't keep gensyming wantonly. So combine with not guard so you only do gensym once.

## Top down evaluation
Why does CLP seem so similar and yet so far? What is up with dif/2
We can express linear equalities, boolean equalities in SLD
We could ackermaize into booleans.
egraph equality can't fail. unification equality can.

Prolog needs a backtrackable union find.




## Constraint handling rules CHR
A question I never had an answer for https://twitter.com/notjfmc/status/1422215450675535877?s=20&t=RyHMtBS3ySaALLC9MuGmUA . CHR afaik are a way of integrating eager rewriting into prolog https://en.wikipedia.org/wiki/Constraint_Handling_Rules (edited) 

http://www.informatik.uni-ulm.de/pm/fileadmin/pm/home/fruehwirth/constraint-handling-rules-book.html
[chr what else](https://arxiv.org/pdf/1701.02668.pdf)

compiler to sql statements. Makes sense.
Multiset rewriting?
- [A More Naive EGraph](#a-more-naive-egraph)


## CAS

Hmm. You know, I feel like egglog is very close to being able to express this concept via a rule already. If you tag terms with context
 foo(ctx2, ?x,?y) = bar(ctx2,?z)  :- foo(ctx1, ?x,?y) = bar(ctx1,?z), ctxs(ctx2), ctx1 <= ctx2  
It is wasteful to some degree. But not obviously more wasteful than just keeping a labelled graph as a disjoint set datastructure
If we can store lattices in the range of foo, it might be even less wateful

carette discussion https://julialang.zulipchat.com/#narrow/stream/236639-symbolic-programming/topic/x-x.20is.20not.20necessarily.20equal.20to.200/near/232167346
https://arxiv.org/abs/1904.02729 - specifying symbolic computation, carette and farmer


Hmmm. Is extraction some kind of quote operation? If I extract and reinsert, it does nothing. Maybe it needs to be guarded somehow. With like a metalevel annotation? foo -> :foo -> ::foo -> :::foo
a = extract(foo)

##  Quoting out of egraph, reflection

extract(foo(a,b)) = cons(:foo, :a, :b) which is a valid term to put back in.
extract = quote
This is how to extract and reduce lambda calc?

see also higher order rules

## Bisimulation finest partition
https://cstheory.stackexchange.com/questions/37177/partition-refinement-in-transition-state-systems-bisimulation-contraction partition refinment bisimulation is an iterative fixpoint algorithm. Sure we could encode transition system as relation. We need reified notion of partition (like eclass id did). I could model using bitsets in souffle. pairition(bs) <= parition(bs) :- SUBSET(bs2,bs1). Goes twoards finest partition. 

## Typeclasses and Rust Chalk

Something to think about: Applications of egglog to typeclass resolution modulo equality. https://arxiv.org/abs/2001.04301 I don't really have a feel for the relation between tabled top down and magic set transformed bottom up although there clearly is one. I guess I'm also not clear on how magic set works in egglog for that matter. I don't know how to execute egglog top down in the first place.
An example would be finding a typeclass for Vec n a where you take the assoc/comm mult and add  axioms on the size index. Presumably the equational axioms must also be associated with instance dictionaries. Hmm. Maybe this is not as straightforward as I thought. You also need instances for congruence?
There isn't a problem with ordinary bottom up egglog, it just seems very wasteful on a query driven application like this

Chalk is also a very good point. [The cahlk blog posts](https://github.com/rust-lang/chalk) exliciitly talk about specialized equality. That is intriguing. I should contact Niko Matsakis.
https://smallcultfollowing.com/babysteps/blog/2017/01/26/lowering-rust-traits-to-logic/ Prolog + euqliaty reasoning he syas he's looking for

Chalk even talks about making a union find.


asscoaited types
IntoIterator
Item

```
intoiter(vec(T)) :- intoiter(T).
iter_item(vec(T)) = T :- intoiter(vec(T)).


eq(T,U)

query:
into_iter(vec(T))


```
Good examples of multi arg typeclasses?


Provenance of chalk or polonius are importatn for good error messages.

Guess missing clauses?

We need harrop for generic problems?


## provenance
How do we not fuck provenance.


## Semiring smenatics
prvoenance
smemiring smenaitcs
[provenance semirings](https://dl.acm.org/doi/10.1145/1265530.1265535)
[lecture on this](https://courses.cs.washington.edu/courses/cse544/12sp/lectures/lecture18-provenance.pdf)
oh. the max plus semiring. I see.
[souffle provenance](https://arxiv.org/abs/1907.05045)
[provennce guided synthesis](https://www.cis.upenn.edu/~mhnaik/papers/popl20.pdf) - is this related to ruler?

Hmmm. Could you write linear tensor recurrences this way? atoms are still indices carries amplitudes though. How do you avoid recomputation? Collecting up inifintie loops via memoization.
Kind of a feynman diagram thing.
Base propagator list 0f u(0,0, ampl). u(0,1,amp).
Magic set. If we want to query just a single vector?
linear algerba and graph algorithms are known to be connected in this way.
Maybe you need the product semiring to also track index provenance.
The lattice character of max seems important
dynamic programming

shortest path
foo( , iter+1) :-  foo() ,iter < MAX
foo(cost) <= foo(cost1) :- cost <= cost1.

Maybe monotonic increase of probability?.. Eh. Still seems hard to mannage
[https://arxiv.org/pdf/2105.14435.pdf](convergence of datalo over pre semirings - remy papers)
https://arxiv.org/pdf/2103.06376.pdf semiring dictionraries


()
()
()

r
delta_r

r :- delta_r
r :- yada yada r
No that's not good enough. It doesn't know r is already saturated over the rules. Is ther any way to trick it? Directly accessing delta relation is a good point.


[efficient encodings of first irder horn to equation logic](https://smallbone.se/papers/horn.pdf)

[equational term graph rewriting](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.53.6428&rep=rep1&type=pdf)

I've still never tried just throwin into vampire. It has a horn clause thing right?

https://github.com/nadia-polikarpova/cyclegg

yihong https://drive.google.com/file/d/18m0_fCT21-RLxB8EedmC1MuzhAM4In6X/view
Profile driven optimization. Better scheduling?

Lvars - Guard conditions are filters

What if we rearranged the storage of analysis in egg.

[twitter egraphs for query planning?](https://twitter.com/justinjaffray/status/1501294656239329281?s=20&t=qlKx5dL5bXCILsNAtcDYfw) "like cascades but better"

parent( x : Id, p : Id)


parent(x,p) <= parent(FIND(x),FIND(p)).
parent(x,p) :- add(x,_,p).
parent(y,p) :- add(_,y,p).
 :- parent(x,$Add(x,y)), parent(y,$Add())

## Lambda 

The hash cons modulo equia version uses maps at lampba sites. But then we need to manipulate the maps at the same time we manipulate expressions.
co debruijn? See jasper

https://pavpanchekha.com/blog/egg-bindings.html brilliant. let is synatcticfied substitution. But do these actually have semantics?
This is related to unmooring of succ in de bruijn. Kiselyov finally tagless combinator conversion did something similar.

What about SPJ's points?

### Extract and do stuff
Extract lambdas, reduce, reinsert. It does seem simple. Doesn't produce as much junk.
Seek
app(lam(?x, ?b), ?y) => eval()

app(lam(?x1,?b1), q) = app(lam(?x2, ?b2), q) :- lam(?x1, ?b1) = lam(?x2, ?b2).
No, only is q fresh. forall q. This might as well be "extract and check alpha". I'm not even sure it's right.



lam(?x1, ?b1) = lam(?x2, ?b2) :- lam(?x1,  app(lam(?x2, ?b2), ?x1)), = lam(?x1, ?b1) 



HVM does jive
- graph representation is reminiscent of egraph already (biartite to eclass nodes)
- read out sounds like extraction
- victor's point about only dealing with things that can result from avalaution of lambda terms sounds like only doing egraphs that come from terms.
- 

Two seperate problems:
Alpha recovery vs substitution/normalization
For sums/intergals alpha is what you need.
sum(xfresh, ) isn't _that_ bad. You need to manually recover alpha though. Sometimes this is important. But I've been willing to make bigger composromises

Some nodes can be turned upside down by (multiported going up) by precomposing with a projection. plus(a,b)probably can't since it doesn't have a unique decomposition once we start rewriting.

Lambdas need? incremental copying?
Different "modes". LambdaVar(l), LambdaBod(l) Lambda(body) you need to tie the not with a fresh var.
Lambda(body) where body was contructed using freshv. Then set freshv = LambdaVar(Lambda(body)). Knot successfuly tied. We have an alpha equiavlant rep.
But how does it know which var is it's... Hmm. This is suspicious. Uhhh. No I guess this is ok.
But it isn't alpha equivalent. to a second construction of the same kind. So be it I guess.

parent pointers is kind of an interesting point generally. We can use parent pointers to drive up one level from a concrete term to a non concrete term (variable in one of it's other leaves) efficiently.

I feel like the dup operation makes it so that we have no sharing anymore.


Interesting for substitution. The parents pointers mean we can easily access at least the immediate context in which x occurs.


just requiring uniqueness of var spots doesn't help? I don't know that you can even maintain this under egraph rewriting. Probably not.

sum( share(x0, x1) , body)

Linear pattern rewriting. with explicit clones.

enode containaing share(x,x) can reduce to enode containing x

break eclass into binary eclass? is binary eclass Differetn from share?
n-arity share nodes? surrree. Not really a problem.

What about eclasses represented using pointer style union find. Then enode is redajustable. Can stil have many incoming nodes.

Low level C runtime style egraph. pointer/graph rewriting engine. How do you search it though? Garbage collection style techniques.
Tags. rust enums are tags though. 
Avoid hashing


explicit sharing

### Yinhong let binings
ambda calculus in gogi:

```
sort term.
rel false() -> term.
rel true() -> term.
rel num(i32) -> term.
rel var(string) -> term.
rel add(term, term) -> term.
rel eq(term, term) -> term.
rel lam(string, term) -> term.
rel let(string, term, term) -> term.
rel fix(term, term) -> term.
rel cond(term, term, term) -> term.

rel free(term, string).
rel const_num(term, i32).
rel const_bool(term, bool).
rel is_const(term).

% constant folding
const_num(c, i) :- num(i, c).
const_num[c] := const_num[a] + const_num[b] 
             if c = add[a, b].
const_bool[true[], true].
const_bool[false[], false].
const_bool[c] := true[]
              if c = eq[a, a].
const_bool[c] := false[]
              if c = eq[a, b],
                 const_num[a] != const_num[b].
is_const(c) :- const_num[c].
is_const(c) :- const_bool[c].

% free variable analysis
free(c, v):- var(v, c).
free[c] := free[a] if c = add[a, b].
% unfolds to free(c, v) :- free(a, v), if c = add[a, b].
free[c] := free[b] if c = add[a, b].
free[c] := free[a] if c = eq[a, b].
free[c] := free[b] if c = eq[a, b].
free(c, v) :- free(body, v) 
           if c = lam[x, body],
              v != x.
% fv(let(x, a, b)) = free(a) + (free(b) \ x)
free(c, v) := free(b, v) 
           if c = let[x, a, b],
              v != x.
free[c] := free[a]
           if c = let[x, a, b].
free(c, v) :- free(body, v) 
           if c = fix[x, body],
              v != x.
free[c] := free[pred] if c = cond[pred, a, b]
free[c] := free[a] if c = cond[pred, a, b]
free[c] := free[b] if c = cond[pred, a, b]

% if-true
then := cond[true[], then, else].
% if-false
else := cond[false[], then, else].
% if-elim
else := cond[eq[var[x], e], then, else]
     if let[x, e, then] = let[x, e, else]
let[x, e, then] :- cond[eq[var[x], e], then, else]
let[x, e, else] :- cond[eq[var[x], e], then, else]
% add-comm
add[b, a] := add[a, b]
% add-assoc
add[a, add[b, c]] := add[add[a, b], c]
% eq-comm
eq[b, a] := eq[a, b]
% fix
let[v, fix[v, e], e] := fix[v, e]
% beta
let[v, e, body] := app[lam[v, body], e]
% let-app
app[let[v, e, a], let[v, e, b]] := let[v, e, app[a, b]]
% let-add
add[let[v, e, a], let[v, e, b]] := let[v, e, add[a, b]]
% let-eq
eq[let[v, e, a], let[v, e, b]] := let[v, e, eq[a, b]]
% let-const
c:= let[v, e, c] if is_const(c)
% let-if
cond[let[v, e, pred], let[v, e, then], let[v, e, else]] 
    := let[v, e, cond[pred, then, else]]
% let-var-same
e := let[v1, e, var[v1]]
% let-var-diff
var[v2] := let[v1, e, var[v2]] if v1 != v2
% let-lam-same
lam[v1, body] := let[v1, e, lam[v1, body]]
% let-lam-diff
lam[v2, let[v1, e, body]] := let[v1, e, lam[v2, body]] 
                          if v1 != v2, free(e, v2)
% capture-avoiding subst
lam[fresh, 
  let[v1, e, 
    let[v2, var[fresh], 
        body]]] 
  := 
let[v1, e, lam[v2, body]] 
  if v1 != v2, !free(e, v2), fresh = gensym().
```


basically a straightforward port of lambda.rs. The most interesting thing is probably the definition of free and is_const, both of which are encoded as plain relations and the analyses logic are easily expressible inside Gogi (i.e., no rust code)

Very interesting. Is stratification not a problem on that !free()?

I love the usage of let

Is stratification not a problem on that !free()?
That's a good point. It seems this program is syntactically non-monotonic. I have to think about this more

I’d a little bit suspect something could become free or unfree under rewriting. I’m not sure I understand what free means on an eclass. Free in every term of the eclass? Free in at least one term?
Both notions can probably be useful
Free in at least one term is monotonic under rewriting
Free in every term isn’t

according to the equational theory, every term in the same e-class should have the same FVs?

I’m not sure. Binding and egraphs don’t play nice
I kind of am suggesting that FV seems like it become multiple distinct definitions when extended to egraphs
Notfree in at least one term is also monotonic
The funniness of negation in datalog/contructive logic. !free and notfree are probably distinct notions
!free is “I don’t have proof it is free” and notfree is “i have proof it is the case it is not free”

If two terms have different set of FVs, then they shouldn't be merged into the same e-class, right?
like var(a) and var(b) should never be equiv to each other (although \a.a and \b.b could)

Hmm. I’m not sure. Guard every rewrite rule on the free variable set?
My sense is that it is easy to have the egraph equate things under your feet

if a rewrite rule does not introduce / eliminate fvs, then the invariant should be maintained

Binding is too syntactic and the egraph is too semantic
Maybe.
If this is true it seems pretty dang subtle to me. What if you put a loop in your egraph? Seems like there could be a way to smuggle a variable somewhere it shouldn’t be

\a. a-a = \b. b - b = \c. 0   b-b = 0 = a-a   what is free and bound in these eclasses (edited) 

Ah I see! That's probably because ?x - ?x => 0 is not FV preserving
We could perhaps tag(a,0)  as distinct from 0

One thing we can do is to associate e-classes with FVs
This may be doable in egglog, and is definitely tricky in egg

The full version of tagging is tagging every subterm with it's context, which seem possible. But then many equal things are not seen as equal and there is a lo of repeated rewriting

https://uw-cse.slack.com/archives/C01JJQNFA3G/p1647627785866149?thread_ts=1647532167.160219&cid=C01JJQNFA3G


Hmm. You know, I feel like egglog is very close to being able to express this concept via a rule already. If you tag terms with context
 foo(ctx2, ?x,?y) = bar(ctx2,?z)  :- foo(ctx1, ?x,?y) = bar(ctx1,?z), ctxs(ctx2), ctx1 <= ctx2  
From a thread in egraph-db | Mar 18th | View reply
But it's interesting that you're suggesting maybe a less heavyhanded way of doing it. You only perhaps need to tag terms that don't already obviously contain variables (edited) 
It does seem subtle though. As does anything that has to do with binding and capture
Well, I think the equational rule of lambda calculus should be fine, so maybe only constants / non-lambda terms are troubling
Here is a trick: change the definition of num from rel num(i32) -> term. to rel num(i32, term)., so two nums with same int won't be unified

Yes.. maybe. tag(a,0)  is a weird kind of anti binder. Maybe it should be considered part of the lambda calculus syntax like let is and the rule is you only make rules that preserve free sets like you said
tag : string -> term -> term
forget? unbind ? Better names
It's like an operation dropping an element of the context like how let adds an element to the context
let = insert  tag = delete because the context is a map structure from vars to terms (edited) 
tag is the opposite of let


let v e (tag v b) -> b
(tag v (let v e b)) -> (let v (tag v e) b)  also. (edited) 
 v1 != v2, tag  ?  <-> ?


I suspect tagging may cause a lot of overhead for terms with large set of fvs
Yes. It's bad. I'm not even sure it's safe
my trick (disallowing constants to be unified) may just work, but I need more time to think about that
would be a fun weekend project
It's sort of intertwining using the SMT theory of arrays for the context with a theory of terms. var(x) means lookup key x in context. (edited) 
I hope we have a hoare logic / separation logic / whatever logic for reasoning e-graphs :joy:

! is always going to be fishy in egglog because we assume no stratification by default because we don't expect saturation
It's use requires very careful hard to check thought
For which I have no formal system that seems appropriate
My rule of thumb is to always have a compositional semantic model in mind. That seems to help (edited) 
The semantic model of something with binding terms is quite puzzling
But usually involves some notion of context.
Your use of !free is to block overzealous gensym right? Maybe there is a way to skolemize it instead
Or maybe just remove that rule entirely
Weaker system, but still perhaps able to do something useful

I adapt from here: https://github.com/egraphs-good/egg/blob/main/tests/lambda.rs#L153
lambda.rs
        rw!("let-lam-diff";
<https://github.com/egraphs-good/egg|egraphs-good/egg>egraphs-good/egg | Added by GitHub

this rule is important though since it implements capture avoiding subst

```
% capture-avoiding subst
lam[fresh(v1,v2,e,body), 
  let[v1, e, 
    let[v2, var[fresh(v1,v2,e,body)], 
        body]]] 
  := 
let[v1, e, lam[v2, body]] 
  if v1 != v2.
```
This is the skolemize idea. I suspect it is incorrect (edited) 
In the term world, fresh could in principle be a deterministic function of the pieces you have around? (edited) 
You don't need a globally fresh variable, just one that doesn't appear in e, body, v1 or v2



## Scoped union find
Max has put forward that we need different union finds floating around indexed in some way

Ok he says that what about just a graph with lbaelled edges. connectivity in this graph is union find. Good point. The graph is then literally 
Can I use souffle choice-domain to do somethign good. It gives me an incremental spanning tree.

A more complex union find structure seems like it could be useful. We might way a full non destructive union find. Another option is a "scoped union find". Scopes form a forest. Deeper scopes get the subsequent unions of their ancestor scopes, but not vice versa. Scopes form a partially ordered set.

Options of multiple related union finds:
1. The fillaitre conchon persistent union find has an angle where you really only have one union find active at a time.
2. Using functional maps to implement union find rather than imperative arrays or refs. You don't get path compression? How important is that really? ln(n) vs inverse_ack(n) both feel close to constant.
3. just copy entire union find every time you need a new one. Updates to higher union finds don't immediately propagate to lower ones for good or bad

When you call `find` on a lower scope, you need to traverse up through the higher scopes to collect up any unions they may have accumulated since the scope was created. Is there a way to not require this?

In a sense, scope boundaries are delimitters that stop certain kinds of union find information from being propagated. Path compression is still fine (?), but calling union on a deeper scope cannot be allowed to affect disjoint sets at higher scope. The indirection of scopes never goes away unless you can explicitly collapse them for some reason (if you keep reference to all your children scopes). Collapsing merges yourself into your parent scope, and then redirects your children to your parent.

Other names:
- Marked union find
- union find trie - we could have scopes tagged with interesting info. Or not generated basted on "gensym" counter more or less, but instead looked up by key as in a trie

The pointer perspective on union find seem like it could be interesting. I wonder if literally stack techniques from delim continuations are useful? Copying stacks, markings stacks. Just carrying a scope identifier in the references? That makes sense. You could just not union beyond your current scope. Maybe depth/name?

No you do path compress. The difference is on union. You find up to the scope barrier.
No union is the same. union doesn't require a find.

You only need to refer to the equivalence classes of the scope above you. So deeper scopes could be quite small. Hmm. Hard to see how to do this.


It almost feels like it might be a "macro scale" union find, but I don't really see how to implement union on two scopes. It would be unionfind merge + ?

Maybe it's sort of keeping a stack of union finds that are implicitly being union find merged. But we hold off to share access to the upper ones.

union = keep top of scope. Perform full find. Set top of scope to this? Or just perform union at top of scope?

Copy on write optimization (COW). `Vec<Option<Box<Page>>>` Separate domain into pages. If None, assume no change compared to scope above. If deep scopes are small changes this could lead to memory savings. At the expense of even more indirection though.


```rust
type SUF =
{
  size : usize,
  ufs : Vec<ScopedUF> 
}


struct Scope(usize); // scopes are just labeled by integers into the ufs vector
struct ScopedUF {
  parent : Option<Scope>, // may be root
  uf : unionfind //Vec<usize> // maybe make Either<Vec<usize>, SimpleMap<usize,usize>>. For scopes where not much action happens, we want a sparse fast map.
}

impl ScopedUF {
  fn default(size) {
    ScopedUF {parent : None, uf : default() } //0..size.collect() }
  }
}

impl SUF {
  fn fresh_scope(self : &mut Self) {
    self.ufs.push(default(self.size))
  }
  fn build_child_scope(self, : &mut Self, s : Scope) {
    let d = default();
    d.parent = Some(s);
    self.ufs.push(d);
  }
  fn upstream(self : &mut Self, s : Scope) {
    // This is just destructive unionfind merge? So we could merge any scope s into any other s' really.
    // iterate through scope calling union on parent
    let uf = vec[s.0];
    if let Some(parent) = uf.parent {
      //let puf = vec[parent.0];
      for i in 0..self.size {
        // but we don't even need to do a full find. We only need to to a local lookup up to next scope
        self.union(i, self.find(i, s), parent);
      }
    }
  }
  fn scope_parent(self : &Self, s : Scope){
    self.ufs[s.0].parent
  }
  // fn upstream_and_destroy? Need to fix children of scope, which if we maintain we can do. Otherwise just call destroy leaf
  //
  fn upstream_range(self : &mut, start : Scope, stop : Scope) {
    // You can call upstream in a loop
    let mut s = start;
    // actually it might be important to start at the top and work down.
    while s != stop {
      self.upstream(start);
      s = self.scope_parent(s);
    }
  }
  fn find(self : &mut Self, id : usize, s : Scope){
    // ordinary path compression
    let uf = vec[s.0];
    while let Some(parent) = uf.parent {

    }
    // No: At scope boundaries, either still do ordinary micro path compression, or perhaps merge up completely to canon of next scope.
  }

  fn delimit_find(self : &mut Self, id : usize, s : Scope, stop : Scope){
    // perform find up to scope `stop`
  }
  fn scope_find(self, id, s) {
    self.delimit_find(id, s, s)
  }
  fn union(self : &mut Self, id1 : usize, id2 : usize, s : Scope) {
    // can union 
    // maybe do parallel find scope by scope so you can early stop.
    //
    

  }
  fn push_parent(){
    // it does seem possible to insert a new scope between yourself and your parent easily.
    // make your own parent pointer point to new scope, make new scope a fresh one pointing to your old parent
  }
  fn destroy_leaf_scope(self : &mut Self, s : Scope){
    // add to dead list, clear it's data. We're incrementally heading towards a memory allocator at that point.
    // But maybe that's ok.
  }
}

```

Can you get away with just tagging? I don't really see how this works.

```rust
struct SUF = {
  uf : Vec<usize>
  scopedepth : Vec<usize>
  rank : Vec<usize>
  scopename : Vec<usize>
}
```


It's reminsceint to me of marking in delimitted containution implementation

In some sense what you want are blockers or barriers to the path compression process. Path compression should only propagate up to the scoping barrier because other scope may have the same parent. - This isn't what I think anymore?
Questions:
Magic set transformation? What does backward chaining in egglog look like? Could this use a partinitioning algo?
I guess you could do it clp/chc style 
{x = y} as a set of constraints doesn't seem that interesting. It's always possible.
Well, just pattern matching doesn't need `=`
What about a static analysis of a rewriting system.
Abstract domains of all seen terms.

## Higher order rules
We could use this
[horn into eq](https://smallbone.se/papers/horn.pdf) to reflect rules themselves into the egraph?
So what if rules are just containutation of sorts. Save points? Defunctionlization kind of? Closure kind of? A little weird. Feels a bit like magic set. Feels a bit like some kind of memoization. We could flatten out all patterns into this.

```
foo(x,y,z) => bar(z,w,q) => ziz(x,z).
```

becomes
```
foo() :- rule1(x,z).
rule1(x,z), bar(z,w,q) => ziz(x,z)
```

even rwrite rule
`foo(?x) <- add(?x,add(?x,z))`
can becomes

```
chk1(q, x) :- q = add(x,y)
foo = add(x,q) :- chrk1(q, x), q = add(x,z)
```

You only need binary multipatterns in other words.


## Semantics of Egraphs
You know, partial equivlaence relations is a more appropriate lattice.
It better represents what ywe're talking about. PER over Nat is exactly ids and unionfind.
Makeset(i)
makeset : Nat -> PERNAT

Function tables are partial functions

The I don't want to talk about "ids" could probably be takn care of via categorical style construction
projection arrows into a thing. Like a pointed thing. L is special.
     ^
     |
  -> L  <- 



[Yihong egglogish semantics](https://necessary-taker-84c.notion.site/Egglogish-6fce65a95af542f4964db0146eb00c8a)
[egg sem remy](https://hackmd.io/@remyw/egg-sem)
[formalizing egg#](https://necessary-taker-84c.notion.site/Formalizing-egg-53b650e3d91f42058172a877caf0950a)
A Herbrand universe U is the set of all possible terms you can build out of some set of function symbols
Example:
- add(-,-), succ(-), zero , the Herbrand universe is {zero, succ(zero), add(zero,zero), succ(add(zero,zero)), ...}
- for symbols a,b,c,  the Herbrand universe is just {a,b,c}.



The powerset of the Herbrand universe $$\mathcal{P}(U)$$ is the set of all sets of terms. Sets form a [lattice](https://en.wikipedia.org/wiki/Lattice_(order)) under the operations of intersection and union.

Lattices are a useful notion in computer science. The algebraic propertiers of commutativity, associatvity, and idempotence means you don't have to be careful or can change the order of operations and everything still comes out good. 

Monotone functions are functions tha respect an ordering $$x \le y \implies f(x) \le f(y)$$.  The [Knaster-Tarki theorem](https://en.wikipedia.org/wiki/Knaster%E2%80%93Tarski_theorem) says that a fixed point of monotone operations exists.
I think a lattice homomorphism is a distinct stronger concept?


A Datalog rule defines a function between $$r : \mathcal{P}(U) \rigtharrow \mathcal{P}(U)$$. You can convert this operation to an expansive operation via the combined rule $$ X -> X \cup r(X)$$ that adds in the new pieces to the pieces that were already there.

A monotonic operation between lattices is one such tha it respects the latice ordering.

Because the Herbrand universe of datalog is finite because it only had atoms / 0-arity function symbols, no monotonic operation can expand forever.

A partition of the Herbrand universe is a subset of the powerset. It is a set of sets whose union is the entire universe and which do not intersect. A partition can be interpreted as equivalence classes.
Partitions also form a lattice. There is a partial order of coarser and finer partitions. You can find the finest partition that is coarser than two others. This is a meet of those two. The partition where every element is in it's own set is the bottom, the partition consisting of just the entire set is top.

The set of all partitions Pa(U) is a subset of the powerset of the powerset of U.

A function symbol is basically a function between n copies of the Herbrand universe.
A function symbol can be lifted to function that work over powersets $$[f] : \mathcal{P}(U)^n \rigtharrow \mathcal{P}(U)$$  $$[f](X) = \{ f(x_1,x_2,...) | x_1 \elem X_1, x_2 \elem X_2, ... \}$$.

It is a little unclear what it means to lift f to work over partitions. But this notion is where the semantics of congruence closure lie.
The problem is we are tempted to be working over one particular egraph. But this is awkward and non compositional?

$$A^B$$ is a notation to represent $$ B -> A $$. The powerset of A can be written as $$2^A$$.

A lattice annotated powerset `X : Pa(U) -> X -> L` = $$ \prod_{X : P(U)} L^X $$. X represents the egraph we are currently working in.

P(T) := True.
(a -> Prop) -> Prop


The relational model
Instead of function symbols, we have relations (over what? integers? abstract keys? Maybe this is what a categorical model might help with.)
function symbols are relation symbols with a functional dependency
Rather than being a tree, a relational term has pieces which are multi ported graphs?
Congruence becomes a secondary consideration

What if we considered each argument to the function being from a different union find 
LxLxL -> L

And then we also have like a forker and joiner or somethig.
    _______
   /
---
   \_______

L -> LxL

LxL -> L  (meet)

does `f` even do anything to the union find structure?
Yes, it has to combine them. It also performs congruence closure on f application

The image of a partition
[f](PP) = { ~f(P) | P \in PP} = { { f(p) | p in P} | P in PP }
Do I need to do some union cleanup here? I might.
{   U_p_in_P{  f(p') |  for P' in PP if p in P' for p' in P  } |  P in PP } assuming {} dedups?

concrete example
{f(a) , f(f(a))}, {a} - image -> {f(a)} { f(f(a)), f(f(f(a))) }
No this isn't a problem. Because f is injective as a term function

Okkkkk....
What we're describing is kind of just egraphs with analysis and conguence close but no rules.

So we end up with L as a fixed point of this process (the analysis normalized egraph congruent?)

So... a rule is... a pattern match is...? 
L -> {matches}

matches also form a lattice. (you can merge bindings)

L -> {matches} of a multipattern can be done in any order and merged

an "applier"

multimatch =  (pat1 | pat2 | pat3) . joinmatch . ( app1 | app2 | app3 )

rule  (searcher | searcher | searcher) . (app1 | app2 | app3) : L -> L
f* = [f] . fork : L -> L
g* = [g] . fork : L -> L
and so on

fix(f* | g* | rule1 | rule2)

Yes.
Whqat really does congruence closure. It's the combo of f with the join semantics

Patterns as egraphs
we never 

Where | has semantics of join/meet on output.

Applier...

match -> L ???

fix(meet(f.fork, g.fork, a, id)) = the whole egraph normalization


exists P : Partition, P -> L
where L is a lattice
```coq
Inductive Term : Type :=
   string -> list Herbrand -> Herbrand.
(* Is this acceptable? Not sure it will be. Anyhow *)




Definition TSet := Term -> Prop. (* Bool? *)

(*
A particular restricted herbrand universe


*)

Definition schema := list (string * nat) 
Definition valid (s : schema) : Term -> bool := fun (t : term) =>
  match t with
  | name, args => if lookup (name) == List.length args then List.forall (valid s) args else false.


Record FinPartitioning := {
  partitions : list TSet;
  complete : TSet
  disjoint : Forall2 partitions (fun x y => forall t, band (x t) (y t) == false)
}.


Record Enode := {
  head : string;
  args : list nat
}.
Definition egraph := list list Enode.
(* 

Definition egraph := { Term -> Term , with fixedpoint exists}.


*)

(** conversion to TSet *)
Definition in_eclass (egraph : egraph) (eclass : list Enode) : Term -> Bool :=

Definition denote (egraph : egraph) : FinPartitioning := 


Definition Partition := list Enode. (* A little disatisfying. *)
Definition Partition := TSet.


```







A lot of things are implicit in the egraph. They have to be because it's talking about an infintie number of terms


Implicitly f(a) = f(b) in the egraph.

Q = (Herbrand Partition -> A) is the basic lattice of the egraph analysis


f : QxQxQ -> Q  is combo congruence closure and analysis transfer function
By default things can just return bottom, so you get to choose which trasnfer functions you define


a : () -> Q

So these are semantics, but then relating this back to a finite representation (and an efficient one)








## NE graph
 See also bisimulation partition refinement
 
https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.637.1146&rep=rep1&type=pdf co-nelson opppen.
Combining theories through inequalities. Somehow condindituvie/bisimulation
analysis + opt https://dl.acm.org/doi/10.1145/565816.503298 sorin. Do dataflow while optimizing?

If you can say _nothing_ in X equals anything in U\X.
cons(a,b) != not cons

So the input to not_eq is 1 set.
a pattern describes a set.
cons(?a,?x)
Indeed this is stating a form of injectivity
Or more than that? We are also saying that f(7) = cons(7,[]) is not allowed
cons is an ocean unto itself.
{cons(?,?), f(?), g(?,?)} is stating closed universe assumption only f and g have access to constructor cons.
Ok, but nothing can call f or g either. too closed

Should this be about terms? Contexts? co-terms? objects? automata? graphs?
What Set representation to use? Is Set the wrong alley?

concrete ineuqalities as the _rules_. Patterns go in the negraph?
If we find 
can match a + b using the current negraph, we derive a pattern from it we assert as disjoint?
cons(?a,?b) is a distinct _pattern_ from all others, even it it isn't a distinct function/constructor really.
cons(cons(?a,?b), ?c) is not distinct from cons(?x,?y)
Multi match? Views? Pattern synonyms
Maybe the pattern match coverage problem is a good one? Coverage and defunctness.
| Cons(Cons(a,b)) ->
| Cons(a,b) -> 
| _ -> 


I guess when you know things have distinct types you know they can never be equal. It doesn't even make sense.
If you have equality reasoning on types that can be touh to say out of hand though (unless injective)

distinct( ?x : hom(?a,?b) )
disctinct( ?x : ob )

types are a way to 



That's interesting. A formalization of the inaccessibility of certain constructions.
private.
{ ctor1, cto2, class_method1, class_metho2,  } states a closed class definition.
Can we express subtyping? Seems hard.

But is separates off an entire mini universe.

can anything call meth1?

?ctx[ clor1(?,?) ] also describes a set of terms. Anything that contain ctor1
{ ?ctx[ match(cons(?a,?b), rhs(?a,?b))  ] , ctx } No. Doesn't really work. We don't want to say has to any rhs has to include the match form?
I guess that's eta expansion. Hmm.

_Everything_ else is seperate? We have to descibe very small equality classes.

Give X, X and U\X are distinct.
What methods of describing X are useful here.
disjoint(X)

disjoint(X,Y,Z) st XuYuZ = U or disjoint(X,Y) Z is implicit = U/X/Y
but disjoint(X) /\ disjoint(Y) is the same thing.


foo(?a) != bar(?a) is there a way to talk about this?
{ .. . .. . . foo} { .... . . . . bar} but what are all the dots?

Just contexts? "dual" to terms?





subclass {ctor1, ctor2, ctor3, meth1, meth2, meth3}
superclass {ctor1, ctor2, meth1, meth2}

->
meth3 can't access ctor1 or ctor2? That's not right.

{}  {} 

pattern matching?
f(cons(?,?))





Lambda binding:
Optimal evluation work relevant?
Compiling lambda terms relevant? Closures? Reflection into host? Uncurrying trasnformation?


[egg-smol](https://github.com/mwillsey/egg-smol)
labelled equalities
eq(l,x,y)
l could be integers, a hierarchy of eq. colors
tuples of integers?
Absract symbols 
they could be a context coming from lower egraph.

## Egraphs over programs. Program analyiys


// seperately sotre the egraph and the term? Which souffle gives you by default.



x := a + b
x := 0
surely a + b is not availabe.

if we assume ssa then we never lose expression.
What if it got overwriten? Then it's not available..?




Functions from store







Wait this is the obvious killer app.
Very busy expression and available expressions




Poinbter analysis.
https://szabta89.github.io/publications/inca-pldi2021.pdf 


unification-based point-to analysis 
VSA: interval analysis + periodic analysis 
https://www.cs.cmu.edu/~aldrich/courses/15-819O-13sp/resources/pointer.pdf Anderson vs steensGaard


kCFA is a place where datalog and lambdas meet. Could there be something good there?


.type Expr = Lit n : number | Lam symbol Expr | App Expr Expr 

init($)

subterms() :-
label($,) :- init()
context(t, Hole):- init(t)
context(x, Add(Hole, y) :: c), context(y, Add(x, Hole) :: c),  :- context(Add(x,y), c)

% literlas are equal under any context.
context(Lit(n), C1) = context(Lit(n), C2) 
% alpha renaming?
context(Var(x), C1) =? context(Var(y), C2)

Interesting idea.

Anyhow. Once you build contexts, you can do cfa
sigma(l, n) :- context(Lit(n), l)
sigma(l, t) :- context(Lam(t), l), sigma( Lam(Hole) :: l,  v )

https://dl.acm.org/doi/10.1145/3136040.3136043 quoted staged rewriting

I don't want all subterms. Equal subterms should be different things.
If egraph gets top, what does that mean? Early termination, what does that mean?

Is a union find a lattice? One that largely stays the same except when crossing binders
join of two egraphs is 1. union of symbols. 2. closure of equalities.
identity element is egraph with no terms.

avalaible expression analysis is reminscent. You could use a (persistent?) hash cons. Well you could store inefficiently, but there is almost certianly some efficient wya to do this. available equalities.

Inner scopes inherit from outer scope except when viarables are introduced.

http://moskal.me/smt/e-matching.pdf ematching for fun and profit
https://github.com/yihozhang/sdl staged datalog
https://github.com/yihozhang/egraph-sqlite egraph on sqlite


grobner bases. What is a term? a monomial? a entire polynomial in canonical order?



https://www.categoricaldata.net/cql/fmc.pdf Fast Left-Kan Extensions Using The Chase. Uniqueness quantifiation

[Sketch-Guided Equality Saturation
Scaling Equality Saturation to Complex Optimizations in Languages with Bindings](https://arxiv.org/pdf/2111.13040.pdf). De bruij indicies for lambda calculus. Guidance of rewrite rules.

<https://github.com/mwillsey/thesis> willsey's thesis

<https://twitter.com/corbinsimpson/status/1456649872182939654?s=20> corbin simpson using egg for categorical combinator.

https://github.com/gussmith23/glenside



staging ego (ocaml egraph) could be fun.
Also could try to extract the alt-ergo implementation.

<https://github.com/mwillsey/snake-egg> I approve 


bdds are both more and less than hash consed if then else trees. See fillaitre paper. Can I embed bdds into an eg raph implementation? Bddbdddb used bdds as the backing store of a datalog. Pretty clever and cool. I actually feel like this might be more powerful in many cases of interest than a relational representation.
eqrel representation is also a lattice. The join is the smallest equivalernce relation that works, that's kind of the beauty of it.
What cody wants is a custom relation domain. .decl(  )


hasahing modulo alpha equivalence
- false positives and false negatives
(x + 1) can be totally different things in different contexts
why not de bruin? ok sure. de bruijn are inefficient. false negaitvfes nad false positive
position maps of variables with structure maps.

e-summary - alhpa equivalent if same e-summary


hashlog using souffle.
If I keep the numbr of entries small, I could make a hashing function and just pretend no collisions happen. Or use cryptographic hash
hashcons + union find = egraph


module system - algebraic specification
discharge using smt? A style with which to structure smt proofs.
discharge using egglog - signatures are queries, impls are databases.
Embedded in ocaml, julia, or python.
sexify using modern looking syntax (ryst like? leanlike?)
mod Foo : {

}
semi-naive ematching
applier should return (Id,Id) tuples and also Id of new nodes generated.
This is the fresh information.

I should try building a datalog.
Maybe take a particular datalog program and write it out.


congruence closure as an axiom in the theory.
f(x,y) = f(z,w) :- f(x,y), f(z,w), x = z, y = w.
seminaive based on an update to one of the equations on the right hand side.

parents(x,f(x,z)).
parents()
 :- x = z, parents(x, f(x,X)), parents(z,f(z,X)).

all tables should only be maintained modulo equality. Stock souffle does not do this for me.



Denali and epegs - denali is almost a parallel idea to the uniqson work but from a theorem prover persepctive rather than constraint programming perspective

Egraphs and ematching and instruction selection
If I did not use any union finding, egraph is a hash cons.
Ematching is hash cons matching

egraphs for program syntehsis. That's intriguing. Armando notes and Koppel paper.


Call ematch. Record all matches
Send off to cover solver.

Combine the blindell universal function + epeg





DOUBLE EGRAPH WOAAAH
Yea. I could just maintain 2 egraphs if I want to have 2 notions of equality.

https://www.southampton.ac.uk/~gk1e17/chaseBench.pdf
The chase. EGD and TGD functional dependencies
These both seem like things i want.

memoization of patterns
I used the devie
pattern(A,B,C) :- yada
to record a pattern match. This reifies the bindings of a pattern into a table.

But we can do the same in the egraph and egglog.

zzz :- f(f(f(f(f(X))))).
patmemo(X) <- f(f(X)).
zzz :- P = patmemo(X), f(f(f(P))).

congruence closure and memofixing updates

semi-naive - just search over changes for one of the predciate.
There are many many implicit flattened predicates.


Proof scripts - query and forget egraph but push result into facts / rules if true.
?+ == copy egraph. query. insert into old egraph if true.

Allows proof scripts like this
?+ x = y.
?+ y = z.
?+ z = foo.

?+ x = y.
?+ .. = z.
?+ .. = foo.

?+ x = y.
.. = z.  // allow suppression of ?+ in presence of .. syntax
.. = foo.


?+ forall X, foo(X) => bar(X).  // make fresh X. qeury egraph. If works, insert as lemma. If fails stop program here.



Scoped rules

{
  foo(X) :- bar(Y).

}
// rule is dropped here.
:- push.
:- pop.
:- push_rules.
:- pop_rules.
:- push(egraph).
:- pop(egraph).

"calculating compilers" using an egraph approach. Is this possible?
Instruction selection as an egraph problem?


Hmm. Can you use the size of eclasses to solve combinatorial problems?
Can you implement ZDD or BDD in egrpah form? hash consing is how that kind of works

https://www.youtube.com/watch?v=W4kveStDZXI&ab_channel=YOW%21Conferences hashing moduleo alpha

hashlog - egglog without the union find
Keep a hashcons data structure. It can store all known facts and trees.
hashcons matchign
Semi naive seems more dable
Could I add hashing to souffle to get egglog back?



"egraphs" for higher homotopy? ports are edges?

egraphs for graphs
Take all subgraphs. You can compose named ports to each other
Self consistency - a loop in the g-graph. (g stands for graph) Like some kind of microscope thing
The hroizotnal and vertical composition i already noted was sort of deconstructing to alll
possible decompositions using the nameless positional encoding of string digrams
If we took a string diagrams and put every possible term decopmosition into it from the get go. Because the graph is already a caonnical form in rgards to associativity. It is not in regards to other equations.
Eclass -> [  ]


Hash consing graphs ( depends on dcomposition method, unless you hold eclasses of all possible decompes)



https://asplos-conference.org/abstracts/asplos21-paper142-extended_abstract.pdf

This is interesting sounding
Did they use egraphs for instruction selection I wonder?
Like suppose you had a rewrite rules going from ir to instruction selected ir (+ a b) = (arm_add32 a b)
But you also had IR -> IR equalities
Like (+ a b) -> (+ b a)
and I suppose MIR -> MIR rules would be possible peephole optimizations

There is a similarity between PEGs and blindell's UR and or expression graphs
Actually, what it would do is pattern matching.
Pattern selection would require a secondary optimizatioon problem

How do you instruction select over an egraph
consider a muladd instruction a*b+c
but you have a*(b+d) in your tree.
You want to egraph rewrite and then cover egraph.

Parition refinement is the "dual" of union find - a system of inequalities.
Reverse congruence. f(a) /= f(b) -> a /= b
contrapositive
the dual of an egraph

If we had observations and had to deduce what went wrong
The greatest equivalence relation consistent with the facts
as compared to egarph which is least equiavlacne relation

algebra and colagebra




Egraphs = coinductive? They are the least solution to a set of equations. The looping behavior is reminscent of CoCaml and CoLP.

Cody seems to think pasting of two pullback squares could work.
Hmm. Can I make an ematching quantifier for pullbacks?

Universal quantifier
d    b
     |
     v
a -> c

p -> b
|
v
a
for any p and two morphism such that square commutes, we get a new morphism univ_0(x,y)
with equalities associated with it. And uniqueness?
so we need to be guarded by some tough equalities
ok but fine.
But then to prove universality of the pasting
We can assert some object and some square.
But we need to deduce the universal morphism and prove that it has good triangles. and uniqueness?

What about just epi and mono. Hmm. I need exactness too
https://en.wikipedia.org/wiki/Five_lemma




An igraph - an egraph for inquality reasoning? Some smart datat structure for holding inequalities rather than union find or in addition to? Then the pieces that arec monotoonic lift inequalities through constructors.

PEGs
Convert imperative to functional program essentially. Gated-SSA. Convert back
Generalizing proofs for compiler optimizations: What? Something about 

prolog gensym
Well, gensym is possibly sufficient for my egraph thing. a map to unification variables? nah



https://www.cs.cornell.edu/~ross/publications/proofgen/
Substitution as a category. Free variable nodes. Can I substitute in an egraph? Is subgraph query doable (prob not right)? Substitution via enforcing an equality between the variable nodes and the terms that 
i'm substiution them for. Or no yeah. I could really go in an cut them out of the eclass they belong to.

https://dash.harvard.edu/bitstream/handle/1/4762396/pldi84-tristan.pdf a followup paper - translation validation
https://www.cs.cornell.edu/~ross/publications/eqsat/ equality saturation compiller optimiztion
PEGs program expression graphs
gated ssa
The nodes of loop variables represent the sequence?
Partial sums. Makes sense from an inifite series perspective.
psum f = 0 : (map f 0..10) + (psum f)
!! is indexing function.
We need to lift various things via repeat
and map under layers
data MultiList acc =
MultiList = a + [] + [[]] + [[[]]] + ...
Multilist a = Free [] a
data MultiList a = Roll [MultiList a] | Pure a
Reminsicent of fock space. Not quite. There's something there.

The sum = eval psum factoring somehow makes the de bruijnn raising and lowering more local?
psum has a short self referential definition in terms of +.
sum does not generically?
There was a comment on zulip about geometric series summation
sum g^n = 1 + g * (sum x ^ n) which is a self referential defintiion.
The endpoint moving definition is in some sense equiavalent to the psum
This is inductive rather than coinductive. Is coinductive + egraph somehow important?
sum(n, f) = f(n) + sum(n-1, f)
sum(0, f) = 0 

psum(expr) = cons(0, psum(expr))
psum(n, expr) = lift(n, )


Pawel - essetnailly algerbaic theories. string diagrams something
freelinearfunctions
linearmaps.jl linearoperators.jl
interacting hopf alggerabs

Right, The term model makes differences between schedulings
Yolk.jl, mixtape
abstractinterpeter
alternaitve exceution pipline
runtimegernated

mixtape emit function can dump out bullcrap
mccoy becker
mjolner irtools
bang bang jl 
temporal relation


IR as a category. Keno papers
Single blocks optimization

The egraph as a database.

pattern matching datalog - 
The PATTERNS are dtalog? You could reify the rebuilt egraph into a database. Sure.
But then querying a pattern agains it?


conjunctive queries - np complete
worst case partial join
schedulers.
Thousands of rules.
credit assignment

simulated annealing?
oliver flatt working on proof production in summer


Explicit contexts and context lifts using named variables.
We want local transfromations, because the ergaph gets in the way of stuff
We want thing to be semantic unless we can turn off selective congurence closure.
Explcit contextx will largely be shared.

bound variable expressions
actually extracting good linear algebra
catlab stuff.
proofs.
datalog



lift(x, e)
select(y,e)
Kind of goofy
barendregt convention. gensym new sums
sum(x, e) * sum(y,e) = sum( sum())
reqiures explcit x != y check
x and y should not be eclasses. They are explicit parameters

We can punt on alpha recovery.
sum(i,i) = sum(j,j) is ok
Not this can cause problems
sum(i,i) * sum(j,j) = sum(i,i) * sum(i,,i) 
!= sum(i, sum(i, i * i)) which may result from missing inequality guard above. 
With rearrangement, is this enough?

used variables analysis. ->
sum(x, e * b) = e * sum(x,b) if e does not contain x

Model: Important.
T = (Key -> Int) -> R
sum : key -> T -> T
sum k f = sum $ map f (\i -> \m -> if k = k' then i else m k') (1 .. 10)

x :: T
x m = toReal $ m "x"

y :: T
y m = toReal $ m "y"




Egraphs as rtelations  http://effect.systems/doc/relational.pdf
Datalog + some fields are equivalence classes?
Souffle could do this?

eclass(a,b) .equiv
eclass(a,b) :- f(x,y,z), f(x1,y1,z1),
              eclass(x,x1), eclass(), ,  // One per congruence relation
enode(n, q) 

equiv(n1,n2) :- f(n1, x, y), f(n2,x2,y2), equiv(x,x1), equiv().
equiv() :-
equiv() :-


A pattern match is a query
Q(Root, A) :- f(Root, x,y), g(X, )



Alpha equiavalence:
New enode type? Pointers down or up from binding site? Or to third kind of node?
Recover alpha equiavalence via explicit search somehow
Explicitly model contexts?  x |- x  is different from a |- x
"Hypothetical" unioning? Requires persistent backtracking of unions. Seems useful for recovering alpha.
lam x f ~? lam y g.  if x ~ y => g ~ f. Niavely requires double search for lam over egraph.
Well, what might trigger a new alpha equivalence?

Name and de bruijn in parallel? lam x g g'
de bruijn. Combinators. Same thing? The swap operator was not good. Pushing lazy raising and lowering. It had some nice properties too. raise(u,l) or something? Maybe swapping sum is a bad example to work with.
Names are nice because we have non local character so we can manipulate binders but leave alone use sites
sum_j sum_i q => sum_i sum_j q
de bruijn is nice because we have canonical names. Alpha equivalence becomes automatic.


CatEngine - get david to draw me a cat engine hybrid
Metaocaml + egraph? Ocaml egraph could be good. As cody was saying

Ematching patent application?
https://patentimages.storage.googleapis.com/26/5a/2e/9a7722870e4dbb/US8103674.pdf

Type isomoprhism reasoning in egraph
fix x, 1 + x  is list.
fix x, f x ~ f (fix x, f x) fix equations
quantifiers
semantic model is relational model of paremtricity?
Theroensm for free reasoner?
Ordinary alegebra 0 + x = x, ayda yada
+ yoneda like things from that one paper
https://homepages.inf.ed.ac.uk/wadler/papers/free-rectypes/free-rectypes.txt
lfix x,, f x ~ forall x, (f x -> x) -> x



Proof production:
Does z3 actually keep proof data in the egraph?
The extended union find gives reasons.
I guess every up link has reasons
We maintain another array registering which parent gets attached to which parent.
And an array when a root becomes a child, we record a canonical term at that time. And perhaps we only need to record an enode at that time, since we can look for older stuff? Does the array build a temrpoal order on the nodes?
The "best" enode. or a random one?
Or is it insertion time. At insertion time Eclass == ENode. Maybe we want to record that?


We need to perform an extraction every time we find a new equality? Best term in Eclass equated to best term in other eclass.


Multipatterns and datalog
Allessandro made this analog that is interesintg.
You can use the egraph as a "database" of terms
And use multipatterns to insert new terms. This is just a hash cons.
But you can also use = as a special symbol on both the left and right side of prolog rules
To create new equalities, and to check out ones
What are the syntactic restrictions on dastalog?
We aren't unifying, we're just pattern matching. Is that a problem? A difference?

Semi naive evaluation. Is it possible to only try to match terms that are updated?


Alessandro has this idea of anti rules. These are probably problematic to integrate with this search. Negation is often a funny fellow


b = c :-  b, a = b, c 







Ok, looking back. Could I make a Z3 version?
Horn solver maybe. Careful triggering.

ZX calculus?

Einsum - compile to matrix form. Then do point-free rewriting. This is spore approach
https://optimized-einsum.readthedocs.io/en/stable/
https://github.com/Jutho/TensorOperations.jl

It also feels like compiling a lambda abstraction to combinators.
The bracket abstraction http://www.cantab.net/users/antoni.diller/brackets/intro.html
lam x. b =>

\x -> E K ()

sum(i -> E) = K

Am I crazy about using explicit indices? Is that not just fine? I should have a crisp objection.


sum_i = 1^T like in probability
a_ijk b_ijk =>  dumb trasnlation of otimes with dup.

logic sound with respect to model
untyped lambda is unsound? With respect to what model?
[x]y = K y
[x]x = I



sum(i, i)
sum(i, i * x) = 
-- we need to know x contains no i. This is not clear. We could imagine an "contains i" analysis.
can contain i vs has to contain i. 



Functional programmng and rewrite systems:
Very similar. 
- pattern matching
- recursion

The dom(hom) type trick more abstractly is that you can internalize
functional programs into the egraph rules. You want to do this lightly.
I used this as both guards and to produce complex reight hand sides 
(non productive ones. Hmm Is this another connection to coninductivity?)




We don't have built in higher order functions.
You lose deterministic control
(lam )
(x :: Gamma) |- succ t = Gamma |- ts
eval t env = 

De bruijn-ish something is important. We don't get alpha renaming for free so it's a natural way
to canonicalize that
Egg went a little extrajudicical
But can you write eval for a sum?
You can, I jst don't want to expand it.
eval (Sum t) env = sum [eval t (i :: env) | i <- 1..10]
Maybe gamma should just be the integer like cody said
An outer context lift
n |- succ var 

lift n = n |-, takes an m < n dimensional function and lifts it to n-d.
We can store n as an int.
Or we could store the range in the context.
Wouldn't that be somethin.

de buijn levels would be useful.
They locally record that we don't need your shit. Uh. Wait. No this is indices
sum(succ(x)) = N * x


succ(x) * succ(y) = succ(x * y) -- can extrude lifting through simple functions
succ(x) + succ(y)
x * y = y * x
sum(x + y) => sum(x) + sum(y)
sum(succ(x) * y) = x * sum(y)
sum(1) = N 
sum(var) = N * (N -1 ) / 2, or benoulli poly 
sum(succ(succ(succ))) = b(n,N)

sum(var(1)) = 
sum(var(2))
succ(var(n)) => var(n+1)

x = x * 1


Those two rules give
sum(succ(x)) => sum (succ(x) * 1) => x * sum(1) => x * N
sum(sum(succ(var) * var)) => sum( var * sum(var) ) => sum(sum(var) * var) =>


sum(sum(x)) = sum(sum(swap(x)))
swap(var) = succ(var)
swap(succ(var)) = var
swap(x * y) == swap(x) * swap(y)
swap(x + y) == swap(x) + swap(y)
swap moves through everything like succ. (Except succ. It doesn't move through succ).
Feels very expensive.


swap(succ(x)) => succ(lift_var(x)) -- Then we don't need the extra swap(succ(var)) rule.
lift_var(var) = succ(var)
lift_var(succ(x)) => succ(x)
lift_var(x * y) = lift_var(x) * lift_var


lower(n, succ(x)) = succ(lower(n-1, x))
lower(0,succ(var)) = 
lower  
raise(0, var) = succ(var)

raise(n, lower(n-1)) ? Can't be. the intermediate state makes no sense.

de bruijn do have these useful rasing and lowering operations though


Well, it may indeed be _an_ encoding. so i dunno.

A setoid egraph. 
congruence closure could be written as a rule
x = y => f x = f y 
enumrate all f, or allow matching in the head position.
This is a guarded rule.

I've said the egraph is semantical because it builds in congruence closure. So you need to be talking about something for which congruence closure makes sense.
In particular, I think you should have a model in mind for your syntax. Given a term, you should be able to give me functions for the function symbols and elements for the constants, such that your mathemtical thing obeys your rewrite system.

Yes, yes. Peano arithmetic is fiendish. I know. But each move is meaningful.
The succ forms opens up the lazy push around.



GATs for binding
Nathanael Arkor12:26 PM
You describe everything internally: so you have a type for "contexts", a type for "types", a type for "terms", etc. Then you have operations for extending contexts, etc. This is essentially the approach of "type-theory-in-type-theory", which uses quotient-inductive-inductive types (which are closely related to GATs).

Equational metalogic Fiore http://ropas.snu.ac.kr/~gil.hur/publications/soeqlog.pdf
https://crypto.stanford.edu/~blynn/lambda/logski.html the oleg paper


Cody:
```coq

Definition Ctxt := nat.

Definition ext : Ctxt -> Ctxt := S.

Inductive Lambda : Ctxt -> Type :=
| Var : Lambda (ext 0)
| App : forall Γ : Ctxt, Lambda Γ -> Lambda Γ -> Lambda Γ
| Abs : forall Γ : Ctxt, Lambda (ext Γ) -> Lambda Γ
| Weak : forall Γ : Ctxt, Lambda Γ -> Lambda (ext Γ).

Definition id : Lambda 0 :=
Abs 0 Var.

Definition K : Lambda 0 :=
Abs 0 (Abs 1 (Weak 1 Var)).
```

I want to do sums:
N^n -> N

Realtion to quantum raising and lowering operators? Is that ludicrous?
extending the context = adding on a free parameter = (N^n -> N) -> (N^{n+1) -> N)
weakening the context = (N^n -> N) -> (N^{n-1} -> N) This makes no sense. You can evaluate it, 
sum over it, maximumize over it. You can't just wekaen it.

Jon sterling using GATs from Kris
http://www.jonmsterling.com/pdfs/algebraic-universes.pdf


Note on Mechanized Equational Reasoning for Categories with Metatheory.jl

Here's the punchline

```julia

```

I'm sure you've always wondered if that was true.

In terms of a string diagrams:
![](/assets/pairproj1proj2.png)


Read on for what this means


Compile the egg version for wasm and embed


## Metatheory and EGraphs

Alessandro Cheli has made an extremely intriguing package called Maththeory.jl. As I understand it

- Homoiconicity
- Some mumbo jumbo that I don't understand but is probably very important about using RuntimeGenerateFunctions to ge the right hand side of rules to be fast
- A tuned and feature complete EGraph implementation

I've discussed implementing EGraphs in Julia on this blog before. 
- https://www.philipzucker.com/egraph-1/
- https://www.philipzucker.com/egraph-2/
EGraphs are a data structure that efficiently achieves sharing of subterms in the presence of equality reasoning. The [egg](https://egraphs-good.github.io/) project has recently innovated and brought this technique to prominence

Here's the basics of how you use the Metatheory EGraph backend at the moment

```julia
using Metatheory
using Metatheory.EGraphs


```

I wrote my EGraph implementation with the intention of using it for this blog post.
I've previously written of my abortive attempts using the automatic theorem provers E prover and Vampire and Z3.
- https://www.philipzucker.com/notes-on-synthesis-and-equation-proving-for-catlab-jl/
- https://www.philipzucker.com/theorem-proving-for-catlab-2-lets-try-z3-this-time-nope/

I don't know why these encodings did not work. It still feels to me that they should've. The black box nature of these systems is a problem for troubleshooting.

Catlab is built around a kind of logic called Generalized Algebraic Theories (GATs). 
- https://algebraicjulia.github.io/Catlab.jl/dev/#What-is-a-GAT?
- https://ncatlab.org/nlab/show/generalized+algebraic+theory

In multi-sorted first order logic, you have terms of simple sorts like G or Int. For convenient encoding of categories you need a little more magumbo. One wants to talk about A as a term of sort Object, but then also talk about $$id(A)$$ as a term of sort $$Hom(A,A)$$. You see that the sort of `id(A)` is dependent upon a term `A`. This means that some kind of dependent type system is at play. This is possible to encode into Coq for example, but GATs are a less complicated system that has enough flexibility to do this. They're some kind of dependent types lite.

Encoding this type system correctly is tricky. I naively thought in the first post that basically the issue could be ignored, since every equation is type preserving. If the types start good, and the rules are type preserving, then everything is ok right? This has not been the correct mental model of the situation. It is highly dependent upon the exact reasoning system the degree to which types can be ignored.

A somewhat brute force way of dealing with types is to encode terms as terms tagged with types. For example, instead of merely using the term `id(A)`, we replace it with the syntax `typ(id(A), Hom(A,A))` or the equiavlaent infix syntax `id(A)::Hom(A,A)` that looks like a pun on Julia syntax or `Hom(id(A), A, A)`. The latter form was pointed out to me by James Fairbanks and it has the advantage of fusing 1 level of indirection (In a sense partially fusing the tags `typ` and `Hom`). Really we want to perform this tagging recursively like `Hom(id(Ob(A)), Ob(A), Ob(A))`. This gets incredibly verbose to type out for a human.

Some other notes

- The rewrite rule `f => id(A) \cdot f` produces A out of nowhere if types are erased. That means they either need to be reconstructed.
- Catlab overloads $\otimes$. This makes sense to Julia, because Julia is tracking the types of things and uses them for dispatch. In order to track the types for dispatch in our
- It has been argued that one could use the Julia type system to encode GAT types. The construction of Metatheory.jl makes this a light weight if possible. I argue that this is unlikely. Part of the cleverness of Catlab was to avoid abusing the host type system like is common for categorical constructions in Haskell or Agda.

Type tagging maybe be familiar to the programmer in you in that is the logical reflection of the technique of dynamic typing. Dynamic types are implemented by maintaining a tag at runtime describing how to interpret the attached data. We are in essence doing the same thing in our syntactic rewriting system. We get dispatch based on this type in the same manner that Julia or python get dispatch off of their type tags.




There is absolutely no reason to overload \otimes for both morphism product and object product from the persepctive of mechanization. I admit it is nice to have such overloadings for people, but they should be quickly stripped off for internal representations via inference. I chose to keep them syntactically distinct.

The technical connotations of "sort" vs "type"



James Fairbanks10:58 AM
GATs are a fixed point of the "X can represent Y" relation so they make a good target as a level of generality


Philip Zucker: I thought way back that this preservation would just happen if you strip the types like it does for STLC or something, but it doesn't. I think this misconception is due to my inexperience doing hand calculations of this type (which almost no one does. String diagrams rule), plus I think you "code the happy path" when doing it by hand and may not even notice a situation where just shallow pattern matching on the syntax would allow you do do something verboten, since you're smart enough to not make that dumb mistake. It's also easy unless you're being hyper careful to take big steps that seem obvious but aren't actually implied by the axioms you have.

Philip Zucker: Furthering this misconception is that for a large majority of the 30 some axioms for cartesian SMC it actually _is_ completely fine, I think. So far I've only identified about 4 or so where it's a problem, one of which is the interchange law.

Philip Zucker: And it also can only be using the axioms in particular directions too. One direction is syntactically safe, but the other direction requires checking typing conditions

Philip Zucker: Trick question: Can you apply the interchange law (f ⊗ g) ⋅ (h ⊗ k) => (f ⋅ h) ⊗ (g ⋅ k) to the term (f ⊗ id(a)) ⋅ (id(b) ⊗ k)?

Philip Zucker: No you can't. In my example, f actually has type b ⊗c -> b ⊗c   and k has type c ⊗a -> c ⊗a .

Philip Zucker: The other direction is always fine though. Given (f ⋅ h) ⊗ (g ⋅ k) is well typed, so is (f ⊗ g) ⋅ (h ⊗ k)  I believe.

Philip Zucker: Again all this is obvious in string diagrams. So obvious as to be unobservable.


Egraphs as a model. Z3 uses EGraphs as models of logical statements about uninterpeted functions
We can also consider sets as models of egraphs.
An egraph is a model of a conjunction of equations if 

An interpretation of an egraph is a mapping of function symbols to functions 
and constants and equivalence classes to elements such that if there is a function symbol enode in an eclass, the function interpretation maps the interpetation of it's children classes to the class the enode lives in.
This might be easier to read in mathemtical symbolism.




### Bits and Bobbles

Where to go from here:

2. Actually integrating with Catlab. Does it scale?
3. Daniele Palombi has brought the coend calculus https://arxiv.org/pdf/1501.02503.pdf This seems like a very interesting application. I think the direct attack on this problem using Metatheory requires understanding how to deal with alpha equivalence in the EGraph Conversation here: <https://julialang.zulipchat.com/#narrow/stream/277860-metatheory.2Ejl/topic/Formal.20Proofs.20for.20CT.3F>
4. String diagrams <https://julialang.zulipchat.com/#narrow/stream/230248-catlab.2Ejl/topic/Using.20Metatheory.20for.20String.20Diagrams> There is a compelling argument that string diagrams are a preferred representation, normalizing the associativity,commutative, and unit properties of a categorical construction. It has been suggested by Alessandro and others that teneralizing the EGraph data structure in some sense may be to go.
4. Proof production. Giving 


One wonders if perhaps with my new understandings I could get z3 to work.

I had some verbose encoding I wanted to try
But I also feel like this internalized type thing could work in z3

The external z3 ematcher


Alpha conversion. Add parameters to function symbols.
sum(n, f) : (N^{n+1} -> N) -> (N^{n} -> N)
n is the number 
sum(0, f) = f ??? Eh why bother
sum(1, f)

ind(n, i) == proj(n,i) : N^n -> N
the ith projection of an n tuple. Play the roles of indices
sum(1, sum(2,  ind(2, 1) * ind(2,2)  )  )  )
sum(n, sum(n-1, ind(n, )  ))
Based on the semantic model, I feel like this makes sense.

Every operation also needs these scope tags.
These scope tags are equiavalent to marking both de bu9ijn indices and level

likewise for derivatives
(R -> R) -> (R -> R)
D(n,m, f) marking n input size and m output size
Or perhaps start with only scalar.

Huh. Maybe everything is working in fock space?
Then sigma is a lowering operator.
Fock space is a useful model of a world where we're moving around the number of variables in existence
Disjoint sum of function spaces

adag is adding on a delta function
adag(x)
adag(i,x)

sum reduces the number of free variables by summing over the leftmost one
Ok, but we want to sum over more than just the leftmost.


What about explicilty modelling homotopy functions.
f(0) = a
f(1) = b
g
g
k(1/2) = f(1) = g(0) - contingent upon a proof of f(1) = g(0) that this new function is definable?
k = compose
allowance of a move
 f(x,t) = f
 f(0,0) == 
f()
forall x : [0,1], f(x) = x
f




A decalartive rule interchange format



Could we literally use smtlib?



(set-option :verbose)
(set-option :scheduler :backoff)
(set-option :node_limit)
(set-option :type-check)
(declare-sort  )
(declare-fun  f)
(declare-fun  f)
(assert (=  expr1 expr2 ))
(assert (!=  expr1 expr2 ))
(check-sat)

(declare-rule )
(declare-eq   )
(check-eq  expr1 expr2)
(addexpr expr)
(simplify  expr :iter :timeout :node_limit :  )
(push) -- pushes rules or pushes state of egraph?
(pop)
(push-rules)
(pop-rules)
(clear-egraph)
(exit)

https://rust-cli.github.io/book/crates/index.html

We could have the thing build the corresponding rust file for you.
For guards we'd need a full programming languge
I could build in the facilities to 


There is something rather intuitinistic about the egraph.
If you had propsitions in the egraph, having 
p = True, is really more like p = Proved.
Since not having it does not imply that p is false
p = False is known falsehood
p = True is known Falsehood
neither is unknown
I guess it's more like 3 valued logic.



Condictivtely, egraph is "well-typed"
Each GUARDED rewrite rule maintains well typed-ness

Given the left hand side has matched a pattern that is in fact well typed,
we can infer some equational constraints about the types.
If these equational constraints do not fully imply the typing of the equational
rule, then these extra conditions must be added as guards.



We should 
1. Check the condition
2. Add the typing terms to the egraph so it at least know to refine these.



Definition of well typed:?



Ituitively speaking, an egraph represents a possibly infinite set of terms.
A pattern represents an infinite set of terms. A pattern `f(?a, x, ?a)` represents the set of terms that include $\{ `f(x,x,x)`, `f(y,x,y)` , f(g(c,d), x, g(c,d))...\} $, ie. all the terms . Tis descrption format of sets forms a lattice. The lattice operations of join and meet are defined via 

A (multi)rooted egraph represents a possibly infinite set of terms. It is the set of terms you can build by starting at the root and following the links.
The egraph `[x a]` where a has an edge to the class represents the terms `{x, a(x), a(a(x)), a(a(a(x))) ... }`. 

The allowance of cycles in the egraph makes it unclear to me how to precisely describe the infinite terms. There's something very coinductively at play here.

Perhaps it represents the set of terms with possibly class ids at the leaves. That feels more honest somehow. You start with the root id S_1 = {root_id}. Then you can expand one step to expand this S_2 = {a(root_id} or S_2 = {a(root_id), root_id}. Eventually a subset of these will be the finite terms. 





James made a number of good points



Maybe a highest de bruijn index analysis?
Lowest?
Bothm right,.
Then if they split

sum_examp = @theory begin
   sum( sum(a(0) * b(1))) |>  sum(a(0)) * sum(b(0)) # factoring
   sum( sum(a(0) * b(1))) |> sum( sum(a(1) * b(0))) # permute sum symbol
end

Bidning forms: de bruij indices with raising and lowering
Avoiding accidental dummy clash. Moving pieces out from 


There was that one paper that showed the equational form of the yoneda lemma
in terms of fxied point operators and stuff.

 = a^b  a * b, yada yada.




Dowke higher order unification mentions associativty as a problem like comprehension axioms.

Uniform type tagging vs inline type annotations is kind of like closure vs defunctionlaization
A uniform representation / a specliazed one. A closed world of constructors vs an open one.
Rust enums make the closed work convenient and fast.

Higher order equality reasoning. Equalities of equalities. Dijsktra style. Equality patterns.
Actually by adding equality nodes to every other node, we get equality patterns for free.
(eq(a, a)) but of course.
eq(x,y) => eq(z,w) but then we also need to propagate z = w downward into the graph. Hmm.
Mutiple kinds of equivalnce relations interacting?

Actually only is eq(x,y) is in same eclass as true should be propagate it
eq(x,y) = p  is a condition equation upon the value of p. only if we learn p is true should we propagate this info.
ForAll([x,y], (eq(x,y) == True) ==  (x == y) )
ForAll([x,y], (eq(x,y) == True) ==  (x == y) ) vs
ForAll([x,y], eq(x,y) ==  (x == y) ) ? This is still correct in a sense. It's just that I want to trigger on (eq(x,y) == True)
Can i write a trigger like that in Z3? I'm not sure i can

and(x,y) == x == y == or(x,y)
egraph equality, predicate equality, 
or(x,y) == or(y,x)
or() = asssociatve
or(x,x) == x

eq(y, y)
eq(y,x) => eq(x,y)
eq()

definitional and propositional equality.
eq() can be a question, tentative knowledge, however equailvance clases in the egraph are known knowledge.

Mathmeth
https://www.cs.cornell.edu/home/gries/Logic/Equationalhistory.html
https://www.cs.cornell.edu/home/gries/Logic/Equational.html

EGraphs that are string rewriting.
Mark out EClass 1 as special markjer for empty string.

a memoizing trie? DAG trie. Double ended trie. Links going up and down.


Take the technique of my path post but add more concrete objects.
We do I need the types. I keep getting confused.
id_A != id_B. They need to be kept as distinct objects.

Note just because I'm using z3 "Function"s does not mean we are only discussing categories of functions. Z3 uninterpeted functions are pretty much a completely syntactical construct. Well, ok in a sense we are in that vbia a Yoneda embedding really they are natural transformations between HomSet functors.


Well, I could just keep generating new canonical types.

```python
def all_objects():
  a,b,c = "a","b","c"
  for i in range(n):
    # instantiate the laws for these news objects?

```

How are we going to denote otimes? At the symbol level?
otimes( F, G  ???
F and G are natural trasnformations and otimes is a thing that takes natural transformations returns a new natural transformation
otimes(F, G, FG)
otimes( hom(-,A), hom(-,B), hom(-,AB))
otimes( hom(-,A), hom(-,B)) -> hom(-, AB)

add in all the definitions of otimes, and it's axioms

The extra bits of GATs are sort of what is needed to throw into first order logic in order to make categorical constructions more elegant, in particular the partiality of compose (compose requires that morphisms meet on the same object)
Compose can be defined as a mere relation on morphisms in first order logic, but it is clunky. compose is typically a partial function, for which you'll need to enforce extra axioms to cut that subset out of relations. Extra axioms that you need to keep using over and over increases the complexity of both interactive and automatic proof. It is worht considering if there is a way to make something so fundamental baked into your reasoning system rather than just an axiom in the system. This is perhaps the entire study of logical reasoning systems baked in a nutshell. Want to study computation? Well maybe beta reduction should be baked in your reasoning system. Or maybe not. Worth trying.
A sufficiently flexible system can also build a raft of technqiues, encodings and lemmas that the underlying fundamental reasoning system is not particularly relevant.


a variable of sort "hom(-,A)" represents an morphisms that ends on (codomain is) A



How does z3 even encode types to it's egraph. The types and arity just become part of the function symbol?
Yeah, you could intern the whole thing. arity, string and types


2 tricks:
1. Using the morphisms ~ function. Associativty axioms replaced by function composition. 
2. Using Z3 sorts as objects
3. brute force object expansion.


Interesting example: two symbo0ls a b and they commute. Each element of this monoid is isopmorphic to just a pair of 
naturals.
Can we prover (2,2) + (2,2) = (4,4)?



Metatheories ability to canonicalzie dynamically might be very useful.




# A More Naive EGraph

I know that a significant fraction of Julians worship at the shrine of performance. This is not my default shrine. I still feel like most problems I encounter in my hobbies and work are limited by how difficult the are to phrase and solve even naively, and once that is done performance is 9 times out of 10 not an issue. If it is, then it is time to tweak and reconsider. This is of course informed by my problem domains, that I'm rarely trying to build libraries for others, and the fact that I leverage a massive, mind boggling raft of tools built by people who are deeply concerned about such things, and, uncharitably, that my work does not matter to anyone.



Alessandro Cheli (who is a dynamo of energy) has been building a package called Metatheory.jl, which includes a more feature complete and optimized egraph implementation written in Julia than the one I've described in my blog posts. In addition, my understanding is he's trying to take homoiconicity seriously.

So the version of egraph I'm about to describe to you is not my recommended one, unless you're in a time crunch maybe. Nevertheless, because of it's simplcity, I think there is more conceptual clarity to it.








Or flip something out of the trie to put it in the egraph.


https://taliasplse.wordpress.com/2020/02/02/automating-transport-with-univalent-e-graphs/ 
Talia had a blog post?

Egraph notebook from allssadro

https://colab.research.google.com/drive/1tNOQijJqe5tw-Pk9iqd6HHb2abC5aRid?usp=sharing#scrollTo=zikCWjHH14YH

Ed Yang's egraph in python 
https://twitter.com/ezyang/status/1340507843292770306?s=20
https://gist.github.com/ezyang/c3db0e55a7661998c8a66ea8619f1081

Struct
head:
args:
end
 curried in maps.

Term => EClass becomes
Head => (Args => EClass)

What happens if two args becomes unified?
Is that a problem to keep them seperate?


It's kind of nice to not key on something changing under our feet.
It's not so nice that we can't resolve args without checking into the
intdisjointset
Using straight up pointers we could be pretty fast.
No. It's hard to make an indexing structure that respects the equivalence relation

If i used fixed size tuples in an array, 
Then it could be pretty fast in a certain sense.

Dict{:Symbol, Array{  Tuples{Args, Class}  }}

The product equiavlanece class? Yes.


merge(E,E) = just append arrays
normalize(  ) = for each symbol
        double loop check? Insufficient since we merge classes
        if we get a match .

Wait. That normalization is congurence colsure?
If all the arguments are equiavlanet, then the terms itself is equivalent.

0, unary and binary can get you pretty far. Maybe everywhere
With precompilation

struct Egraph
unionfind :: IntDisjointSet
constants :: Dict{Symbol, Vector{ Int64 }}} # IntSet?
unary:: Dict{Symbol, Vector{ Tuple{ Int64, Int64  }  }}
binary:: Dict{Symbol, Vector{ Tuple{ Int64, Int64  }  }}

abtriary?::Dict{ (Symbol,arity Int64)   , Array{Int64, 2}}}
parent_sym?:: Dict{Term, :Symbol}

end


Could keep the vectors weakly sorted?




Metatheroy.jl and alessandro
Matchcore
He's using Expr. Fine. I still don't really see how 
RuntimeGeneratedFunctions.jl
The world age problem - modality?


Perhaps a mistake was trying to match catlab syntax as much as possible
From haskell, I know many type parameters are inferrable.
Inferrable data in some sense should be kept out of the syntax tree. They just gum it up.
I got bit exactly by this when working with a theorem prover whose mechanism was out of my control
Forward reasoning on forall id(A).f => f because it could instantive f with a nonsenical type unless guarded.
I still can't hel but feel my original opinion in my first pose was essentially correct

Catlab annotates more than Haskell does, but it is not fully annotated either
It relies on type inferrence
We could add perhaps inferable annotations to catlab.
We also want a de-elaboration for the rewrite system?
Stupid algo: try to remove parameter, run inferrance. If it comes back something?

I feel like we don't need types. The rewrite rules respect the typing. Preservation.




id(A) the A is unnecassary. there is always an id that works.
Unification

If the translation from Catlab is nontrivial and not general purpose for all GATs, that takes a lot of bite out of working in julia in the first place. I might as well just stick to native rust.




A special patern for assocaitve rules.
Have rerwite rule comp/2( A comp/2(B,C)) => comp/3( A,B,C) # normalizaes associativty


Allow matching for neighboring positions in term
comp/n(... ,  X,Y, ... ) => 


2 obvious tasks
Make pattern matching fast
actually try to apply to catlab or some other domain?


http://taktoa.me/eqsat/Thesis.pdf remy gldschmit's bachleor's eqsat
https://gist.github.com/ezyang/c3db0e55a7661998c8a66ea8619f1081 yang python egg



In an application where we are trying to do equational reasoning, we have some pile of universally quantified equations like $\forall x. x + zero = x$ .
The e-graph is storing ground terms, ones that do not represent a universal quantification. The e-graph stores the fact $seven + zero = seven$ and $two + zero = two$ separately even though these are instantiations of the same underlying fact.

A natural approach to equational rewriting is to turn your equations into rewrite rules, which are a related but distinct thing. Rewrite rules pattern match a left hand side and replace it with a right hand side. 
Rewrite rules have an asymmetric flavor between the right and left hand side, whereas equality is more symmetric.
Applications of rewrite rules do no necessarily commute. Applying rule 1 and then rule 2 is not necessarily the same as applying 2 then 1.
One can then apply in some order the rewrite rules, hoping for the best, or maintain a set of all terms reachable.


SymbolicUtils arranges its matchers to take an expression, a dictionary of already known bindings, and a callback function that takes a new dictionary and the number of elements matched. Here's a snippet from the library for comparison.

[link](https://github.com/JuliaSymbolics/SymbolicUtils.jl/blob/cd18c76fd09078c38393b398e681329f257ecfe8/src/matchers.jl#L1)
```julia

#### Pattern matching
### Matching procedures
# A matcher is a function which takes 3 arguments
# 1. Expression
# 2. Dictionary
# 3. Callback: takes arguments Dictionary × Number of elements matched
#
function matcher(val::Any)
    function literal_matcher(next, data, bindings)
        islist(data) && isequal(car(data), val) ? next(bindings, 1) : nothing
    end
end
```


Related Libs:
* SymbolicUtils
* Match.jl
* MLStyle.jl

Make egraph generic like egg by implementing in terms of a children and istree function.
Kind of how symbiolic utils does it.

end

generic egraph over anything that implements istree yada yada. call getchildren(::T) rather than .args
children!() tpo update children
children
Basically converts it to


$\forall x. x + zero = x$ becomes 

The equations that produce these equivalences

In application to finding new rewrites, we need to be adding new equalities to the EGraph.





Duality
If you reverse all edges, DAGs remain DAGs.
Unification propagates downwards
Congruence closure propagates upwards

What if we reversed the dag and hash consed all the parents? How similar would congruence closure look like to unifacation

Hashcons Ids vs union find ids. "Fundamental" indirections. The catlab people have convinced me that there is some fundamaenetal character to the indirection that occurs via foreign keys inb databases. So should we too consider there to be a fundamental character to the Ids?
memo :: f(Id) -> Id. Set of endofunctors f on Id? 
A unification relation ~ ( term(x) , otherterm(x) ) . The signatures of the two don't have to match, but the variable sets do. Whereas the opposite is true of composition.
 f Id  <-  Id  ->  g Id  ~~~~ g Id2 <- Id2 -> h Id2.
 Pullback gives Id3 and equivalence set of Id1 Id2.
  Maybe consider the hash cons version?   Id <- Id -> Id
  the met parts perform union [ 2,6,8,1]   [2,5,7,9,5,3]
  No it makes more sense going the other way.
    Stuff -> EquivClas <- Stuff   union find
    [1 ,4,5]   
    Then 
function compose_cospan(f,g)
   unionfind = IntDisjointSet( max(f.apex, g.apex))
   for (x,y) in zip( f.right, g.left)
     merge(x,y) 
    end
    newleft = find_root.( f.left)
    NEWRIGHT = find_root(g.right)
    # maybe a normalization step to return to a range 1..Nequiv_class
    CoSpan( newapex, newleft, newright )
end

It does seem like this makes sense. I dunno what you do with it.
I mean, a pushout _is_ union right?
This is in catlab under colimit.
Does this suggest that maybe I should be implementing EGraphs as a CSet?
There is this complicated schema of Equiavlanece nodes and hash Ids.
    [1,56,7,8,  ] 


EGraphs as a CSet
objects:
EClass
FunHash

morphisms:
1 per function symbol
Maybe functors? Takes multiples EClass

congruence closure does feel like some kind of universal property. It's the largest relation under something

If function symbols are morphisms,
then They can be represented as n-d arrays on the available equaivlance classes.
It avoids the need for a hashmap. At the great expense of being able to be less lazy?
We need like a lazy sparse array. That uses 0 to denote uncomputed.
But a hash table is a lazy sparse array
I guess we could have 1 hash per function symbol. Since we always know the head.
Yea, these forms don't seem useful, but it's an interesting persepctive.

A data structure is a lot like a database

I guess the other interesting takeway that one might have is the other direwction
A hashmap can be like an avluator. memo[ f x y z ] = result.
memo[f x y z] ~ curry ~ memo[f][x y z]
So we don't have to make the correspondence morphism = array

So, where to next?
3 roads.

- implemente pattern matching in z3py
- implement pattern matching in julia
- bind to egg

Egg reference the Z3 macthing paper nad the simplify matching paper
"
E-graphs and E-matching. E-graph were originally proposed several decades ago as an efficient
data structure for maintaining congruence closure [Kozen 1977; Nelson 1980; Nelson and Oppen
1980]. E-graphs continue to be a critical component in successful SMT solvers where they are
used for combining satisfiability theories by sharing equality information [De Moura and Bjørner
2008]. A key difference between past implementations of e-graphs and egg’s e-graph is our novel
rebuilding algorithm that maintains invariants only at certain critical points (Section 3). This makes
egg more efficient for the purpose of equality saturation. egg implements the pattern compilation
strategy introduced by de Moura et al. [de Moura and Bjørner 2007] that is used in state of the art
theorem provers [De Moura and Bjørner 2008]. Some provers [De Moura and Bjørner 2008; Detlefs
et al. 2005] propose optimizations like mod-time, pattern-element and inverted-path-index to find
new terms and relevant patterns for matching, and avoid redundant matches. So far, we have found
egg to be faster than several prior e-graph implementations even without these optimizations.
They are, however, compatible with egg’s design and could be explored in the future. Another key
difference is egg’s powerful e-class analysis abstraction and flexible interface. They empower the
programmer to easily leverage e-graphs for problems involving complex semantic reasoning
"

Term Indexing - Chapter 26 of the Handbook of Automated Reasoning
Data strucures - 
 - Trees or dags. Aggressive sharing vs hash cons. Nelson Oppejn 1980
 - Flatterms. flatten out tree into preoder traversal. Possilby with skip pointer
  - Prolog terms


  - automata based
  - Code trees 


String based indexing - idea: convert patterns into approximate string matching pattern

position sdtrings. We can lay out the terms in some sequence, let's say a preorder traversal. In addition can annotate with positions
This does actually remind me of Code2Vec

https://link.springer.com/chapter/10.1007/3-540-62950-5_59 shostak congurence as a completion algorithm

https://www.microsoft.com/en-us/research/wp-content/uploads/2016/12/krml253.pdf - leino pit claudel

It seems like having a slow but interpretable e matcher would be helpful. 


bjorner and de moura good ematching
prolog - warren machine
Avbstract machine
 pc - current instruction ? weird.
 reg[] - storing ground terms
 bstack - backtracking

```haskell
type Symbol = String
data Machine = 
  | Init Symbol Machine
  | Bind Int Symbol 

data State = State {pc :: Int, bstack :: [Machine] , reg :: [Term] }

cont (Init m) = m

run :: Machine -> State -> State
run (Init f ) { r = [Term f args]   } = s { reg = args  }

```



code trees


Path indexing